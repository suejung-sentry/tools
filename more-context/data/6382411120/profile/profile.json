{
  "frames": [
    {
      "file": "celery",
      "image": "__main__",
      "is_application": false,
      "line": 8,
      "name": "<module>",
      "path": "/usr/local/bin/celery"
    },
    {
      "file": "celery/__main__.py",
      "image": "celery.__main__",
      "is_application": false,
      "line": 15,
      "name": "main",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/__main__.py"
    },
    {
      "file": "celery/bin/celery.py",
      "image": "celery.bin.celery",
      "is_application": false,
      "line": 236,
      "name": "main",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/bin/celery.py"
    },
    {
      "file": "click/core.py",
      "image": "click.core",
      "is_application": false,
      "line": 1161,
      "name": "BaseCommand.__call__",
      "path": "/usr/local/lib/python3.11/dist-packages/click/core.py"
    },
    {
      "file": "click/core.py",
      "image": "click.core",
      "is_application": false,
      "line": 1082,
      "name": "BaseCommand.main",
      "path": "/usr/local/lib/python3.11/dist-packages/click/core.py"
    },
    {
      "file": "click/core.py",
      "image": "click.core",
      "is_application": false,
      "line": 1697,
      "name": "MultiCommand.invoke",
      "path": "/usr/local/lib/python3.11/dist-packages/click/core.py"
    },
    {
      "file": "click/core.py",
      "image": "click.core",
      "is_application": false,
      "line": 1443,
      "name": "Command.invoke",
      "path": "/usr/local/lib/python3.11/dist-packages/click/core.py"
    },
    {
      "file": "click/core.py",
      "image": "click.core",
      "is_application": false,
      "line": 788,
      "name": "Context.invoke",
      "path": "/usr/local/lib/python3.11/dist-packages/click/core.py"
    },
    {
      "file": "click/decorators.py",
      "image": "click.decorators",
      "is_application": false,
      "line": 33,
      "name": "pass_context.<locals>.new_func",
      "path": "/usr/local/lib/python3.11/dist-packages/click/decorators.py"
    },
    {
      "file": "celery/bin/base.py",
      "image": "celery.bin.base",
      "is_application": false,
      "line": 134,
      "name": "handle_preload_options.<locals>.caller",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/bin/base.py"
    },
    {
      "file": "celery/bin/worker.py",
      "image": "celery.bin.worker",
      "is_application": false,
      "line": 356,
      "name": "worker",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/bin/worker.py"
    },
    {
      "file": "celery/worker/worker.py",
      "image": "celery.worker.worker",
      "is_application": false,
      "line": 202,
      "name": "WorkController.start",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/worker/worker.py"
    },
    {
      "file": "celery/bootsteps.py",
      "image": "celery.bootsteps",
      "is_application": false,
      "line": 116,
      "name": "Blueprint.start",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/bootsteps.py"
    },
    {
      "file": "celery/bootsteps.py",
      "image": "celery.bootsteps",
      "is_application": false,
      "line": 365,
      "name": "StartStopStep.start",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/bootsteps.py"
    },
    {
      "file": "celery/concurrency/base.py",
      "image": "celery.concurrency.base",
      "is_application": false,
      "line": 130,
      "name": "BasePool.start",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/concurrency/base.py"
    },
    {
      "file": "celery/concurrency/prefork.py",
      "image": "celery.concurrency.prefork",
      "is_application": false,
      "line": 109,
      "name": "TaskPool.on_start",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/concurrency/prefork.py"
    },
    {
      "file": "celery/concurrency/asynpool.py",
      "image": "celery.concurrency.asynpool",
      "is_application": false,
      "line": 464,
      "name": "AsynPool.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/concurrency/asynpool.py"
    },
    {
      "file": "billiard/pool.py",
      "image": "billiard.pool",
      "is_application": false,
      "line": 1046,
      "name": "Pool.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/pool.py"
    },
    {
      "file": "celery/concurrency/asynpool.py",
      "image": "celery.concurrency.asynpool",
      "is_application": false,
      "line": 482,
      "name": "AsynPool._create_worker_process",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/concurrency/asynpool.py"
    },
    {
      "file": "billiard/pool.py",
      "image": "billiard.pool",
      "is_application": false,
      "line": 1158,
      "name": "Pool._create_worker_process",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/pool.py"
    },
    {
      "file": "billiard/process.py",
      "image": "billiard.process",
      "is_application": false,
      "line": 120,
      "name": "BaseProcess.start",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/process.py"
    },
    {
      "file": "billiard/context.py",
      "image": "billiard.context",
      "is_application": false,
      "line": 331,
      "name": "ForkProcess._Popen",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/context.py"
    },
    {
      "file": "billiard/popen_fork.py",
      "image": "billiard.popen_fork",
      "is_application": false,
      "line": 22,
      "name": "Popen.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/popen_fork.py"
    },
    {
      "file": "billiard/popen_fork.py",
      "image": "billiard.popen_fork",
      "is_application": false,
      "line": 77,
      "name": "Popen._launch",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/popen_fork.py"
    },
    {
      "file": "billiard/process.py",
      "image": "billiard.process",
      "is_application": false,
      "line": 323,
      "name": "BaseProcess._bootstrap",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/process.py"
    },
    {
      "file": "billiard/process.py",
      "image": "billiard.process",
      "is_application": false,
      "line": 110,
      "name": "BaseProcess.run",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/process.py"
    },
    {
      "file": "billiard/pool.py",
      "image": "billiard.pool",
      "is_application": false,
      "line": 292,
      "name": "Worker.__call__",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/pool.py"
    },
    {
      "file": "sentry_sdk/integrations/celery/__init__.py",
      "image": "sentry_sdk.integrations.celery",
      "is_application": false,
      "line": 470,
      "name": "_patch_worker_exit.<locals>.sentry_workloop",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/integrations/celery/__init__.py"
    },
    {
      "file": "billiard/pool.py",
      "image": "billiard.pool",
      "is_application": false,
      "line": 362,
      "name": "Worker.workloop",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/pool.py"
    },
    {
      "file": "celery/app/trace.py",
      "image": "celery.app.trace",
      "is_application": false,
      "line": 675,
      "name": "fast_trace_task",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/app/trace.py"
    },
    {
      "file": "sentry_sdk/utils.py",
      "image": "sentry_sdk.utils",
      "is_application": false,
      "line": 1783,
      "name": "ensure_integration_enabled.<locals>.patcher.<locals>.runner",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/utils.py"
    },
    {
      "file": "sentry_sdk/integrations/celery/__init__.py",
      "image": "sentry_sdk.integrations.celery",
      "is_application": false,
      "line": 343,
      "name": "_wrap_tracer.<locals>._inner",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/integrations/celery/__init__.py"
    },
    {
      "file": "celery/app/trace.py",
      "image": "celery.app.trace",
      "is_application": false,
      "line": 477,
      "name": "build_tracer.<locals>.trace_task",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/app/trace.py"
    },
    {
      "file": "sentry_sdk/integrations/celery/__init__.py",
      "image": "sentry_sdk.integrations.celery",
      "is_application": false,
      "line": 410,
      "name": "_wrap_task_call.<locals>._inner",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/integrations/celery/__init__.py"
    },
    {
      "file": "celery/app/trace.py",
      "image": "celery.app.trace",
      "is_application": false,
      "line": 760,
      "name": "_install_stack_protection.<locals>.__protected_call__",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/app/trace.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_step.py",
      "image": "seer.automation.codegen.relevant_warnings_step",
      "is_application": true,
      "line": 52,
      "name": "relevant_warnings_task",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_step.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_step.py",
        "line": 52,
        "name": "relevant_warnings_task",
        "code": "\nlogger = logging.getLogger(__name__)\n\n\n@celery_app.task(\n    time_limit=AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,\n    soft_time_limit=AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,\n)\ndef relevant_warnings_task(*args, request: dict[str, Any]):\n    RelevantWarningsStep(request, DbStateRunTypes.RELEVANT_WARNINGS).invoke()\n\n\nclass RelevantWarningsStepRequest(PipelineStepTaskRequest, CodegenRelevantWarningsRequest):\n    pass\n\n\nclass RelevantWarningsStep(CodegenStep):\n    \"\"\"\n    Predicts which static analysis warnings in a pull request are relevant to a past Sentry issue.\n    \"\"\"",
        "lineRange": {
          "start": 43,
          "end": 62
        },
        "lines": [
          "import itertools",
          "import json",
          "import logging",
          "from typing import Any",
          "",
          "import requests",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from celery_app.app import celery_app",
          "from integrations.codecov.codecov_auth import get_codecov_auth_header",
          "from seer.automation.autofix.config import (",
          "    AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "from seer.automation.codebase.models import PrFile",
          "from seer.automation.codebase.repo_client import RepoClientType",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodegenRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          ")",
          "from seer.automation.codegen.relevant_warnings_component import (",
          "    AreIssuesFixableComponent,",
          "    AssociateWarningsWithIssuesComponent,",
          "    FetchIssuesComponent,",
          "    FilterWarningsComponent,",
          "    StaticAnalysisSuggestionsComponent,",
          ")",
          "from seer.automation.codegen.step import CodegenStep",
          "from seer.automation.pipeline import PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "@celery_app.task(",
          "    time_limit=AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    soft_time_limit=AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "def relevant_warnings_task(*args, request: dict[str, Any]):",
          "    RelevantWarningsStep(request, DbStateRunTypes.RELEVANT_WARNINGS).invoke()",
          "",
          "",
          "class RelevantWarningsStepRequest(PipelineStepTaskRequest, CodegenRelevantWarningsRequest):",
          "    pass",
          "",
          "",
          "class RelevantWarningsStep(CodegenStep):",
          "    \"\"\"",
          "    Predicts which static analysis warnings in a pull request are relevant to a past Sentry issue.",
          "    \"\"\"",
          "",
          "    name = \"RelevantWarningsStep\"",
          "    request: RelevantWarningsStepRequest",
          "    max_retries = 2",
          "",
          "    @staticmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> RelevantWarningsStepRequest:",
          "        return RelevantWarningsStepRequest.model_validate(request)",
          "",
          "    @staticmethod",
          "    def get_task():",
          "        return relevant_warnings_task",
          "",
          "    @inject",
          "    def _post_results_to_overwatch(",
          "        self,",
          "        llm_suggestions: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "        config: AppConfig = injected,",
          "    ):",
          "",
          "        if not self.request.should_post_to_overwatch:",
          "            self.logger.info(\"Skipping posting relevant warnings results to Overwatch.\")",
          "            return",
          "",
          "        # This should be a temporary solution until we can update",
          "        # Overwatch to accept the new format.",
          "        suggestions_to_overwatch_expected_format = (",
          "            [",
          "                suggestion.to_overwatch_format().model_dump()",
          "                for suggestion in llm_suggestions.suggestions",
          "            ]",
          "            if llm_suggestions",
          "            else []",
          "        )",
          "",
          "        request = {",
          "            \"run_id\": self.context.run_id,",
          "            \"results\": suggestions_to_overwatch_expected_format,",
          "            \"diagnostics\": diagnostics or [],",
          "        }",
          "        request_data = json.dumps(request, separators=(\",\", \":\")).encode(\"utf-8\")",
          "        headers = get_codecov_auth_header(",
          "            request_data,",
          "            signature_header=\"X-GEN-AI-AUTH-SIGNATURE\",",
          "            signature_secret=config.OVERWATCH_OUTGOING_SIGNATURE_SECRET,",
          "        )",
          "        requests.post(",
          "            url=self.request.callback_url,",
          "            headers=headers,",
          "            data=request_data,",
          "        ).raise_for_status()",
          "",
          "    def _complete_run(",
          "        self,",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "    ):",
          "        try:",
          "            self._post_results_to_overwatch(static_analysis_suggestions_output, diagnostics)",
          "        except Exception:",
          "            self.logger.exception(\"Error posting relevant warnings results to Overwatch\")",
          "            raise",
          "        finally:",
          "            self.context.event_manager.mark_completed_and_extend_static_analysis_suggestions(",
          "                static_analysis_suggestions_output.suggestions",
          "                if static_analysis_suggestions_output",
          "                else []",
          "            )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings Step\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings Step\")",
          "    def _invoke(self, **kwargs) -> None:",
          "        self.logger.info(\"Executing Codegen - Relevant Warnings Step\")",
          "        self.context.event_manager.mark_running()",
          "        diagnostics = []",
          "",
          "        # 1. Read the PR.",
          "        repo_client = self.context.get_repo_client(type=RepoClientType.READ)",
          "        pr_files = repo_client.repo.get_pull(self.request.pr_id).get_files()",
          "        pr_files = [",
          "            PrFile(",
          "                filename=file.filename,",
          "                patch=file.patch,",
          "                status=file.status,",
          "                changes=file.changes,",
          "                sha=file.sha,",
          "            )",
          "            for file in pr_files",
          "            if file.patch",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Read PR\",",
          "                \"pr_files\": [pr_file.filename for pr_file in pr_files],",
          "                \"warnings\": [warning.id for warning in self.request.warnings],",
          "            }",
          "        )",
          "",
          "        # 2. Only consider warnings from lines changed in the PR.",
          "        filter_warnings_component = FilterWarningsComponent(self.context)",
          "        filter_warnings_request = FilterWarningsRequest(",
          "            warnings=self.request.warnings, pr_files=pr_files",
          "        )",
          "        filter_warnings_output: FilterWarningsOutput = filter_warnings_component.invoke(",
          "            filter_warnings_request",
          "        )",
          "        warning_and_pr_files = filter_warnings_output.warning_and_pr_files",
          "",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Filter Warnings Component\",",
          "                \"filtered_warning_and_pr_files\": [",
          "                    [item.warning.id, item.pr_file.filename]",
          "                    for item in filter_warnings_output.warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "        if not warning_and_pr_files:  # exit early to avoid unnecessary issue-fetching.",
          "            self.logger.info(\"No warnings to predict relevancy for.\")",
          "            self._complete_run(None, diagnostics)",
          "            return",
          "",
          "        # 3. Fetch issues related to the PR.",
          "        fetch_issues_component = FetchIssuesComponent(self.context)",
          "        fetch_issues_request = CodeFetchIssuesRequest(",
          "            organization_id=self.request.organization_id, pr_files=pr_files",
          "        )",
          "        fetch_issues_output: CodeFetchIssuesOutput = fetch_issues_component.invoke(",
          "            fetch_issues_request",
          "        )",
          "        # Clamp issue to max_num_issues_analyzed",
          "        all_selected_issues = list(",
          "            itertools.chain.from_iterable(fetch_issues_output.filename_to_issues.values())",
          "        )",
          "        all_selected_issues = all_selected_issues[: self.request.max_num_issues_analyzed]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Fetch Issues Component\",",
          "                \"all_selected_issues\": [issue.id for issue in all_selected_issues],",
          "            }",
          "        )",
          "",
          "        # 4. Limit the number of warning-issue associations we analyze to the top",
          "        #    max_num_associations.",
          "        association_component = AssociateWarningsWithIssuesComponent(self.context)",
          "        associations_request = AssociateWarningsWithIssuesRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            filename_to_issues=fetch_issues_output.filename_to_issues,",
          "            max_num_associations=self.request.max_num_associations,",
          "        )",
          "        associations_output: AssociateWarningsWithIssuesOutput = association_component.invoke(",
          "            associations_request",
          "        )",
          "        # Annotate the warnings with potential issues associated",
          "        for association in associations_output.candidate_associations:",
          "            assoc_warning, assoc_issue = association",
          "            warning_from_list = next(",
          "                (w for w in warning_and_pr_files if w.warning.id == assoc_warning.warning.id), None",
          "            )",
          "            if warning_from_list:",
          "                if isinstance(warning_from_list.warning.potentially_related_issue_titles, list):",
          "                    warning_from_list.warning.potentially_related_issue_titles.append(",
          "                        assoc_issue.title",
          "                    )",
          "                else:",
          "                    warning_from_list.warning.potentially_related_issue_titles = [assoc_issue.title]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Associate Warnings With Issues Component\",",
          "                \"candidate_associations\": [",
          "                    {",
          "                        \"warning_id\": association[0].warning.id,",
          "                        \"pr_file\": association[0].pr_file.filename,",
          "                        \"issue_id\": association[1].id,",
          "                    }",
          "                    for association in associations_output.candidate_associations",
          "                ],",
          "                \"potentially_related_issue_titles\": [",
          "                    {",
          "                        \"warning_id\": warning_and_pr_file.warning.id,",
          "                        \"potentially_related_issue_titles\": warning_and_pr_file.warning.potentially_related_issue_titles,",
          "                        \"pr_file\": warning_and_pr_file.pr_file.filename,",
          "                    }",
          "                    for warning_and_pr_file in warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "",
          "        # 5. Filter out unfixable issues b/c it doesn't make much sense to raise suggestions for issues you can't fix.",
          "        are_issues_fixable_component = AreIssuesFixableComponent(self.context)",
          "        are_fixable_output: CodeAreIssuesFixableOutput = are_issues_fixable_component.invoke(",
          "            CodeAreIssuesFixableRequest(",
          "                candidate_issues=all_selected_issues,",
          "                max_num_issues_analyzed=self.request.max_num_issues_analyzed,",
          "            )",
          "        )",
          "        fixable_issues = [",
          "            issue",
          "            for issue, is_fixable in zip(",
          "                all_selected_issues,",
          "                are_fixable_output.are_fixable,",
          "                strict=True,",
          "            )",
          "            if is_fixable",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Are Issues Fixable Component\",",
          "                \"fixable_issues\": [issue.id for issue in fixable_issues],",
          "            }",
          "        )",
          "",
          "        # 6. Suggest issues based on static analysis warnings and fixable issues.",
          "        static_analysis_suggestions_component = StaticAnalysisSuggestionsComponent(self.context)",
          "        static_analysis_suggestions_request = CodePredictStaticAnalysisSuggestionsRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            fixable_issues=fixable_issues,",
          "            pr_files=pr_files,",
          "        )",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput = (",
          "            static_analysis_suggestions_component.invoke(static_analysis_suggestions_request)",
          "        )",
          "",
          "        # 7. Save results.",
          "        self._complete_run(static_analysis_suggestions_output, diagnostics)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/pipeline.py",
      "image": "seer.automation.pipeline",
      "is_application": true,
      "line": 72,
      "name": "PipelineStep.__init__",
      "path": "/app/src/seer/automation/pipeline.py",
      "codeContext": {
        "file": "seer/automation/pipeline.py",
        "line": 72,
        "name": "PipelineStep.__init__",
        "code": "    Main method that is run is _invoke, which should be implemented by the subclass.\n    \"\"\"\n\n    name = \"PipelineStep\"\n    request: _RequestType\n    context: _ContextType\n\n    def __init__(self, request: dict[str, Any], type: DbStateRunTypes | None = None):\n        self.request = self._instantiate_request(request)\n        self.context = self._instantiate_context(self.request, type)\n\n    def invoke(self) -> Any:\n        try:\n            if not self._pre_invoke():\n                self._cleanup()\n                return\n            result = self._invoke(**self._get_extra_invoke_kwargs())\n            self._post_invoke(result)\n            return result\n        except Exception as e:",
        "lineRange": {
          "start": 63,
          "end": 82
        },
        "lines": [
          "import abc",
          "import logging",
          "import uuid",
          "from functools import cached_property",
          "from typing import Any, Generic, TypeVar",
          "",
          "from celery import Task, signature",
          "from pydantic import BaseModel, Field",
          "",
          "from seer.automation.state import DbStateRunTypes, State",
          "from seer.utils import prefix_logger",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "Signature = Any",
          "SerializedSignature = Any",
          "",
          "DEFAULT_PIPELINE_STEP_SOFT_TIME_LIMIT_SECS = 50  # 50 seconds",
          "DEFAULT_PIPELINE_STEP_HARD_TIME_LIMIT_SECS = 60  # 60 seconds",
          "",
          "PIPELINE_SYNC_SIGNAL = \"pipeline_run_mode:sync\"",
          "",
          "",
          "class PipelineContext(abc.ABC):",
          "    state: State",
          "",
          "    def __init__(self, state: State):",
          "        self.state = state",
          "",
          "    @property",
          "    @abc.abstractmethod",
          "    def run_id(self) -> int:",
          "        pass",
          "",
          "    @property",
          "    @abc.abstractmethod",
          "    def signals(self) -> list[str]:",
          "        pass",
          "",
          "    @signals.setter",
          "    @abc.abstractmethod",
          "    def signals(self, signals: list[str]):",
          "        pass",
          "",
          "",
          "class PipelineStepTaskRequest(BaseModel):",
          "    run_id: int",
          "    step_id: int = Field(default_factory=lambda: uuid.uuid4().int)",
          "",
          "",
          "def make_step_request_fields(context: PipelineContext):",
          "    return {\"run_id\": context.run_id}",
          "",
          "",
          "# Define a type variable that is bound to PipelineStepTaskRequest",
          "_RequestType = TypeVar(\"_RequestType\", bound=PipelineStepTaskRequest)",
          "_ContextType = TypeVar(\"_ContextType\", bound=PipelineContext)",
          "",
          "",
          "class PipelineStep(abc.ABC, Generic[_RequestType, _ContextType]):",
          "    \"\"\"",
          "    A step in the automation pipeline, complete with the context, request, logging + error handling utils.",
          "    Main method that is run is _invoke, which should be implemented by the subclass.",
          "    \"\"\"",
          "",
          "    name = \"PipelineStep\"",
          "    request: _RequestType",
          "    context: _ContextType",
          "",
          "    def __init__(self, request: dict[str, Any], type: DbStateRunTypes | None = None):",
          "        self.request = self._instantiate_request(request)",
          "        self.context = self._instantiate_context(self.request, type)",
          "",
          "    def invoke(self) -> Any:",
          "        try:",
          "            if not self._pre_invoke():",
          "                self._cleanup()",
          "                return",
          "            result = self._invoke(**self._get_extra_invoke_kwargs())",
          "            self._post_invoke(result)",
          "            return result",
          "        except Exception as e:",
          "            self._handle_exception(e)",
          "            raise e",
          "        finally:",
          "            self._cleanup()",
          "",
          "    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
          "        return {}",
          "",
          "    def _pre_invoke(self) -> bool:",
          "        return True",
          "",
          "    def _post_invoke(self, result: Any) -> Any:",
          "        pass",
          "",
          "    def _cleanup(self):",
          "        pass",
          "",
          "    @cached_property",
          "    def logger(self):",
          "        run_id = self.context.run_id",
          "        name = self.name",
          "        prefix = f\"[{run_id=}] [{name}] \"",
          "        return prefix_logger(prefix, logger)",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def get_task() -> Task:",
          "        pass",
          "",
          "    @classmethod",
          "    def get_signature(cls, request: PipelineStepTaskRequest, **kwargs) -> Signature:",
          "        return cls.get_task().signature(",
          "            kwargs={\"request\": request.model_dump(mode=\"json\")}, **kwargs",
          "        )",
          "",
          "    @staticmethod",
          "    def instantiate_signature(serialized_signature: SerializedSignature | Signature) -> Signature:",
          "        return signature(serialized_signature)",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> _RequestType:",
          "        pass",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def _instantiate_context(",
          "        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None",
          "    ) -> _ContextType:",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def _handle_exception(self, exception: Exception):",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def _invoke(self, **kwargs) -> Any:",
          "        pass",
          "",
          "    @property",
          "    def step_request_fields(self):",
          "        return make_step_request_fields(self.context)",
          "",
          "",
          "class PipelineChain(PipelineStep):",
          "    \"\"\"",
          "    A PipelineStep which can call other steps.",
          "    \"\"\"",
          "",
          "    def next(self, sig: SerializedSignature | Signature, **apply_async_kwargs):",
          "        if PIPELINE_SYNC_SIGNAL in self.context.signals:",
          "            signature(sig).apply(**apply_async_kwargs)",
          "        else:",
          "            signature(sig).apply_async(**apply_async_kwargs)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/step.py",
      "image": "seer.automation.codegen.step",
      "is_application": true,
      "line": 21,
      "name": "CodegenStep._instantiate_context",
      "path": "/app/src/seer/automation/codegen/step.py",
      "codeContext": {
        "file": "seer/automation/codegen/step.py",
        "line": 21,
        "name": "CodegenStep._instantiate_context",
        "code": "class CodegenStep(PipelineStep):\n    context: CodegenContext\n\n    @staticmethod\n    def _instantiate_context(\n        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None\n    ) -> PipelineContext:\n        if type is None:\n            type = DbStateRunTypes.UNIT_TEST\n        return CodegenContext.from_run_id(request.run_id, type=type)\n\n    def _invoke(self, **kwargs: Any) -> Any:\n        sentry_sdk.set_tag(\"run_id\", self.context.run_id)\n        super()._invoke(**kwargs)\n\n    def _pre_invoke(self) -> bool:\n        done_signal = make_done_signal(self.request.step_id)\n        return done_signal not in self.context.signals\n\n    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
        "lineRange": {
          "start": 12,
          "end": 31
        },
        "lines": [
          "from typing import Any",
          "",
          "import sentry_sdk",
          "",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import CodegenStatus",
          "from seer.automation.pipeline import PipelineContext, PipelineStep, PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.automation.utils import make_done_signal",
          "",
          "",
          "class CodegenStep(PipelineStep):",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _instantiate_context(",
          "        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None",
          "    ) -> PipelineContext:",
          "        if type is None:",
          "            type = DbStateRunTypes.UNIT_TEST",
          "        return CodegenContext.from_run_id(request.run_id, type=type)",
          "",
          "    def _invoke(self, **kwargs: Any) -> Any:",
          "        sentry_sdk.set_tag(\"run_id\", self.context.run_id)",
          "        super()._invoke(**kwargs)",
          "",
          "    def _pre_invoke(self) -> bool:",
          "        done_signal = make_done_signal(self.request.step_id)",
          "        return done_signal not in self.context.signals",
          "",
          "    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
          "        try:",
          "            current_state = self.context.state.get()",
          "            repo = self.context.repo",
          "",
          "            tags = {",
          "                \"run_id\": current_state.run_id,",
          "                \"repo\": repo.full_name,",
          "                \"repo_id\": repo.external_id,",
          "            }",
          "",
          "            metadata = {\"run_id\": current_state.run_id, \"repo\": repo}",
          "            langfuse_tags = [f\"{key}:{value}\" for key, value in tags.items() if value is not None]",
          "",
          "            return {",
          "                \"langfuse_tags\": langfuse_tags,",
          "                \"langfuse_metadata\": metadata,",
          "                \"langfuse_session_id\": str(current_state.run_id),",
          "                \"sentry_tags\": tags,",
          "                \"sentry_data\": metadata,",
          "            }",
          "        except Exception:",
          "            return {}",
          "",
          "    def _post_invoke(self, result: Any):",
          "        with self.context.state.update() as current_state:",
          "            signal = make_done_signal(self.request.step_id)",
          "            current_state.signals.append(signal)",
          "",
          "    def _handle_exception(self, exception: Exception):",
          "        self.logger.error(f\"Failed to run {self.request.step_id}. Error: {str(exception)}\")",
          "",
          "        with self.context.state.update() as current_state:",
          "            current_state.status = CodegenStatus.ERRORED",
          "            sentry_sdk.set_context(\"codegen_state\", current_state.dict())",
          "            sentry_sdk.capture_exception(exception)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/codegen_context.py",
      "image": "seer.automation.codegen.codegen_context",
      "is_application": true,
      "line": 41,
      "name": "CodegenContext.from_run_id",
      "path": "/app/src/seer/automation/codegen/codegen_context.py",
      "codeContext": {
        "file": "seer/automation/codegen/codegen_context.py",
        "line": 41,
        "name": "CodegenContext.from_run_id",
        "code": "        self.repo = request.repo\n        self.state = state\n        self.event_manager = CodegenEventManager(state)\n\n        logger.info(f\"CodegenContext initialized with run_id {self.run_id}\")\n\n    @classmethod\n    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):\n        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)\n        with state.update() as cur:\n            cur.mark_triggered()\n\n        return cls(state)\n\n    @property\n    def run_id(self) -> int:\n        return self.state.get().run_id\n\n    @property\n    def signals(self) -> list[str]:",
        "lineRange": {
          "start": 32,
          "end": 51
        },
        "lines": [
          "import logging",
          "",
          "from seer.automation.agent.models import Message",
          "from seer.automation.codebase.repo_client import RepoClient, RepoClientType",
          "from seer.automation.codegen.codegen_event_manager import CodegenEventManager",
          "from seer.automation.codegen.models import CodegenContinuation, UnitTestRunMemory",
          "from seer.automation.codegen.state import CodegenContinuationState",
          "from seer.automation.models import RepoDefinition",
          "from seer.automation.pipeline import PipelineContext",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.db import DbPrContextToUnitTestGenerationRunIdMapping, DbRunMemory, Session",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "RepoExternalId = str",
          "RepoInternalId = int",
          "RepoKey = RepoExternalId | RepoInternalId",
          "RepoIdentifiers = tuple[RepoExternalId, RepoInternalId]",
          "",
          "",
          "class CodegenContext(PipelineContext):",
          "    state: CodegenContinuationState",
          "    event_manager: CodegenEventManager",
          "    repo: RepoDefinition",
          "",
          "    def __init__(",
          "        self,",
          "        state: CodegenContinuationState,",
          "    ):",
          "        request = state.get().request",
          "",
          "        self.repo = request.repo",
          "        self.state = state",
          "        self.event_manager = CodegenEventManager(state)",
          "",
          "        logger.info(f\"CodegenContext initialized with run_id {self.run_id}\")",
          "",
          "    @classmethod",
          "    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):",
          "        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)",
          "        with state.update() as cur:",
          "            cur.mark_triggered()",
          "",
          "        return cls(state)",
          "",
          "    @property",
          "    def run_id(self) -> int:",
          "        return self.state.get().run_id",
          "",
          "    @property",
          "    def signals(self) -> list[str]:",
          "        return self.state.get().signals",
          "",
          "    @signals.setter",
          "    def signals(self, value: list[str]):",
          "        with self.state.update() as state:",
          "            state.signals = value",
          "",
          "    def get_repo_client(",
          "        self, repo_name: str | None = None, type: RepoClientType = RepoClientType.READ",
          "    ):",
          "        \"\"\"",
          "        Gets a repo client for the current single repo or for a given repo name.",
          "        If there are more than 1 repos, a repo name must be provided.",
          "        \"\"\"",
          "        return RepoClient.from_repo_definition(self.repo, type)",
          "",
          "    def get_file_contents(",
          "        self, path: str, repo_name: str | None = None, ignore_local_changes: bool = False",
          "    ) -> str | None:",
          "        repo_client = self.get_repo_client()",
          "",
          "        file_contents, _ = repo_client.get_file_content(path)",
          "",
          "        if not ignore_local_changes:",
          "            cur_state = self.state.get()",
          "            current_file_changes = list(filter(lambda x: x.path == path, cur_state.file_changes))",
          "            for file_change in current_file_changes:",
          "                file_contents = file_change.apply(file_contents)",
          "",
          "        return file_contents",
          "",
          "    def store_memory(self, key: str, memory: list[Message]):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == self.run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                memory_model = UnitTestRunMemory(run_id=self.run_id)",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def update_stored_memory(self, key: str, memory: list[Message], original_run_id: int):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory)",
          "                .where(DbRunMemory.run_id == original_run_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                raise RuntimeError(",
          "                    f\"No memory record found for run_id {original_run_id}. Cannot update stored memory.\"",
          "                )",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def get_memory(self, key: str, past_run_id: int) -> list[Message]:",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == past_run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                return []",
          "",
          "            return UnitTestRunMemory.from_db_model(memory_record).memory.get(key, [])",
          "",
          "    def get_previous_run_context(",
          "        self, owner: str, repo: str, pr_id: int",
          "    ) -> DbPrContextToUnitTestGenerationRunIdMapping | None:",
          "        with Session() as session:",
          "            previous_context = (",
          "                session.query(DbPrContextToUnitTestGenerationRunIdMapping)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.owner == owner)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.repo == repo)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.pr_id == pr_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            return previous_context",
          ""
        ]
      }
    },
    {
      "file": "contextlib.py",
      "image": "contextlib",
      "is_application": false,
      "line": 137,
      "name": "_GeneratorContextManager.__enter__",
      "path": "/usr/lib/python3.11/contextlib.py"
    },
    {
      "file": "seer/automation/state.py",
      "image": "seer.automation.state",
      "is_application": true,
      "line": 126,
      "name": "DbState.update",
      "path": "/app/src/seer/automation/state.py",
      "codeContext": {
        "file": "seer/automation/state.py",
        "line": 126,
        "name": "DbState.update",
        "code": "    @contextlib.contextmanager\n    def update(self):\n        \"\"\"\n        Uses a 'with for update' clause on the db run id, ensuring it is safe against concurrent transactions.\n        Note however, that if you have two competing updates in which neither can fully complete (say a circle\n        of inter related locks), the database may reach a deadlock state which last until the lock timeout configured\n        on the postgres database.\n        \"\"\"\n        with Session() as session:\n            r = session.execute(\n                select(DbRunState).where(DbRunState.id == self.id).with_for_update()\n            ).scalar_one_or_none()\n            self.validate(r)\n            assert r\n            value = self.model.model_validate(r.value)\n            yield value\n            self.before_update(value)\n            db_state = DbRunState(id=self.id, value=value.model_dump(mode=\"json\"))\n            self.apply_to_run_state(value, db_state)\n            session.merge(db_state)",
        "lineRange": {
          "start": 117,
          "end": 136
        },
        "lines": [
          "import abc",
          "import contextlib",
          "import dataclasses",
          "import functools",
          "import threading",
          "from enum import Enum",
          "from typing import Any, ContextManager, Generic, Iterator, Type, TypeVar",
          "",
          "from pydantic import BaseModel",
          "from sqlalchemy import select",
          "",
          "from seer.db import DbRunState, Session",
          "",
          "_State = TypeVar(\"_State\", bound=BaseModel)",
          "_StateB = TypeVar(\"_StateB\", bound=BaseModel)",
          "",
          "",
          "class DbStateRunTypes(str, Enum):",
          "    AUTOFIX = \"autofix\"",
          "    UNIT_TEST = \"unit-test\"",
          "    PR_REVIEW = \"pr-review\"",
          "    RELEVANT_WARNINGS = \"relevant-warnings\"",
          "    PR_CLOSED = \"pr-closed\"",
          "    UNIT_TESTS_RETRY = \"unit-test-retry\"",
          "",
          "",
          "class State(abc.ABC, Generic[_State]):",
          "    \"\"\"",
          "    An abstract state buffer that attempts to push state changes to a sink.",
          "    \"\"\"",
          "",
          "    @abc.abstractmethod",
          "    def get(self) -> _State:",
          "        \"\"\"",
          "        A method to return a locally secure copy of the state.  Note that mutations to this",
          "        value are not likely to be reflected, see `update`.",
          "        \"\"\"",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def update(self) -> ContextManager[_State]:",
          "        \"\"\"",
          "        A method to atomically get and mutate a state value.  Subclasses should implement",
          "        concurrency primitives around this method to ensure that concurrent access is limited",
          "        by the context of the update itself.",
          "        :return:",
          "        \"\"\"",
          "        pass",
          "",
          "",
          "@dataclasses.dataclass",
          "class LocalMemoryState(State[_State]):",
          "    val: _State",
          "    lock: threading.RLock = dataclasses.field(default_factory=threading.RLock)",
          "",
          "    def get(self) -> _State:",
          "        return self.val",
          "",
          "    @contextlib.contextmanager",
          "    def update(self):",
          "        with self.lock:",
          "            val = self.get()",
          "            yield val",
          "            # Mostly a no-op, except in the case that `get` has semantics copying",
          "            self.val = val",
          "",
          "",
          "@dataclasses.dataclass",
          "class DbState(State[_State]):",
          "    \"\"\"",
          "    State that is stored in postgres: DbRunState model.",
          "    \"\"\"",
          "",
          "    id: int",
          "    model: Type[_State]",
          "    type: DbStateRunTypes",
          "",
          "    @classmethod",
          "    def new(",
          "        cls, value: _State, *, group_id: int | None = None, t: DbStateRunTypes",
          "    ) -> \"DbState[_State]\":",
          "        with Session() as session:",
          "            db_state = DbRunState(value=value.model_dump(mode=\"json\"), group_id=group_id, type=t)",
          "            session.add(db_state)",
          "            session.flush()",
          "            value.run_id = db_state.id",
          "            db_state.value = value.model_dump(mode=\"json\")",
          "            session.merge(db_state)",
          "            session.commit()",
          "            return cls(id=db_state.id, model=type(value), type=t)",
          "",
          "    def get(self) -> _State:",
          "        with Session() as session:",
          "            db_state = session.get(DbRunState, self.id)",
          "            self.validate(db_state)",
          "            assert db_state",
          "            return self.model.model_validate(db_state.value)",
          "",
          "    def validate(self, db_state: DbRunState | None):",
          "        if db_state is None:",
          "            raise ValueError(f\"No state found for id {self.id}\")",
          "        if db_state.type != self.type:",
          "            raise ValueError(f\"Invalid state type: '{db_state.type}', expected: '{self.type}'\")",
          "",
          "    def apply_to_run_state(self, value: _State, run_state: DbRunState):",
          "        \"\"\"",
          "        Can be used to pass down context from state into the db context",
          "        \"\"\"",
          "        pass",
          "",
          "    def before_update(self, value: _State):",
          "        \"\"\"",
          "        Can be used to run some logic before the update is applied to the db",
          "        \"\"\"",
          "        pass",
          "",
          "    @contextlib.contextmanager",
          "    def update(self):",
          "        \"\"\"",
          "        Uses a 'with for update' clause on the db run id, ensuring it is safe against concurrent transactions.",
          "        Note however, that if you have two competing updates in which neither can fully complete (say a circle",
          "        of inter related locks), the database may reach a deadlock state which last until the lock timeout configured",
          "        on the postgres database.",
          "        \"\"\"",
          "        with Session() as session:",
          "            r = session.execute(",
          "                select(DbRunState).where(DbRunState.id == self.id).with_for_update()",
          "            ).scalar_one_or_none()",
          "            self.validate(r)",
          "            assert r",
          "            value = self.model.model_validate(r.value)",
          "            yield value",
          "            self.before_update(value)",
          "            db_state = DbRunState(id=self.id, value=value.model_dump(mode=\"json\"))",
          "            self.apply_to_run_state(value, db_state)",
          "            session.merge(db_state)",
          "            session.commit()",
          "",
          "",
          "@functools.total_ordering",
          "class BufferedMemoryState(State[_State]):",
          "    values: list[_State] = dataclasses.field(default_factory=list)",
          "    lock: threading.RLock = dataclasses.field(default_factory=threading.RLock)",
          "",
          "    def __init__(self, initial: _State):",
          "        self.values.append(initial)",
          "",
          "    def get(self) -> _State:",
          "        return self.values[-1]",
          "",
          "    def __eq__(self, other: Any) -> bool:",
          "        return self.values[-1] == other",
          "",
          "    def __lt__(self, other: Any) -> bool:",
          "        return self.values[-1] < other",
          "",
          "    def __hash__(self) -> int:",
          "        return hash(self.values[-1])",
          "",
          "    def __contains__(self, other: Any) -> bool:",
          "        return other in self.values",
          "",
          "    def __iter__(self) -> Iterator[_State]:",
          "        return iter(self.values)",
          "",
          "    @contextlib.contextmanager",
          "    def update(self):",
          "        with self.lock:",
          "            value = self.get()",
          "            yield value",
          "            self.values.append(value)",
          ""
        ]
      }
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 2308,
      "name": "Session.execute",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 2190,
      "name": "Session._execute_internal",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/context.py",
      "image": "sqlalchemy.orm.context",
      "is_application": false,
      "line": 293,
      "name": "AbstractORMCompileState.orm_execute_statement",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/context.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 1416,
      "name": "Connection.execute",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/sql/elements.py",
      "image": "sqlalchemy.sql.elements",
      "is_application": false,
      "line": 517,
      "name": "ClauseElement._execute_on_connection",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 1639,
      "name": "Connection._execute_clauseelement",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 1848,
      "name": "Connection._execute_context",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 1969,
      "name": "Connection._exec_single_context",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 922,
      "name": "DefaultDialect.do_execute",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 728,
      "name": "Cursor.execute",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 969,
      "name": "Connection.wait",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/waiting.py",
      "image": "psycopg.waiting",
      "is_application": false,
      "line": 342,
      "name": "wait_poll",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/waiting.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 209,
      "name": "BaseCursor._execute_gen",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 403,
      "name": "BaseCursor._start_query",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 499,
      "name": "BaseConnection._start_query",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 467,
      "name": "BaseConnection._exec_command",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 307,
      "name": "PGconn.send_query_params",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/generators.py",
      "image": "psycopg.generators",
      "is_application": false,
      "line": 102,
      "name": "_execute",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/generators.py"
    },
    {
      "file": "psycopg/generators.py",
      "image": "psycopg.generators",
      "is_application": false,
      "line": 141,
      "name": "_fetch_many",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/generators.py"
    },
    {
      "file": "psycopg/generators.py",
      "image": "psycopg.generators",
      "is_application": false,
      "line": 173,
      "name": "_fetch",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/generators.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 529,
      "name": "PGconn.consume_input",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 531,
      "name": "BaseCursor._select_current_result",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/_transform.py",
      "image": "psycopg._transform",
      "is_application": false,
      "line": 152,
      "name": "Transformer.set_pgresult",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_transform.py"
    },
    {
      "file": "psycopg/_transform.py",
      "image": "psycopg._transform",
      "is_application": false,
      "line": 153,
      "name": "Transformer.set_pgresult.<locals>.<listcomp>",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_transform.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 809,
      "name": "PGresult.ftype",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 779,
      "name": "PGresult.status",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 1803,
      "name": "DefaultExecutionContext._setup_result_proxy",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 127,
      "name": "BaseCursor.description",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 127,
      "name": "BaseCursor.description.<locals>.<listcomp>",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/_column.py",
      "image": "psycopg._column",
      "is_application": false,
      "line": 35,
      "name": "Column.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_column.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 660,
      "name": "BaseCursor._encoding",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/_encodings.py",
      "image": "psycopg._encodings",
      "is_application": false,
      "line": 100,
      "name": "pgconn_encoding",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_encodings.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 208,
      "name": "PGconn.status",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 216,
      "name": "PGconn.parameter_status",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "sqlalchemy/sql/elements.py",
      "image": "sqlalchemy.sql.elements",
      "is_application": false,
      "line": 705,
      "name": "ClauseElement._compile_w_cache",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py"
    },
    {
      "file": "sqlalchemy/sql/elements.py",
      "image": "sqlalchemy.sql.elements",
      "is_application": false,
      "line": 317,
      "name": "CompilerElement._compiler",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 1427,
      "name": "SQLCompiler.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 867,
      "name": "Compiled.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 912,
      "name": "Compiled.process",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/visitors.py",
      "image": "sqlalchemy.sql.visitors",
      "is_application": false,
      "line": 143,
      "name": "Visitable._generate_compiler_dispatch.<locals>._compiler_dispatch",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/visitors.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 4754,
      "name": "SQLCompiler.visit_select",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 4916,
      "name": "SQLCompiler._compose_select_body",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 2727,
      "name": "SQLCompiler._generate_delimited_and_list",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 3345,
      "name": "SQLCompiler.visit_binary",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 3400,
      "name": "SQLCompiler._generate_generic_binary",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 3728,
      "name": "SQLCompiler.visit_bindparam",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/compiler.py",
      "image": "sqlalchemy.sql.compiler",
      "is_application": false,
      "line": 3912,
      "name": "SQLCompiler.bindparam_string",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/compiler.py"
    },
    {
      "file": "sqlalchemy/sql/type_api.py",
      "image": "sqlalchemy.sql.type_api",
      "is_application": false,
      "line": 873,
      "name": "TypeEngine._unwrapped_dialect_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/type_api.py"
    },
    {
      "file": "sqlalchemy/sql/base.py",
      "image": "sqlalchemy.sql.base",
      "is_application": false,
      "line": 687,
      "name": "CompileState.create_for_statement",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/base.py"
    },
    {
      "file": "sqlalchemy/orm/context.py",
      "image": "sqlalchemy.orm.context",
      "is_application": false,
      "line": 1091,
      "name": "ORMSelectCompileState.create_for_statement",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/context.py"
    },
    {
      "file": "sqlalchemy/orm/context.py",
      "image": "sqlalchemy.orm.context",
      "is_application": false,
      "line": 2537,
      "name": "_QueryEntity.to_compile_state",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/context.py"
    },
    {
      "file": "sqlalchemy/orm/context.py",
      "image": "sqlalchemy.orm.context",
      "is_application": false,
      "line": 2617,
      "name": "_MapperEntity.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/context.py"
    },
    {
      "file": "sqlalchemy/util/langhelpers.py",
      "image": "sqlalchemy.util.langhelpers",
      "is_application": false,
      "line": 1260,
      "name": "HasMemoized.memoized_attribute.__get__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/langhelpers.py"
    },
    {
      "file": "sqlalchemy/orm/mapper.py",
      "image": "sqlalchemy.orm.mapper",
      "is_application": false,
      "line": 2707,
      "name": "Mapper._post_inspect",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/mapper.py"
    },
    {
      "file": "sqlalchemy/orm/mapper.py",
      "image": "sqlalchemy.orm.mapper",
      "is_application": false,
      "line": 2386,
      "name": "Mapper._check_configure",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/mapper.py"
    },
    {
      "file": "sqlalchemy/orm/mapper.py",
      "image": "sqlalchemy.orm.mapper",
      "is_application": false,
      "line": 4199,
      "name": "_configure_registries",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/mapper.py"
    },
    {
      "file": "sqlalchemy/orm/mapper.py",
      "image": "sqlalchemy.orm.mapper",
      "is_application": false,
      "line": 4240,
      "name": "_do_configure_registries",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/mapper.py"
    },
    {
      "file": "sqlalchemy/orm/mapper.py",
      "image": "sqlalchemy.orm.mapper",
      "is_application": false,
      "line": 2403,
      "name": "Mapper._post_configure_properties",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/mapper.py"
    },
    {
      "file": "sqlalchemy/orm/interfaces.py",
      "image": "sqlalchemy.orm.interfaces",
      "is_application": false,
      "line": 579,
      "name": "MapperProperty.init",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/interfaces.py"
    },
    {
      "file": "sqlalchemy/orm/relationships.py",
      "image": "sqlalchemy.orm.relationships",
      "is_application": false,
      "line": 1645,
      "name": "RelationshipProperty.do_init",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/relationships.py"
    },
    {
      "file": "sqlalchemy/orm/interfaces.py",
      "image": "sqlalchemy.orm.interfaces",
      "is_application": false,
      "line": 1081,
      "name": "StrategizedProperty.do_init",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/interfaces.py"
    },
    {
      "file": "sqlalchemy/orm/interfaces.py",
      "image": "sqlalchemy.orm.interfaces",
      "is_application": false,
      "line": 1033,
      "name": "StrategizedProperty._get_strategy",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/interfaces.py"
    },
    {
      "file": "sqlalchemy/orm/strategies.py",
      "image": "sqlalchemy.orm.strategies",
      "is_application": false,
      "line": 746,
      "name": "LazyLoader.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/strategies.py"
    },
    {
      "file": "sqlalchemy/sql/elements.py",
      "image": "sqlalchemy.sql.elements",
      "is_application": false,
      "line": 632,
      "name": "ClauseElement.compare",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py"
    },
    {
      "file": "sqlalchemy/sql/traversals.py",
      "image": "sqlalchemy.sql.traversals",
      "is_application": false,
      "line": 51,
      "name": "compare",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/traversals.py"
    },
    {
      "file": "sqlalchemy/sql/traversals.py",
      "image": "sqlalchemy.sql.traversals",
      "is_application": false,
      "line": 513,
      "name": "TraversalComparatorStrategy.compare",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/traversals.py"
    },
    {
      "file": "sqlalchemy/sql/traversals.py",
      "image": "sqlalchemy.sql.traversals",
      "is_application": false,
      "line": 961,
      "name": "TraversalComparatorStrategy.compare_binary",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/traversals.py"
    },
    {
      "file": "sqlalchemy/sql/traversals.py",
      "image": "sqlalchemy.sql.traversals",
      "is_application": false,
      "line": 575,
      "name": "TraversalComparatorStrategy.compare_inner",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/traversals.py"
    },
    {
      "file": "sqlalchemy/orm/properties.py",
      "image": "sqlalchemy.orm.properties",
      "is_application": false,
      "line": 305,
      "name": "ColumnProperty.do_init",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/properties.py"
    },
    {
      "file": "sqlalchemy/orm/strategies.py",
      "image": "sqlalchemy.orm.strategies",
      "is_application": false,
      "line": 201,
      "name": "ColumnLoader.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/strategies.py"
    },
    {
      "file": "sqlalchemy/util/langhelpers.py",
      "image": "sqlalchemy.util.langhelpers",
      "is_application": false,
      "line": 1314,
      "name": "MemoizedSlots.__getattr__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/langhelpers.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 2047,
      "name": "Session._connection_for_bind",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "<string>",
      "image": "sqlalchemy.orm.session",
      "is_application": true,
      "line": 2,
      "name": "_connection_for_bind",
      "path": "/app/<string>"
    },
    {
      "file": "sqlalchemy/orm/state_changes.py",
      "image": "sqlalchemy.orm.state_changes",
      "is_application": false,
      "line": 139,
      "name": "_StateChange.declare_states.<locals>._go",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state_changes.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 1143,
      "name": "SessionTransaction._connection_for_bind",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 3269,
      "name": "Engine.connect",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 145,
      "name": "Connection.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 3293,
      "name": "Engine.raw_connection",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 452,
      "name": "Pool.connect",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 1306,
      "name": "_ConnectionFairy._checkout",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 709,
      "name": "DefaultDialect._do_ping_w_event",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "sqlalchemy/dialects/postgresql/_psycopg_common.py",
      "image": "sqlalchemy.dialects.postgresql._psycopg_common",
      "is_application": false,
      "line": 181,
      "name": "_PGDialect_common_psycopg.do_ping",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/dialects/postgresql/_psycopg_common.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 311,
      "name": "BaseCursor._maybe_prepare_gen",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 477,
      "name": "BaseCursor._execute_send",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 275,
      "name": "PGconn.send_query",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/generators.py",
      "image": "psycopg.generators",
      "is_application": false,
      "line": 118,
      "name": "_send",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/generators.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 550,
      "name": "PGconn.flush",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 482,
      "name": "BaseCursor._convert_query",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/_queries.py",
      "image": "psycopg._queries",
      "is_application": false,
      "line": 54,
      "name": "PostgresQuery.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_queries.py"
    },
    {
      "file": "psycopg/_encodings.py",
      "image": "psycopg._encodings",
      "is_application": false,
      "line": 90,
      "name": "conn_encoding",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_encodings.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 852,
      "name": "Connection.cursor",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 481,
      "name": "BaseConnection._check_connection_ok",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 716,
      "name": "_ConnectionRecord.checkout",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/pool/impl.py",
      "image": "sqlalchemy.pool.impl",
      "is_application": false,
      "line": 167,
      "name": "QueuePool._do_get",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/impl.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 393,
      "name": "Pool._create_connection",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 678,
      "name": "_ConnectionRecord.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 898,
      "name": "_ConnectionRecord.__connect",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/engine/create.py",
      "image": "sqlalchemy.engine.create",
      "is_application": false,
      "line": 645,
      "name": "create_engine.<locals>.connect",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/create.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 616,
      "name": "DefaultDialect.connect",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 732,
      "name": "Connection.connect",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 983,
      "name": "Connection._wait_conn",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/waiting.py",
      "image": "psycopg.waiting",
      "is_application": false,
      "line": 91,
      "name": "wait_conn",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/waiting.py"
    },
    {
      "file": "selectors.py",
      "image": "selectors",
      "is_application": false,
      "line": 468,
      "name": "EpollSelector.select",
      "path": "/usr/lib/python3.11/selectors.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 436,
      "name": "BaseConnection._connect_gen",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/generators.py",
      "image": "psycopg.generators",
      "is_application": false,
      "line": 68,
      "name": "_connect",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/generators.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 123,
      "name": "PGconn.connect_poll",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 724,
      "name": "PGconn._call_int",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "sqlalchemy/event/attr.py",
      "image": "sqlalchemy.event.attr",
      "is_application": false,
      "line": 485,
      "name": "_CompoundListener._exec_w_sync_on_first_run",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/event/attr.py"
    },
    {
      "file": "sqlalchemy/event/attr.py",
      "image": "sqlalchemy.event.attr",
      "is_application": false,
      "line": 499,
      "name": "_CompoundListener.__call__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/event/attr.py"
    },
    {
      "file": "sqlalchemy/util/langhelpers.py",
      "image": "sqlalchemy.util.langhelpers",
      "is_application": false,
      "line": 1917,
      "name": "only_once.<locals>.go",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/langhelpers.py"
    },
    {
      "file": "sqlalchemy/engine/create.py",
      "image": "sqlalchemy.engine.create",
      "is_application": false,
      "line": 748,
      "name": "create_engine.<locals>.first_connect",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/create.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 692,
      "name": "DefaultDialect.do_rollback",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 901,
      "name": "Connection.rollback",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "sqlalchemy/dialects/postgresql/psycopg.py",
      "image": "sqlalchemy.dialects.postgresql.psycopg",
      "is_application": false,
      "line": 353,
      "name": "PGDialect_psycopg.initialize",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/dialects/postgresql/psycopg.py"
    },
    {
      "file": "sqlalchemy/dialects/postgresql/base.py",
      "image": "sqlalchemy.dialects.postgresql.base",
      "is_application": false,
      "line": 3049,
      "name": "PGDialect.initialize",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/dialects/postgresql/base.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 527,
      "name": "DefaultDialect.initialize",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 578,
      "name": "DefaultDialect.get_default_isolation_level",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "sqlalchemy/dialects/postgresql/psycopg.py",
      "image": "sqlalchemy.dialects.postgresql.psycopg",
      "is_application": false,
      "line": 431,
      "name": "PGDialect_psycopg.get_isolation_level",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/dialects/postgresql/psycopg.py"
    },
    {
      "file": "sqlalchemy/dialects/postgresql/base.py",
      "image": "sqlalchemy.dialects.postgresql.base",
      "is_application": false,
      "line": 3083,
      "name": "PGDialect.get_isolation_level",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/dialects/postgresql/base.py"
    },
    {
      "file": "sqlalchemy/dialects/postgresql/psycopg.py",
      "image": "sqlalchemy.dialects.postgresql.psycopg",
      "is_application": false,
      "line": 350,
      "name": "PGDialect_psycopg._type_info_fetch",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/dialects/postgresql/psycopg.py"
    },
    {
      "file": "psycopg/_typeinfo.py",
      "image": "psycopg._typeinfo",
      "is_application": false,
      "line": 81,
      "name": "TypeInfo.fetch",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_typeinfo.py"
    },
    {
      "file": "psycopg/_typeinfo.py",
      "image": "psycopg._typeinfo",
      "is_application": false,
      "line": 99,
      "name": "TypeInfo._fetch",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_typeinfo.py"
    },
    {
      "file": "sqlalchemy/engine/result.py",
      "image": "sqlalchemy.engine.result",
      "is_application": false,
      "line": 1499,
      "name": "Result.scalar_one_or_none",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/result.py"
    },
    {
      "file": "sqlalchemy/engine/result.py",
      "image": "sqlalchemy.engine.result",
      "is_application": false,
      "line": 754,
      "name": "ResultInternal._only_one_row",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/result.py"
    },
    {
      "file": "sqlalchemy/engine/result.py",
      "image": "sqlalchemy.engine.result",
      "is_application": false,
      "line": 2279,
      "name": "IteratorResult._fetchone_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/result.py"
    },
    {
      "file": "sqlalchemy/orm/loading.py",
      "image": "sqlalchemy.orm.loading",
      "is_application": false,
      "line": 217,
      "name": "instances.<locals>.chunks",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/loading.py"
    },
    {
      "file": "sqlalchemy/engine/result.py",
      "image": "sqlalchemy.engine.result",
      "is_application": false,
      "line": 543,
      "name": "ResultInternal._raw_all_rows",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/result.py"
    },
    {
      "file": "sqlalchemy/engine/cursor.py",
      "image": "sqlalchemy.engine.cursor",
      "is_application": false,
      "line": 2103,
      "name": "CursorResult._fetchall_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/cursor.py"
    },
    {
      "file": "sqlalchemy/engine/cursor.py",
      "image": "sqlalchemy.engine.cursor",
      "is_application": false,
      "line": 1135,
      "name": "CursorFetchStrategy.fetchall",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/cursor.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 859,
      "name": "Cursor.fetchall",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "psycopg/_transform.py",
      "image": "psycopg._transform",
      "is_application": false,
      "line": 307,
      "name": "Transformer.load_rows",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_transform.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 822,
      "name": "PGresult.get_value",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 790,
      "name": "PGresult.ntuples",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "seer/automation/codegen/codegen_context.py",
      "image": "seer.automation.codegen.codegen_context",
      "is_application": true,
      "line": 30,
      "name": "CodegenContext.__init__",
      "path": "/app/src/seer/automation/codegen/codegen_context.py",
      "codeContext": {
        "file": "seer/automation/codegen/codegen_context.py",
        "line": 30,
        "name": "CodegenContext.__init__",
        "code": "class CodegenContext(PipelineContext):\n    state: CodegenContinuationState\n    event_manager: CodegenEventManager\n    repo: RepoDefinition\n\n    def __init__(\n        self,\n        state: CodegenContinuationState,\n    ):\n        request = state.get().request\n\n        self.repo = request.repo\n        self.state = state\n        self.event_manager = CodegenEventManager(state)\n\n        logger.info(f\"CodegenContext initialized with run_id {self.run_id}\")\n\n    @classmethod\n    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):\n        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)",
        "lineRange": {
          "start": 21,
          "end": 40
        },
        "lines": [
          "import logging",
          "",
          "from seer.automation.agent.models import Message",
          "from seer.automation.codebase.repo_client import RepoClient, RepoClientType",
          "from seer.automation.codegen.codegen_event_manager import CodegenEventManager",
          "from seer.automation.codegen.models import CodegenContinuation, UnitTestRunMemory",
          "from seer.automation.codegen.state import CodegenContinuationState",
          "from seer.automation.models import RepoDefinition",
          "from seer.automation.pipeline import PipelineContext",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.db import DbPrContextToUnitTestGenerationRunIdMapping, DbRunMemory, Session",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "RepoExternalId = str",
          "RepoInternalId = int",
          "RepoKey = RepoExternalId | RepoInternalId",
          "RepoIdentifiers = tuple[RepoExternalId, RepoInternalId]",
          "",
          "",
          "class CodegenContext(PipelineContext):",
          "    state: CodegenContinuationState",
          "    event_manager: CodegenEventManager",
          "    repo: RepoDefinition",
          "",
          "    def __init__(",
          "        self,",
          "        state: CodegenContinuationState,",
          "    ):",
          "        request = state.get().request",
          "",
          "        self.repo = request.repo",
          "        self.state = state",
          "        self.event_manager = CodegenEventManager(state)",
          "",
          "        logger.info(f\"CodegenContext initialized with run_id {self.run_id}\")",
          "",
          "    @classmethod",
          "    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):",
          "        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)",
          "        with state.update() as cur:",
          "            cur.mark_triggered()",
          "",
          "        return cls(state)",
          "",
          "    @property",
          "    def run_id(self) -> int:",
          "        return self.state.get().run_id",
          "",
          "    @property",
          "    def signals(self) -> list[str]:",
          "        return self.state.get().signals",
          "",
          "    @signals.setter",
          "    def signals(self, value: list[str]):",
          "        with self.state.update() as state:",
          "            state.signals = value",
          "",
          "    def get_repo_client(",
          "        self, repo_name: str | None = None, type: RepoClientType = RepoClientType.READ",
          "    ):",
          "        \"\"\"",
          "        Gets a repo client for the current single repo or for a given repo name.",
          "        If there are more than 1 repos, a repo name must be provided.",
          "        \"\"\"",
          "        return RepoClient.from_repo_definition(self.repo, type)",
          "",
          "    def get_file_contents(",
          "        self, path: str, repo_name: str | None = None, ignore_local_changes: bool = False",
          "    ) -> str | None:",
          "        repo_client = self.get_repo_client()",
          "",
          "        file_contents, _ = repo_client.get_file_content(path)",
          "",
          "        if not ignore_local_changes:",
          "            cur_state = self.state.get()",
          "            current_file_changes = list(filter(lambda x: x.path == path, cur_state.file_changes))",
          "            for file_change in current_file_changes:",
          "                file_contents = file_change.apply(file_contents)",
          "",
          "        return file_contents",
          "",
          "    def store_memory(self, key: str, memory: list[Message]):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == self.run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                memory_model = UnitTestRunMemory(run_id=self.run_id)",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def update_stored_memory(self, key: str, memory: list[Message], original_run_id: int):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory)",
          "                .where(DbRunMemory.run_id == original_run_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                raise RuntimeError(",
          "                    f\"No memory record found for run_id {original_run_id}. Cannot update stored memory.\"",
          "                )",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def get_memory(self, key: str, past_run_id: int) -> list[Message]:",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == past_run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                return []",
          "",
          "            return UnitTestRunMemory.from_db_model(memory_record).memory.get(key, [])",
          "",
          "    def get_previous_run_context(",
          "        self, owner: str, repo: str, pr_id: int",
          "    ) -> DbPrContextToUnitTestGenerationRunIdMapping | None:",
          "        with Session() as session:",
          "            previous_context = (",
          "                session.query(DbPrContextToUnitTestGenerationRunIdMapping)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.owner == owner)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.repo == repo)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.pr_id == pr_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            return previous_context",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/state.py",
      "image": "seer.automation.state",
      "is_application": true,
      "line": 94,
      "name": "DbState.get",
      "path": "/app/src/seer/automation/state.py",
      "codeContext": {
        "file": "seer/automation/state.py",
        "line": 94,
        "name": "DbState.get",
        "code": "            session.flush()\n            value.run_id = db_state.id\n            db_state.value = value.model_dump(mode=\"json\")\n            session.merge(db_state)\n            session.commit()\n            return cls(id=db_state.id, model=type(value), type=t)\n\n    def get(self) -> _State:\n        with Session() as session:\n            db_state = session.get(DbRunState, self.id)\n            self.validate(db_state)\n            assert db_state\n            return self.model.model_validate(db_state.value)\n\n    def validate(self, db_state: DbRunState | None):\n        if db_state is None:\n            raise ValueError(f\"No state found for id {self.id}\")\n        if db_state.type != self.type:\n            raise ValueError(f\"Invalid state type: '{db_state.type}', expected: '{self.type}'\")\n",
        "lineRange": {
          "start": 85,
          "end": 104
        },
        "lines": [
          "import abc",
          "import contextlib",
          "import dataclasses",
          "import functools",
          "import threading",
          "from enum import Enum",
          "from typing import Any, ContextManager, Generic, Iterator, Type, TypeVar",
          "",
          "from pydantic import BaseModel",
          "from sqlalchemy import select",
          "",
          "from seer.db import DbRunState, Session",
          "",
          "_State = TypeVar(\"_State\", bound=BaseModel)",
          "_StateB = TypeVar(\"_StateB\", bound=BaseModel)",
          "",
          "",
          "class DbStateRunTypes(str, Enum):",
          "    AUTOFIX = \"autofix\"",
          "    UNIT_TEST = \"unit-test\"",
          "    PR_REVIEW = \"pr-review\"",
          "    RELEVANT_WARNINGS = \"relevant-warnings\"",
          "    PR_CLOSED = \"pr-closed\"",
          "    UNIT_TESTS_RETRY = \"unit-test-retry\"",
          "",
          "",
          "class State(abc.ABC, Generic[_State]):",
          "    \"\"\"",
          "    An abstract state buffer that attempts to push state changes to a sink.",
          "    \"\"\"",
          "",
          "    @abc.abstractmethod",
          "    def get(self) -> _State:",
          "        \"\"\"",
          "        A method to return a locally secure copy of the state.  Note that mutations to this",
          "        value are not likely to be reflected, see `update`.",
          "        \"\"\"",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def update(self) -> ContextManager[_State]:",
          "        \"\"\"",
          "        A method to atomically get and mutate a state value.  Subclasses should implement",
          "        concurrency primitives around this method to ensure that concurrent access is limited",
          "        by the context of the update itself.",
          "        :return:",
          "        \"\"\"",
          "        pass",
          "",
          "",
          "@dataclasses.dataclass",
          "class LocalMemoryState(State[_State]):",
          "    val: _State",
          "    lock: threading.RLock = dataclasses.field(default_factory=threading.RLock)",
          "",
          "    def get(self) -> _State:",
          "        return self.val",
          "",
          "    @contextlib.contextmanager",
          "    def update(self):",
          "        with self.lock:",
          "            val = self.get()",
          "            yield val",
          "            # Mostly a no-op, except in the case that `get` has semantics copying",
          "            self.val = val",
          "",
          "",
          "@dataclasses.dataclass",
          "class DbState(State[_State]):",
          "    \"\"\"",
          "    State that is stored in postgres: DbRunState model.",
          "    \"\"\"",
          "",
          "    id: int",
          "    model: Type[_State]",
          "    type: DbStateRunTypes",
          "",
          "    @classmethod",
          "    def new(",
          "        cls, value: _State, *, group_id: int | None = None, t: DbStateRunTypes",
          "    ) -> \"DbState[_State]\":",
          "        with Session() as session:",
          "            db_state = DbRunState(value=value.model_dump(mode=\"json\"), group_id=group_id, type=t)",
          "            session.add(db_state)",
          "            session.flush()",
          "            value.run_id = db_state.id",
          "            db_state.value = value.model_dump(mode=\"json\")",
          "            session.merge(db_state)",
          "            session.commit()",
          "            return cls(id=db_state.id, model=type(value), type=t)",
          "",
          "    def get(self) -> _State:",
          "        with Session() as session:",
          "            db_state = session.get(DbRunState, self.id)",
          "            self.validate(db_state)",
          "            assert db_state",
          "            return self.model.model_validate(db_state.value)",
          "",
          "    def validate(self, db_state: DbRunState | None):",
          "        if db_state is None:",
          "            raise ValueError(f\"No state found for id {self.id}\")",
          "        if db_state.type != self.type:",
          "            raise ValueError(f\"Invalid state type: '{db_state.type}', expected: '{self.type}'\")",
          "",
          "    def apply_to_run_state(self, value: _State, run_state: DbRunState):",
          "        \"\"\"",
          "        Can be used to pass down context from state into the db context",
          "        \"\"\"",
          "        pass",
          "",
          "    def before_update(self, value: _State):",
          "        \"\"\"",
          "        Can be used to run some logic before the update is applied to the db",
          "        \"\"\"",
          "        pass",
          "",
          "    @contextlib.contextmanager",
          "    def update(self):",
          "        \"\"\"",
          "        Uses a 'with for update' clause on the db run id, ensuring it is safe against concurrent transactions.",
          "        Note however, that if you have two competing updates in which neither can fully complete (say a circle",
          "        of inter related locks), the database may reach a deadlock state which last until the lock timeout configured",
          "        on the postgres database.",
          "        \"\"\"",
          "        with Session() as session:",
          "            r = session.execute(",
          "                select(DbRunState).where(DbRunState.id == self.id).with_for_update()",
          "            ).scalar_one_or_none()",
          "            self.validate(r)",
          "            assert r",
          "            value = self.model.model_validate(r.value)",
          "            yield value",
          "            self.before_update(value)",
          "            db_state = DbRunState(id=self.id, value=value.model_dump(mode=\"json\"))",
          "            self.apply_to_run_state(value, db_state)",
          "            session.merge(db_state)",
          "            session.commit()",
          "",
          "",
          "@functools.total_ordering",
          "class BufferedMemoryState(State[_State]):",
          "    values: list[_State] = dataclasses.field(default_factory=list)",
          "    lock: threading.RLock = dataclasses.field(default_factory=threading.RLock)",
          "",
          "    def __init__(self, initial: _State):",
          "        self.values.append(initial)",
          "",
          "    def get(self) -> _State:",
          "        return self.values[-1]",
          "",
          "    def __eq__(self, other: Any) -> bool:",
          "        return self.values[-1] == other",
          "",
          "    def __lt__(self, other: Any) -> bool:",
          "        return self.values[-1] < other",
          "",
          "    def __hash__(self) -> int:",
          "        return hash(self.values[-1])",
          "",
          "    def __contains__(self, other: Any) -> bool:",
          "        return other in self.values",
          "",
          "    def __iter__(self) -> Iterator[_State]:",
          "        return iter(self.values)",
          "",
          "    @contextlib.contextmanager",
          "    def update(self):",
          "        with self.lock:",
          "            value = self.get()",
          "            yield value",
          "            self.values.append(value)",
          ""
        ]
      }
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 3653,
      "name": "Session.get",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 3833,
      "name": "Session._get_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/loading.py",
      "image": "sqlalchemy.orm.loading",
      "is_application": false,
      "line": 692,
      "name": "load_on_pk_identity",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/loading.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 815,
      "name": "PGresult.fsize",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 232,
      "name": "PGconn.socket",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "sqlalchemy/engine/result.py",
      "image": "sqlalchemy.engine.result",
      "is_application": false,
      "line": 1824,
      "name": "ScalarResult.one",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/result.py"
    },
    {
      "file": "sqlalchemy/engine/result.py",
      "image": "sqlalchemy.engine.result",
      "is_application": false,
      "line": 1687,
      "name": "FilterResult._fetchone_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/result.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 1750,
      "name": "Session.__exit__",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 2468,
      "name": "Session.close",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 2537,
      "name": "Session._close_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "<string>",
      "image": "sqlalchemy.orm.session",
      "is_application": true,
      "line": 2,
      "name": "close",
      "path": "/app/<string>"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 1362,
      "name": "SessionTransaction.close",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 2577,
      "name": "Transaction.close",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 2715,
      "name": "RootTransaction._do_close",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 2701,
      "name": "RootTransaction._close_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 2693,
      "name": "RootTransaction._connection_rollback_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 1119,
      "name": "Connection._rollback_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 563,
      "name": "BaseConnection._rollback_gen",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "seer/automation/codegen/codegen_context.py",
      "image": "seer.automation.codegen.codegen_context",
      "is_application": true,
      "line": 48,
      "name": "CodegenContext.run_id",
      "path": "/app/src/seer/automation/codegen/codegen_context.py",
      "codeContext": {
        "file": "seer/automation/codegen/codegen_context.py",
        "line": 48,
        "name": "CodegenContext.run_id",
        "code": "    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):\n        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)\n        with state.update() as cur:\n            cur.mark_triggered()\n\n        return cls(state)\n\n    @property\n    def run_id(self) -> int:\n        return self.state.get().run_id\n\n    @property\n    def signals(self) -> list[str]:\n        return self.state.get().signals\n\n    @signals.setter\n    def signals(self, value: list[str]):\n        with self.state.update() as state:\n            state.signals = value\n",
        "lineRange": {
          "start": 39,
          "end": 58
        },
        "lines": [
          "import logging",
          "",
          "from seer.automation.agent.models import Message",
          "from seer.automation.codebase.repo_client import RepoClient, RepoClientType",
          "from seer.automation.codegen.codegen_event_manager import CodegenEventManager",
          "from seer.automation.codegen.models import CodegenContinuation, UnitTestRunMemory",
          "from seer.automation.codegen.state import CodegenContinuationState",
          "from seer.automation.models import RepoDefinition",
          "from seer.automation.pipeline import PipelineContext",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.db import DbPrContextToUnitTestGenerationRunIdMapping, DbRunMemory, Session",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "RepoExternalId = str",
          "RepoInternalId = int",
          "RepoKey = RepoExternalId | RepoInternalId",
          "RepoIdentifiers = tuple[RepoExternalId, RepoInternalId]",
          "",
          "",
          "class CodegenContext(PipelineContext):",
          "    state: CodegenContinuationState",
          "    event_manager: CodegenEventManager",
          "    repo: RepoDefinition",
          "",
          "    def __init__(",
          "        self,",
          "        state: CodegenContinuationState,",
          "    ):",
          "        request = state.get().request",
          "",
          "        self.repo = request.repo",
          "        self.state = state",
          "        self.event_manager = CodegenEventManager(state)",
          "",
          "        logger.info(f\"CodegenContext initialized with run_id {self.run_id}\")",
          "",
          "    @classmethod",
          "    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):",
          "        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)",
          "        with state.update() as cur:",
          "            cur.mark_triggered()",
          "",
          "        return cls(state)",
          "",
          "    @property",
          "    def run_id(self) -> int:",
          "        return self.state.get().run_id",
          "",
          "    @property",
          "    def signals(self) -> list[str]:",
          "        return self.state.get().signals",
          "",
          "    @signals.setter",
          "    def signals(self, value: list[str]):",
          "        with self.state.update() as state:",
          "            state.signals = value",
          "",
          "    def get_repo_client(",
          "        self, repo_name: str | None = None, type: RepoClientType = RepoClientType.READ",
          "    ):",
          "        \"\"\"",
          "        Gets a repo client for the current single repo or for a given repo name.",
          "        If there are more than 1 repos, a repo name must be provided.",
          "        \"\"\"",
          "        return RepoClient.from_repo_definition(self.repo, type)",
          "",
          "    def get_file_contents(",
          "        self, path: str, repo_name: str | None = None, ignore_local_changes: bool = False",
          "    ) -> str | None:",
          "        repo_client = self.get_repo_client()",
          "",
          "        file_contents, _ = repo_client.get_file_content(path)",
          "",
          "        if not ignore_local_changes:",
          "            cur_state = self.state.get()",
          "            current_file_changes = list(filter(lambda x: x.path == path, cur_state.file_changes))",
          "            for file_change in current_file_changes:",
          "                file_contents = file_change.apply(file_contents)",
          "",
          "        return file_contents",
          "",
          "    def store_memory(self, key: str, memory: list[Message]):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == self.run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                memory_model = UnitTestRunMemory(run_id=self.run_id)",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def update_stored_memory(self, key: str, memory: list[Message], original_run_id: int):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory)",
          "                .where(DbRunMemory.run_id == original_run_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                raise RuntimeError(",
          "                    f\"No memory record found for run_id {original_run_id}. Cannot update stored memory.\"",
          "                )",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def get_memory(self, key: str, past_run_id: int) -> list[Message]:",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == past_run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                return []",
          "",
          "            return UnitTestRunMemory.from_db_model(memory_record).memory.get(key, [])",
          "",
          "    def get_previous_run_context(",
          "        self, owner: str, repo: str, pr_id: int",
          "    ) -> DbPrContextToUnitTestGenerationRunIdMapping | None:",
          "        with Session() as session:",
          "            previous_context = (",
          "                session.query(DbPrContextToUnitTestGenerationRunIdMapping)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.owner == owner)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.repo == repo)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.pr_id == pr_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            return previous_context",
          ""
        ]
      }
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 212,
      "name": "PGconn.transaction_status",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 812,
      "name": "PGresult.fmod",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 533,
      "name": "PGconn.is_busy",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "contextlib.py",
      "image": "contextlib",
      "is_application": false,
      "line": 144,
      "name": "_GeneratorContextManager.__exit__",
      "path": "/usr/lib/python3.11/contextlib.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 1969,
      "name": "Session.commit",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "<string>",
      "image": "sqlalchemy.orm.session",
      "is_application": true,
      "line": 2,
      "name": "commit",
      "path": "/app/<string>"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 1263,
      "name": "SessionTransaction.commit",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 2620,
      "name": "Transaction.commit",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 2725,
      "name": "RootTransaction._do_commit",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 2696,
      "name": "RootTransaction._connection_commit_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 1136,
      "name": "Connection._commit_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/engine/default.py",
      "image": "sqlalchemy.engine.default",
      "is_application": false,
      "line": 695,
      "name": "DefaultDialect.do_commit",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 896,
      "name": "Connection.commit",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 538,
      "name": "BaseConnection._commit_gen",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "<string>",
      "image": "sqlalchemy.orm.session",
      "is_application": true,
      "line": 2,
      "name": "_prepare_impl",
      "path": "/app/<string>"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 1231,
      "name": "SessionTransaction._prepare_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 4312,
      "name": "Session.flush",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/session.py",
      "image": "sqlalchemy.orm.session",
      "is_application": false,
      "line": 4408,
      "name": "Session._flush",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py"
    },
    {
      "file": "sqlalchemy/orm/unitofwork.py",
      "image": "sqlalchemy.orm.unitofwork",
      "is_application": false,
      "line": 466,
      "name": "UOWTransaction.execute",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/unitofwork.py"
    },
    {
      "file": "sqlalchemy/orm/unitofwork.py",
      "image": "sqlalchemy.orm.unitofwork",
      "is_application": false,
      "line": 642,
      "name": "SaveUpdateAll.execute",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/unitofwork.py"
    },
    {
      "file": "sqlalchemy/orm/persistence.py",
      "image": "sqlalchemy.orm.persistence",
      "is_application": false,
      "line": 85,
      "name": "save_obj",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/persistence.py"
    },
    {
      "file": "sqlalchemy/orm/persistence.py",
      "image": "sqlalchemy.orm.persistence",
      "is_application": false,
      "line": 910,
      "name": "_emit_update_statements",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/persistence.py"
    },
    {
      "file": "seer/automation/pipeline.py",
      "image": "seer.automation.pipeline",
      "is_application": true,
      "line": 76,
      "name": "PipelineStep.invoke",
      "path": "/app/src/seer/automation/pipeline.py",
      "codeContext": {
        "file": "seer/automation/pipeline.py",
        "line": 76,
        "name": "PipelineStep.invoke",
        "code": "    request: _RequestType\n    context: _ContextType\n\n    def __init__(self, request: dict[str, Any], type: DbStateRunTypes | None = None):\n        self.request = self._instantiate_request(request)\n        self.context = self._instantiate_context(self.request, type)\n\n    def invoke(self) -> Any:\n        try:\n            if not self._pre_invoke():\n                self._cleanup()\n                return\n            result = self._invoke(**self._get_extra_invoke_kwargs())\n            self._post_invoke(result)\n            return result\n        except Exception as e:\n            self._handle_exception(e)\n            raise e\n        finally:\n            self._cleanup()",
        "lineRange": {
          "start": 67,
          "end": 86
        },
        "lines": [
          "import abc",
          "import logging",
          "import uuid",
          "from functools import cached_property",
          "from typing import Any, Generic, TypeVar",
          "",
          "from celery import Task, signature",
          "from pydantic import BaseModel, Field",
          "",
          "from seer.automation.state import DbStateRunTypes, State",
          "from seer.utils import prefix_logger",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "Signature = Any",
          "SerializedSignature = Any",
          "",
          "DEFAULT_PIPELINE_STEP_SOFT_TIME_LIMIT_SECS = 50  # 50 seconds",
          "DEFAULT_PIPELINE_STEP_HARD_TIME_LIMIT_SECS = 60  # 60 seconds",
          "",
          "PIPELINE_SYNC_SIGNAL = \"pipeline_run_mode:sync\"",
          "",
          "",
          "class PipelineContext(abc.ABC):",
          "    state: State",
          "",
          "    def __init__(self, state: State):",
          "        self.state = state",
          "",
          "    @property",
          "    @abc.abstractmethod",
          "    def run_id(self) -> int:",
          "        pass",
          "",
          "    @property",
          "    @abc.abstractmethod",
          "    def signals(self) -> list[str]:",
          "        pass",
          "",
          "    @signals.setter",
          "    @abc.abstractmethod",
          "    def signals(self, signals: list[str]):",
          "        pass",
          "",
          "",
          "class PipelineStepTaskRequest(BaseModel):",
          "    run_id: int",
          "    step_id: int = Field(default_factory=lambda: uuid.uuid4().int)",
          "",
          "",
          "def make_step_request_fields(context: PipelineContext):",
          "    return {\"run_id\": context.run_id}",
          "",
          "",
          "# Define a type variable that is bound to PipelineStepTaskRequest",
          "_RequestType = TypeVar(\"_RequestType\", bound=PipelineStepTaskRequest)",
          "_ContextType = TypeVar(\"_ContextType\", bound=PipelineContext)",
          "",
          "",
          "class PipelineStep(abc.ABC, Generic[_RequestType, _ContextType]):",
          "    \"\"\"",
          "    A step in the automation pipeline, complete with the context, request, logging + error handling utils.",
          "    Main method that is run is _invoke, which should be implemented by the subclass.",
          "    \"\"\"",
          "",
          "    name = \"PipelineStep\"",
          "    request: _RequestType",
          "    context: _ContextType",
          "",
          "    def __init__(self, request: dict[str, Any], type: DbStateRunTypes | None = None):",
          "        self.request = self._instantiate_request(request)",
          "        self.context = self._instantiate_context(self.request, type)",
          "",
          "    def invoke(self) -> Any:",
          "        try:",
          "            if not self._pre_invoke():",
          "                self._cleanup()",
          "                return",
          "            result = self._invoke(**self._get_extra_invoke_kwargs())",
          "            self._post_invoke(result)",
          "            return result",
          "        except Exception as e:",
          "            self._handle_exception(e)",
          "            raise e",
          "        finally:",
          "            self._cleanup()",
          "",
          "    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
          "        return {}",
          "",
          "    def _pre_invoke(self) -> bool:",
          "        return True",
          "",
          "    def _post_invoke(self, result: Any) -> Any:",
          "        pass",
          "",
          "    def _cleanup(self):",
          "        pass",
          "",
          "    @cached_property",
          "    def logger(self):",
          "        run_id = self.context.run_id",
          "        name = self.name",
          "        prefix = f\"[{run_id=}] [{name}] \"",
          "        return prefix_logger(prefix, logger)",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def get_task() -> Task:",
          "        pass",
          "",
          "    @classmethod",
          "    def get_signature(cls, request: PipelineStepTaskRequest, **kwargs) -> Signature:",
          "        return cls.get_task().signature(",
          "            kwargs={\"request\": request.model_dump(mode=\"json\")}, **kwargs",
          "        )",
          "",
          "    @staticmethod",
          "    def instantiate_signature(serialized_signature: SerializedSignature | Signature) -> Signature:",
          "        return signature(serialized_signature)",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> _RequestType:",
          "        pass",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def _instantiate_context(",
          "        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None",
          "    ) -> _ContextType:",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def _handle_exception(self, exception: Exception):",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def _invoke(self, **kwargs) -> Any:",
          "        pass",
          "",
          "    @property",
          "    def step_request_fields(self):",
          "        return make_step_request_fields(self.context)",
          "",
          "",
          "class PipelineChain(PipelineStep):",
          "    \"\"\"",
          "    A PipelineStep which can call other steps.",
          "    \"\"\"",
          "",
          "    def next(self, sig: SerializedSignature | Signature, **apply_async_kwargs):",
          "        if PIPELINE_SYNC_SIGNAL in self.context.signals:",
          "            signature(sig).apply(**apply_async_kwargs)",
          "        else:",
          "            signature(sig).apply_async(**apply_async_kwargs)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/step.py",
      "image": "seer.automation.codegen.step",
      "is_application": true,
      "line": 29,
      "name": "CodegenStep._pre_invoke",
      "path": "/app/src/seer/automation/codegen/step.py",
      "codeContext": {
        "file": "seer/automation/codegen/step.py",
        "line": 29,
        "name": "CodegenStep._pre_invoke",
        "code": "            type = DbStateRunTypes.UNIT_TEST\n        return CodegenContext.from_run_id(request.run_id, type=type)\n\n    def _invoke(self, **kwargs: Any) -> Any:\n        sentry_sdk.set_tag(\"run_id\", self.context.run_id)\n        super()._invoke(**kwargs)\n\n    def _pre_invoke(self) -> bool:\n        done_signal = make_done_signal(self.request.step_id)\n        return done_signal not in self.context.signals\n\n    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:\n        try:\n            current_state = self.context.state.get()\n            repo = self.context.repo\n\n            tags = {\n                \"run_id\": current_state.run_id,\n                \"repo\": repo.full_name,\n                \"repo_id\": repo.external_id,",
        "lineRange": {
          "start": 20,
          "end": 39
        },
        "lines": [
          "from typing import Any",
          "",
          "import sentry_sdk",
          "",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import CodegenStatus",
          "from seer.automation.pipeline import PipelineContext, PipelineStep, PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.automation.utils import make_done_signal",
          "",
          "",
          "class CodegenStep(PipelineStep):",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _instantiate_context(",
          "        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None",
          "    ) -> PipelineContext:",
          "        if type is None:",
          "            type = DbStateRunTypes.UNIT_TEST",
          "        return CodegenContext.from_run_id(request.run_id, type=type)",
          "",
          "    def _invoke(self, **kwargs: Any) -> Any:",
          "        sentry_sdk.set_tag(\"run_id\", self.context.run_id)",
          "        super()._invoke(**kwargs)",
          "",
          "    def _pre_invoke(self) -> bool:",
          "        done_signal = make_done_signal(self.request.step_id)",
          "        return done_signal not in self.context.signals",
          "",
          "    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
          "        try:",
          "            current_state = self.context.state.get()",
          "            repo = self.context.repo",
          "",
          "            tags = {",
          "                \"run_id\": current_state.run_id,",
          "                \"repo\": repo.full_name,",
          "                \"repo_id\": repo.external_id,",
          "            }",
          "",
          "            metadata = {\"run_id\": current_state.run_id, \"repo\": repo}",
          "            langfuse_tags = [f\"{key}:{value}\" for key, value in tags.items() if value is not None]",
          "",
          "            return {",
          "                \"langfuse_tags\": langfuse_tags,",
          "                \"langfuse_metadata\": metadata,",
          "                \"langfuse_session_id\": str(current_state.run_id),",
          "                \"sentry_tags\": tags,",
          "                \"sentry_data\": metadata,",
          "            }",
          "        except Exception:",
          "            return {}",
          "",
          "    def _post_invoke(self, result: Any):",
          "        with self.context.state.update() as current_state:",
          "            signal = make_done_signal(self.request.step_id)",
          "            current_state.signals.append(signal)",
          "",
          "    def _handle_exception(self, exception: Exception):",
          "        self.logger.error(f\"Failed to run {self.request.step_id}. Error: {str(exception)}\")",
          "",
          "        with self.context.state.update() as current_state:",
          "            current_state.status = CodegenStatus.ERRORED",
          "            sentry_sdk.set_context(\"codegen_state\", current_state.dict())",
          "            sentry_sdk.capture_exception(exception)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/codegen_context.py",
      "image": "seer.automation.codegen.codegen_context",
      "is_application": true,
      "line": 52,
      "name": "CodegenContext.signals",
      "path": "/app/src/seer/automation/codegen/codegen_context.py",
      "codeContext": {
        "file": "seer/automation/codegen/codegen_context.py",
        "line": 52,
        "name": "CodegenContext.signals",
        "code": "\n        return cls(state)\n\n    @property\n    def run_id(self) -> int:\n        return self.state.get().run_id\n\n    @property\n    def signals(self) -> list[str]:\n        return self.state.get().signals\n\n    @signals.setter\n    def signals(self, value: list[str]):\n        with self.state.update() as state:\n            state.signals = value\n\n    def get_repo_client(\n        self, repo_name: str | None = None, type: RepoClientType = RepoClientType.READ\n    ):\n        \"\"\"",
        "lineRange": {
          "start": 43,
          "end": 62
        },
        "lines": [
          "import logging",
          "",
          "from seer.automation.agent.models import Message",
          "from seer.automation.codebase.repo_client import RepoClient, RepoClientType",
          "from seer.automation.codegen.codegen_event_manager import CodegenEventManager",
          "from seer.automation.codegen.models import CodegenContinuation, UnitTestRunMemory",
          "from seer.automation.codegen.state import CodegenContinuationState",
          "from seer.automation.models import RepoDefinition",
          "from seer.automation.pipeline import PipelineContext",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.db import DbPrContextToUnitTestGenerationRunIdMapping, DbRunMemory, Session",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "RepoExternalId = str",
          "RepoInternalId = int",
          "RepoKey = RepoExternalId | RepoInternalId",
          "RepoIdentifiers = tuple[RepoExternalId, RepoInternalId]",
          "",
          "",
          "class CodegenContext(PipelineContext):",
          "    state: CodegenContinuationState",
          "    event_manager: CodegenEventManager",
          "    repo: RepoDefinition",
          "",
          "    def __init__(",
          "        self,",
          "        state: CodegenContinuationState,",
          "    ):",
          "        request = state.get().request",
          "",
          "        self.repo = request.repo",
          "        self.state = state",
          "        self.event_manager = CodegenEventManager(state)",
          "",
          "        logger.info(f\"CodegenContext initialized with run_id {self.run_id}\")",
          "",
          "    @classmethod",
          "    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):",
          "        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)",
          "        with state.update() as cur:",
          "            cur.mark_triggered()",
          "",
          "        return cls(state)",
          "",
          "    @property",
          "    def run_id(self) -> int:",
          "        return self.state.get().run_id",
          "",
          "    @property",
          "    def signals(self) -> list[str]:",
          "        return self.state.get().signals",
          "",
          "    @signals.setter",
          "    def signals(self, value: list[str]):",
          "        with self.state.update() as state:",
          "            state.signals = value",
          "",
          "    def get_repo_client(",
          "        self, repo_name: str | None = None, type: RepoClientType = RepoClientType.READ",
          "    ):",
          "        \"\"\"",
          "        Gets a repo client for the current single repo or for a given repo name.",
          "        If there are more than 1 repos, a repo name must be provided.",
          "        \"\"\"",
          "        return RepoClient.from_repo_definition(self.repo, type)",
          "",
          "    def get_file_contents(",
          "        self, path: str, repo_name: str | None = None, ignore_local_changes: bool = False",
          "    ) -> str | None:",
          "        repo_client = self.get_repo_client()",
          "",
          "        file_contents, _ = repo_client.get_file_content(path)",
          "",
          "        if not ignore_local_changes:",
          "            cur_state = self.state.get()",
          "            current_file_changes = list(filter(lambda x: x.path == path, cur_state.file_changes))",
          "            for file_change in current_file_changes:",
          "                file_contents = file_change.apply(file_contents)",
          "",
          "        return file_contents",
          "",
          "    def store_memory(self, key: str, memory: list[Message]):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == self.run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                memory_model = UnitTestRunMemory(run_id=self.run_id)",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def update_stored_memory(self, key: str, memory: list[Message], original_run_id: int):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory)",
          "                .where(DbRunMemory.run_id == original_run_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                raise RuntimeError(",
          "                    f\"No memory record found for run_id {original_run_id}. Cannot update stored memory.\"",
          "                )",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def get_memory(self, key: str, past_run_id: int) -> list[Message]:",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == past_run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                return []",
          "",
          "            return UnitTestRunMemory.from_db_model(memory_record).memory.get(key, [])",
          "",
          "    def get_previous_run_context(",
          "        self, owner: str, repo: str, pr_id: int",
          "    ) -> DbPrContextToUnitTestGenerationRunIdMapping | None:",
          "        with Session() as session:",
          "            previous_context = (",
          "                session.query(DbPrContextToUnitTestGenerationRunIdMapping)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.owner == owner)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.repo == repo)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.pr_id == pr_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            return previous_context",
          ""
        ]
      }
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 525,
      "name": "PGconn.get_result",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 192,
      "name": "BaseConnection.autocommit",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 987,
      "name": "Connection._set_autocommit",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 198,
      "name": "BaseConnection._set_autocommit_gen",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 260,
      "name": "BaseConnection._check_intrans_gen",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "seer/automation/codegen/step.py",
      "image": "seer.automation.codegen.step",
      "is_application": true,
      "line": 33,
      "name": "CodegenStep._get_extra_invoke_kwargs",
      "path": "/app/src/seer/automation/codegen/step.py",
      "codeContext": {
        "file": "seer/automation/codegen/step.py",
        "line": 33,
        "name": "CodegenStep._get_extra_invoke_kwargs",
        "code": "        sentry_sdk.set_tag(\"run_id\", self.context.run_id)\n        super()._invoke(**kwargs)\n\n    def _pre_invoke(self) -> bool:\n        done_signal = make_done_signal(self.request.step_id)\n        return done_signal not in self.context.signals\n\n    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:\n        try:\n            current_state = self.context.state.get()\n            repo = self.context.repo\n\n            tags = {\n                \"run_id\": current_state.run_id,\n                \"repo\": repo.full_name,\n                \"repo_id\": repo.external_id,\n            }\n\n            metadata = {\"run_id\": current_state.run_id, \"repo\": repo}\n            langfuse_tags = [f\"{key}:{value}\" for key, value in tags.items() if value is not None]",
        "lineRange": {
          "start": 24,
          "end": 43
        },
        "lines": [
          "from typing import Any",
          "",
          "import sentry_sdk",
          "",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import CodegenStatus",
          "from seer.automation.pipeline import PipelineContext, PipelineStep, PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.automation.utils import make_done_signal",
          "",
          "",
          "class CodegenStep(PipelineStep):",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _instantiate_context(",
          "        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None",
          "    ) -> PipelineContext:",
          "        if type is None:",
          "            type = DbStateRunTypes.UNIT_TEST",
          "        return CodegenContext.from_run_id(request.run_id, type=type)",
          "",
          "    def _invoke(self, **kwargs: Any) -> Any:",
          "        sentry_sdk.set_tag(\"run_id\", self.context.run_id)",
          "        super()._invoke(**kwargs)",
          "",
          "    def _pre_invoke(self) -> bool:",
          "        done_signal = make_done_signal(self.request.step_id)",
          "        return done_signal not in self.context.signals",
          "",
          "    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
          "        try:",
          "            current_state = self.context.state.get()",
          "            repo = self.context.repo",
          "",
          "            tags = {",
          "                \"run_id\": current_state.run_id,",
          "                \"repo\": repo.full_name,",
          "                \"repo_id\": repo.external_id,",
          "            }",
          "",
          "            metadata = {\"run_id\": current_state.run_id, \"repo\": repo}",
          "            langfuse_tags = [f\"{key}:{value}\" for key, value in tags.items() if value is not None]",
          "",
          "            return {",
          "                \"langfuse_tags\": langfuse_tags,",
          "                \"langfuse_metadata\": metadata,",
          "                \"langfuse_session_id\": str(current_state.run_id),",
          "                \"sentry_tags\": tags,",
          "                \"sentry_data\": metadata,",
          "            }",
          "        except Exception:",
          "            return {}",
          "",
          "    def _post_invoke(self, result: Any):",
          "        with self.context.state.update() as current_state:",
          "            signal = make_done_signal(self.request.step_id)",
          "            current_state.signals.append(signal)",
          "",
          "    def _handle_exception(self, exception: Exception):",
          "        self.logger.error(f\"Failed to run {self.request.step_id}. Error: {str(exception)}\")",
          "",
          "        with self.context.state.update() as current_state:",
          "            current_state.status = CodegenStatus.ERRORED",
          "            sentry_sdk.set_context(\"codegen_state\", current_state.dict())",
          "            sentry_sdk.capture_exception(exception)",
          ""
        ]
      }
    },
    {
      "file": "langfuse/decorators/langfuse_decorator.py",
      "image": "langfuse.decorators.langfuse_decorator",
      "is_application": false,
      "line": 254,
      "name": "LangfuseDecorator._sync_observe.<locals>.sync_wrapper",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/decorators/langfuse_decorator.py"
    },
    {
      "file": "sentry_sdk/ai/monitoring.py",
      "image": "sentry_sdk.ai.monitoring",
      "is_application": false,
      "line": 47,
      "name": "ai_track.<locals>.decorator.<locals>.sync_wrapped",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/ai/monitoring.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_step.py",
      "image": "seer.automation.codegen.relevant_warnings_step",
      "is_application": true,
      "line": 136,
      "name": "RelevantWarningsStep._invoke",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_step.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_step.py",
        "line": 136,
        "name": "RelevantWarningsStep._invoke",
        "code": "            self.context.event_manager.mark_completed_and_extend_static_analysis_suggestions(\n                static_analysis_suggestions_output.suggestions\n                if static_analysis_suggestions_output\n                else []\n            )\n\n    @observe(name=\"Codegen - Relevant Warnings Step\")\n    @ai_track(description=\"Codegen - Relevant Warnings Step\")\n    def _invoke(self, **kwargs) -> None:\n        self.logger.info(\"Executing Codegen - Relevant Warnings Step\")\n        self.context.event_manager.mark_running()\n        diagnostics = []\n\n        # 1. Read the PR.\n        repo_client = self.context.get_repo_client(type=RepoClientType.READ)\n        pr_files = repo_client.repo.get_pull(self.request.pr_id).get_files()\n        pr_files = [\n            PrFile(\n                filename=file.filename,\n                patch=file.patch,",
        "lineRange": {
          "start": 127,
          "end": 146
        },
        "lines": [
          "import itertools",
          "import json",
          "import logging",
          "from typing import Any",
          "",
          "import requests",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from celery_app.app import celery_app",
          "from integrations.codecov.codecov_auth import get_codecov_auth_header",
          "from seer.automation.autofix.config import (",
          "    AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "from seer.automation.codebase.models import PrFile",
          "from seer.automation.codebase.repo_client import RepoClientType",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodegenRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          ")",
          "from seer.automation.codegen.relevant_warnings_component import (",
          "    AreIssuesFixableComponent,",
          "    AssociateWarningsWithIssuesComponent,",
          "    FetchIssuesComponent,",
          "    FilterWarningsComponent,",
          "    StaticAnalysisSuggestionsComponent,",
          ")",
          "from seer.automation.codegen.step import CodegenStep",
          "from seer.automation.pipeline import PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "@celery_app.task(",
          "    time_limit=AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    soft_time_limit=AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "def relevant_warnings_task(*args, request: dict[str, Any]):",
          "    RelevantWarningsStep(request, DbStateRunTypes.RELEVANT_WARNINGS).invoke()",
          "",
          "",
          "class RelevantWarningsStepRequest(PipelineStepTaskRequest, CodegenRelevantWarningsRequest):",
          "    pass",
          "",
          "",
          "class RelevantWarningsStep(CodegenStep):",
          "    \"\"\"",
          "    Predicts which static analysis warnings in a pull request are relevant to a past Sentry issue.",
          "    \"\"\"",
          "",
          "    name = \"RelevantWarningsStep\"",
          "    request: RelevantWarningsStepRequest",
          "    max_retries = 2",
          "",
          "    @staticmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> RelevantWarningsStepRequest:",
          "        return RelevantWarningsStepRequest.model_validate(request)",
          "",
          "    @staticmethod",
          "    def get_task():",
          "        return relevant_warnings_task",
          "",
          "    @inject",
          "    def _post_results_to_overwatch(",
          "        self,",
          "        llm_suggestions: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "        config: AppConfig = injected,",
          "    ):",
          "",
          "        if not self.request.should_post_to_overwatch:",
          "            self.logger.info(\"Skipping posting relevant warnings results to Overwatch.\")",
          "            return",
          "",
          "        # This should be a temporary solution until we can update",
          "        # Overwatch to accept the new format.",
          "        suggestions_to_overwatch_expected_format = (",
          "            [",
          "                suggestion.to_overwatch_format().model_dump()",
          "                for suggestion in llm_suggestions.suggestions",
          "            ]",
          "            if llm_suggestions",
          "            else []",
          "        )",
          "",
          "        request = {",
          "            \"run_id\": self.context.run_id,",
          "            \"results\": suggestions_to_overwatch_expected_format,",
          "            \"diagnostics\": diagnostics or [],",
          "        }",
          "        request_data = json.dumps(request, separators=(\",\", \":\")).encode(\"utf-8\")",
          "        headers = get_codecov_auth_header(",
          "            request_data,",
          "            signature_header=\"X-GEN-AI-AUTH-SIGNATURE\",",
          "            signature_secret=config.OVERWATCH_OUTGOING_SIGNATURE_SECRET,",
          "        )",
          "        requests.post(",
          "            url=self.request.callback_url,",
          "            headers=headers,",
          "            data=request_data,",
          "        ).raise_for_status()",
          "",
          "    def _complete_run(",
          "        self,",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "    ):",
          "        try:",
          "            self._post_results_to_overwatch(static_analysis_suggestions_output, diagnostics)",
          "        except Exception:",
          "            self.logger.exception(\"Error posting relevant warnings results to Overwatch\")",
          "            raise",
          "        finally:",
          "            self.context.event_manager.mark_completed_and_extend_static_analysis_suggestions(",
          "                static_analysis_suggestions_output.suggestions",
          "                if static_analysis_suggestions_output",
          "                else []",
          "            )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings Step\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings Step\")",
          "    def _invoke(self, **kwargs) -> None:",
          "        self.logger.info(\"Executing Codegen - Relevant Warnings Step\")",
          "        self.context.event_manager.mark_running()",
          "        diagnostics = []",
          "",
          "        # 1. Read the PR.",
          "        repo_client = self.context.get_repo_client(type=RepoClientType.READ)",
          "        pr_files = repo_client.repo.get_pull(self.request.pr_id).get_files()",
          "        pr_files = [",
          "            PrFile(",
          "                filename=file.filename,",
          "                patch=file.patch,",
          "                status=file.status,",
          "                changes=file.changes,",
          "                sha=file.sha,",
          "            )",
          "            for file in pr_files",
          "            if file.patch",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Read PR\",",
          "                \"pr_files\": [pr_file.filename for pr_file in pr_files],",
          "                \"warnings\": [warning.id for warning in self.request.warnings],",
          "            }",
          "        )",
          "",
          "        # 2. Only consider warnings from lines changed in the PR.",
          "        filter_warnings_component = FilterWarningsComponent(self.context)",
          "        filter_warnings_request = FilterWarningsRequest(",
          "            warnings=self.request.warnings, pr_files=pr_files",
          "        )",
          "        filter_warnings_output: FilterWarningsOutput = filter_warnings_component.invoke(",
          "            filter_warnings_request",
          "        )",
          "        warning_and_pr_files = filter_warnings_output.warning_and_pr_files",
          "",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Filter Warnings Component\",",
          "                \"filtered_warning_and_pr_files\": [",
          "                    [item.warning.id, item.pr_file.filename]",
          "                    for item in filter_warnings_output.warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "        if not warning_and_pr_files:  # exit early to avoid unnecessary issue-fetching.",
          "            self.logger.info(\"No warnings to predict relevancy for.\")",
          "            self._complete_run(None, diagnostics)",
          "            return",
          "",
          "        # 3. Fetch issues related to the PR.",
          "        fetch_issues_component = FetchIssuesComponent(self.context)",
          "        fetch_issues_request = CodeFetchIssuesRequest(",
          "            organization_id=self.request.organization_id, pr_files=pr_files",
          "        )",
          "        fetch_issues_output: CodeFetchIssuesOutput = fetch_issues_component.invoke(",
          "            fetch_issues_request",
          "        )",
          "        # Clamp issue to max_num_issues_analyzed",
          "        all_selected_issues = list(",
          "            itertools.chain.from_iterable(fetch_issues_output.filename_to_issues.values())",
          "        )",
          "        all_selected_issues = all_selected_issues[: self.request.max_num_issues_analyzed]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Fetch Issues Component\",",
          "                \"all_selected_issues\": [issue.id for issue in all_selected_issues],",
          "            }",
          "        )",
          "",
          "        # 4. Limit the number of warning-issue associations we analyze to the top",
          "        #    max_num_associations.",
          "        association_component = AssociateWarningsWithIssuesComponent(self.context)",
          "        associations_request = AssociateWarningsWithIssuesRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            filename_to_issues=fetch_issues_output.filename_to_issues,",
          "            max_num_associations=self.request.max_num_associations,",
          "        )",
          "        associations_output: AssociateWarningsWithIssuesOutput = association_component.invoke(",
          "            associations_request",
          "        )",
          "        # Annotate the warnings with potential issues associated",
          "        for association in associations_output.candidate_associations:",
          "            assoc_warning, assoc_issue = association",
          "            warning_from_list = next(",
          "                (w for w in warning_and_pr_files if w.warning.id == assoc_warning.warning.id), None",
          "            )",
          "            if warning_from_list:",
          "                if isinstance(warning_from_list.warning.potentially_related_issue_titles, list):",
          "                    warning_from_list.warning.potentially_related_issue_titles.append(",
          "                        assoc_issue.title",
          "                    )",
          "                else:",
          "                    warning_from_list.warning.potentially_related_issue_titles = [assoc_issue.title]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Associate Warnings With Issues Component\",",
          "                \"candidate_associations\": [",
          "                    {",
          "                        \"warning_id\": association[0].warning.id,",
          "                        \"pr_file\": association[0].pr_file.filename,",
          "                        \"issue_id\": association[1].id,",
          "                    }",
          "                    for association in associations_output.candidate_associations",
          "                ],",
          "                \"potentially_related_issue_titles\": [",
          "                    {",
          "                        \"warning_id\": warning_and_pr_file.warning.id,",
          "                        \"potentially_related_issue_titles\": warning_and_pr_file.warning.potentially_related_issue_titles,",
          "                        \"pr_file\": warning_and_pr_file.pr_file.filename,",
          "                    }",
          "                    for warning_and_pr_file in warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "",
          "        # 5. Filter out unfixable issues b/c it doesn't make much sense to raise suggestions for issues you can't fix.",
          "        are_issues_fixable_component = AreIssuesFixableComponent(self.context)",
          "        are_fixable_output: CodeAreIssuesFixableOutput = are_issues_fixable_component.invoke(",
          "            CodeAreIssuesFixableRequest(",
          "                candidate_issues=all_selected_issues,",
          "                max_num_issues_analyzed=self.request.max_num_issues_analyzed,",
          "            )",
          "        )",
          "        fixable_issues = [",
          "            issue",
          "            for issue, is_fixable in zip(",
          "                all_selected_issues,",
          "                are_fixable_output.are_fixable,",
          "                strict=True,",
          "            )",
          "            if is_fixable",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Are Issues Fixable Component\",",
          "                \"fixable_issues\": [issue.id for issue in fixable_issues],",
          "            }",
          "        )",
          "",
          "        # 6. Suggest issues based on static analysis warnings and fixable issues.",
          "        static_analysis_suggestions_component = StaticAnalysisSuggestionsComponent(self.context)",
          "        static_analysis_suggestions_request = CodePredictStaticAnalysisSuggestionsRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            fixable_issues=fixable_issues,",
          "            pr_files=pr_files,",
          "        )",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput = (",
          "            static_analysis_suggestions_component.invoke(static_analysis_suggestions_request)",
          "        )",
          "",
          "        # 7. Save results.",
          "        self._complete_run(static_analysis_suggestions_output, diagnostics)",
          ""
        ]
      }
    },
    {
      "file": "functools.py",
      "image": "functools",
      "is_application": false,
      "line": 1001,
      "name": "cached_property.__get__",
      "path": "/usr/lib/python3.11/functools.py"
    },
    {
      "file": "seer/automation/pipeline.py",
      "image": "seer.automation.pipeline",
      "is_application": true,
      "line": 102,
      "name": "PipelineStep.logger",
      "path": "/app/src/seer/automation/pipeline.py",
      "codeContext": {
        "file": "seer/automation/pipeline.py",
        "line": 102,
        "name": "PipelineStep.logger",
        "code": "\n    def _post_invoke(self, result: Any) -> Any:\n        pass\n\n    def _cleanup(self):\n        pass\n\n    @cached_property\n    def logger(self):\n        run_id = self.context.run_id\n        name = self.name\n        prefix = f\"[{run_id=}] [{name}] \"\n        return prefix_logger(prefix, logger)\n\n    @staticmethod\n    @abc.abstractmethod\n    def get_task() -> Task:\n        pass\n\n    @classmethod",
        "lineRange": {
          "start": 93,
          "end": 112
        },
        "lines": [
          "import abc",
          "import logging",
          "import uuid",
          "from functools import cached_property",
          "from typing import Any, Generic, TypeVar",
          "",
          "from celery import Task, signature",
          "from pydantic import BaseModel, Field",
          "",
          "from seer.automation.state import DbStateRunTypes, State",
          "from seer.utils import prefix_logger",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "Signature = Any",
          "SerializedSignature = Any",
          "",
          "DEFAULT_PIPELINE_STEP_SOFT_TIME_LIMIT_SECS = 50  # 50 seconds",
          "DEFAULT_PIPELINE_STEP_HARD_TIME_LIMIT_SECS = 60  # 60 seconds",
          "",
          "PIPELINE_SYNC_SIGNAL = \"pipeline_run_mode:sync\"",
          "",
          "",
          "class PipelineContext(abc.ABC):",
          "    state: State",
          "",
          "    def __init__(self, state: State):",
          "        self.state = state",
          "",
          "    @property",
          "    @abc.abstractmethod",
          "    def run_id(self) -> int:",
          "        pass",
          "",
          "    @property",
          "    @abc.abstractmethod",
          "    def signals(self) -> list[str]:",
          "        pass",
          "",
          "    @signals.setter",
          "    @abc.abstractmethod",
          "    def signals(self, signals: list[str]):",
          "        pass",
          "",
          "",
          "class PipelineStepTaskRequest(BaseModel):",
          "    run_id: int",
          "    step_id: int = Field(default_factory=lambda: uuid.uuid4().int)",
          "",
          "",
          "def make_step_request_fields(context: PipelineContext):",
          "    return {\"run_id\": context.run_id}",
          "",
          "",
          "# Define a type variable that is bound to PipelineStepTaskRequest",
          "_RequestType = TypeVar(\"_RequestType\", bound=PipelineStepTaskRequest)",
          "_ContextType = TypeVar(\"_ContextType\", bound=PipelineContext)",
          "",
          "",
          "class PipelineStep(abc.ABC, Generic[_RequestType, _ContextType]):",
          "    \"\"\"",
          "    A step in the automation pipeline, complete with the context, request, logging + error handling utils.",
          "    Main method that is run is _invoke, which should be implemented by the subclass.",
          "    \"\"\"",
          "",
          "    name = \"PipelineStep\"",
          "    request: _RequestType",
          "    context: _ContextType",
          "",
          "    def __init__(self, request: dict[str, Any], type: DbStateRunTypes | None = None):",
          "        self.request = self._instantiate_request(request)",
          "        self.context = self._instantiate_context(self.request, type)",
          "",
          "    def invoke(self) -> Any:",
          "        try:",
          "            if not self._pre_invoke():",
          "                self._cleanup()",
          "                return",
          "            result = self._invoke(**self._get_extra_invoke_kwargs())",
          "            self._post_invoke(result)",
          "            return result",
          "        except Exception as e:",
          "            self._handle_exception(e)",
          "            raise e",
          "        finally:",
          "            self._cleanup()",
          "",
          "    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
          "        return {}",
          "",
          "    def _pre_invoke(self) -> bool:",
          "        return True",
          "",
          "    def _post_invoke(self, result: Any) -> Any:",
          "        pass",
          "",
          "    def _cleanup(self):",
          "        pass",
          "",
          "    @cached_property",
          "    def logger(self):",
          "        run_id = self.context.run_id",
          "        name = self.name",
          "        prefix = f\"[{run_id=}] [{name}] \"",
          "        return prefix_logger(prefix, logger)",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def get_task() -> Task:",
          "        pass",
          "",
          "    @classmethod",
          "    def get_signature(cls, request: PipelineStepTaskRequest, **kwargs) -> Signature:",
          "        return cls.get_task().signature(",
          "            kwargs={\"request\": request.model_dump(mode=\"json\")}, **kwargs",
          "        )",
          "",
          "    @staticmethod",
          "    def instantiate_signature(serialized_signature: SerializedSignature | Signature) -> Signature:",
          "        return signature(serialized_signature)",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> _RequestType:",
          "        pass",
          "",
          "    @staticmethod",
          "    @abc.abstractmethod",
          "    def _instantiate_context(",
          "        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None",
          "    ) -> _ContextType:",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def _handle_exception(self, exception: Exception):",
          "        pass",
          "",
          "    @abc.abstractmethod",
          "    def _invoke(self, **kwargs) -> Any:",
          "        pass",
          "",
          "    @property",
          "    def step_request_fields(self):",
          "        return make_step_request_fields(self.context)",
          "",
          "",
          "class PipelineChain(PipelineStep):",
          "    \"\"\"",
          "    A PipelineStep which can call other steps.",
          "    \"\"\"",
          "",
          "    def next(self, sig: SerializedSignature | Signature, **apply_async_kwargs):",
          "        if PIPELINE_SYNC_SIGNAL in self.context.signals:",
          "            signature(sig).apply(**apply_async_kwargs)",
          "        else:",
          "            signature(sig).apply_async(**apply_async_kwargs)",
          ""
        ]
      }
    },
    {
      "file": "psycopg/generators.py",
      "image": "psycopg.generators",
      "is_application": false,
      "line": 237,
      "name": "_consume_notifies",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/generators.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 571,
      "name": "PGconn.notifies",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 797,
      "name": "PGresult.fname",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "seer/automation/codegen/codegen_event_manager.py",
      "image": "seer.automation.codegen.codegen_event_manager",
      "is_application": true,
      "line": 15,
      "name": "CodegenEventManager.mark_running",
      "path": "/app/src/seer/automation/codegen/codegen_event_manager.py",
      "codeContext": {
        "file": "seer/automation/codegen/codegen_event_manager.py",
        "line": 15,
        "name": "CodegenEventManager.mark_running",
        "code": "from seer.automation.codegen.state import CodegenContinuationState\nfrom seer.automation.models import FileChange\n\n\n@dataclasses.dataclass\nclass CodegenEventManager:\n    state: CodegenContinuationState\n\n    def mark_running(self):\n        with self.state.update() as cur:\n            cur.status = CodegenStatus.IN_PROGRESS\n\n    def mark_completed(self):\n        with self.state.update() as cur:\n            cur.completed_at = datetime.now()\n            cur.status = CodegenStatus.COMPLETED\n\n    def add_log(self, message: str):\n        pass\n",
        "lineRange": {
          "start": 6,
          "end": 25
        },
        "lines": [
          "import dataclasses",
          "from datetime import datetime",
          "",
          "from seer.automation.autofix.components.insight_sharing.models import InsightSharingOutput",
          "from seer.automation.codegen.models import CodegenStatus, StaticAnalysisSuggestion",
          "from seer.automation.codegen.state import CodegenContinuationState",
          "from seer.automation.models import FileChange",
          "",
          "",
          "@dataclasses.dataclass",
          "class CodegenEventManager:",
          "    state: CodegenContinuationState",
          "",
          "    def mark_running(self):",
          "        with self.state.update() as cur:",
          "            cur.status = CodegenStatus.IN_PROGRESS",
          "",
          "    def mark_completed(self):",
          "        with self.state.update() as cur:",
          "            cur.completed_at = datetime.now()",
          "            cur.status = CodegenStatus.COMPLETED",
          "",
          "    def add_log(self, message: str):",
          "        pass",
          "",
          "    def append_file_change(self, file_change: FileChange):",
          "        with self.state.update() as current_state:",
          "            current_state.file_changes.append(file_change)",
          "",
          "    def mark_completed_and_extend_static_analysis_suggestions(",
          "        self, static_analysis_suggestions: list[StaticAnalysisSuggestion]",
          "    ):",
          "        with self.state.update() as cur:",
          "            cur.static_analysis_suggestions.extend(static_analysis_suggestions)",
          "            cur.completed_at = datetime.now()",
          "            cur.status = CodegenStatus.COMPLETED",
          "",
          "    def on_error(",
          "        self, error_msg: str = \"Something went wrong\", should_completely_error: bool = True",
          "    ):",
          "        with self.state.update() as cur:",
          "            cur.status = CodegenStatus.ERRORED",
          "",
          "    def send_insight(self, insight: InsightSharingOutput):",
          "        # Do nothing for now, this is only used for autofix",
          "        pass",
          ""
        ]
      }
    },
    {
      "file": "psycopg/connection.py",
      "image": "psycopg.connection",
      "is_application": false,
      "line": 173,
      "name": "BaseConnection.closed",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/connection.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 750,
      "name": "PGresult.__del__",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 760,
      "name": "PGresult.clear",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "github/Repository.py",
      "image": "github.Repository",
      "is_application": false,
      "line": 3032,
      "name": "Repository.get_pull",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Repository.py"
    },
    {
      "file": "github/Requester.py",
      "image": "github.Requester",
      "is_application": false,
      "line": 494,
      "name": "Requester.requestJsonAndCheck",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Requester.py"
    },
    {
      "file": "github/Requester.py",
      "image": "github.Requester",
      "is_application": false,
      "line": 629,
      "name": "Requester.requestJson",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Requester.py"
    },
    {
      "file": "github/Requester.py",
      "image": "github.Requester",
      "is_application": false,
      "line": 726,
      "name": "Requester.__requestEncode",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Requester.py"
    },
    {
      "file": "github/Requester.py",
      "image": "github.Requester",
      "is_application": false,
      "line": 760,
      "name": "Requester.__requestRaw",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Requester.py"
    },
    {
      "file": "github/Requester.py",
      "image": "github.Requester",
      "is_application": false,
      "line": 174,
      "name": "HTTPSRequestsConnectionClass.getresponse",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Requester.py"
    },
    {
      "file": "requests/sessions.py",
      "image": "requests.sessions",
      "is_application": false,
      "line": 602,
      "name": "Session.get",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/sessions.py"
    },
    {
      "file": "requests/sessions.py",
      "image": "requests.sessions",
      "is_application": false,
      "line": 589,
      "name": "Session.request",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/sessions.py"
    },
    {
      "file": "requests/sessions.py",
      "image": "requests.sessions",
      "is_application": false,
      "line": 703,
      "name": "Session.send",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/sessions.py"
    },
    {
      "file": "requests/adapters.py",
      "image": "requests.adapters",
      "is_application": false,
      "line": 589,
      "name": "HTTPAdapter.send",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/adapters.py"
    },
    {
      "file": "urllib3/connectionpool.py",
      "image": "urllib3.connectionpool",
      "is_application": false,
      "line": 715,
      "name": "HTTPConnectionPool.urlopen",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py"
    },
    {
      "file": "urllib3/connectionpool.py",
      "image": "urllib3.connectionpool",
      "is_application": false,
      "line": 462,
      "name": "HTTPConnectionPool._make_request",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py"
    },
    {
      "file": "sentry_sdk/integrations/stdlib.py",
      "image": "sentry_sdk.integrations.stdlib",
      "is_application": false,
      "line": 131,
      "name": "_install_httplib.<locals>.getresponse",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/integrations/stdlib.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 1374,
      "name": "HTTPConnection.getresponse",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 318,
      "name": "HTTPResponse.begin",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 279,
      "name": "HTTPResponse._read_status",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "socket.py",
      "image": "socket",
      "is_application": false,
      "line": 705,
      "name": "SocketIO.readinto",
      "path": "/usr/lib/python3.11/socket.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 1278,
      "name": "SSLSocket.recv_into",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 1134,
      "name": "SSLSocket.read",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "urllib3/connectionpool.py",
      "image": "urllib3.connectionpool",
      "is_application": false,
      "line": 1060,
      "name": "HTTPSConnectionPool._validate_conn",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py"
    },
    {
      "file": "urllib3/connection.py",
      "image": "urllib3.connection",
      "is_application": false,
      "line": 363,
      "name": "HTTPSConnection.connect",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connection.py"
    },
    {
      "file": "urllib3/connection.py",
      "image": "urllib3.connection",
      "is_application": false,
      "line": 174,
      "name": "HTTPConnection._new_conn",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connection.py"
    },
    {
      "file": "urllib3/util/connection.py",
      "image": "urllib3.util.connection",
      "is_application": false,
      "line": 85,
      "name": "create_connection",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py"
    },
    {
      "file": "socket.py",
      "image": "socket",
      "is_application": false,
      "line": 961,
      "name": "getaddrinfo",
      "path": "/usr/lib/python3.11/socket.py"
    },
    {
      "file": "urllib3/util/ssl_.py",
      "image": "urllib3.util.ssl_",
      "is_application": false,
      "line": 449,
      "name": "ssl_wrap_socket",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/ssl_.py"
    },
    {
      "file": "urllib3/util/ssl_.py",
      "image": "urllib3.util.ssl_",
      "is_application": false,
      "line": 493,
      "name": "_ssl_wrap_socket_impl",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/ssl_.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 517,
      "name": "SSLContext.wrap_socket",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 1075,
      "name": "SSLSocket._create",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 1346,
      "name": "SSLSocket.do_handshake",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "urllib3/connection.py",
      "image": "urllib3.connection",
      "is_application": false,
      "line": 244,
      "name": "HTTPConnection.request",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connection.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 1282,
      "name": "HTTPConnection.request",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 1328,
      "name": "HTTPConnection._send_request",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 1277,
      "name": "HTTPConnection.endheaders",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 1037,
      "name": "HTTPConnection._send_output",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 998,
      "name": "HTTPConnection.send",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 1241,
      "name": "SSLSocket.sendall",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 1210,
      "name": "SSLSocket.send",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "urllib3/connectionpool.py",
      "image": "urllib3.connectionpool",
      "is_application": false,
      "line": 292,
      "name": "HTTPConnectionPool._get_conn",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 960,
      "name": "HTTPConnection.close",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "socket.py",
      "image": "socket",
      "is_application": false,
      "line": 502,
      "name": "socket.close",
      "path": "/usr/lib/python3.11/socket.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 1337,
      "name": "SSLSocket._real_close",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "socket.py",
      "image": "socket",
      "is_application": false,
      "line": 496,
      "name": "socket._real_close",
      "path": "/usr/lib/python3.11/socket.py"
    },
    {
      "file": "requests/models.py",
      "image": "requests.models",
      "is_application": false,
      "line": 902,
      "name": "Response.content",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/models.py"
    },
    {
      "file": "requests/models.py",
      "image": "requests.models",
      "is_application": false,
      "line": 820,
      "name": "Response.iter_content.<locals>.generate",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/models.py"
    },
    {
      "file": "urllib3/response.py",
      "image": "urllib3.response",
      "is_application": false,
      "line": 624,
      "name": "HTTPResponse.stream",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/response.py"
    },
    {
      "file": "urllib3/response.py",
      "image": "urllib3.response",
      "is_application": false,
      "line": 831,
      "name": "HTTPResponse.read_chunked",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/response.py"
    },
    {
      "file": "urllib3/response.py",
      "image": "urllib3.response",
      "is_application": false,
      "line": 784,
      "name": "HTTPResponse._handle_chunk",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/response.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 630,
      "name": "HTTPResponse._safe_read",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "urllib3/response.py",
      "image": "urllib3.response",
      "is_application": false,
      "line": 407,
      "name": "HTTPResponse._decode",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/response.py"
    },
    {
      "file": "urllib3/response.py",
      "image": "urllib3.response",
      "is_application": false,
      "line": 94,
      "name": "GzipDecoder.decompress",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/response.py"
    },
    {
      "file": "github/Requester.py",
      "image": "github.Requester",
      "is_application": false,
      "line": 828,
      "name": "Requester.__deferRequest",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Requester.py"
    },
    {
      "file": "github/Auth.py",
      "image": "github.Auth",
      "is_application": false,
      "line": 301,
      "name": "AppInstallationAuth.token",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Auth.py"
    },
    {
      "file": "github/Auth.py",
      "image": "github.Auth",
      "is_application": false,
      "line": 312,
      "name": "AppInstallationAuth._get_installation_authorization",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Auth.py"
    },
    {
      "file": "github/GithubIntegration.py",
      "image": "github.GithubIntegration",
      "is_application": false,
      "line": 198,
      "name": "GithubIntegration.get_access_token",
      "path": "/usr/local/lib/python3.11/dist-packages/github/GithubIntegration.py"
    },
    {
      "file": "github/Auth.py",
      "image": "github.Auth",
      "is_application": false,
      "line": 179,
      "name": "AppAuth.token",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Auth.py"
    },
    {
      "file": "github/Auth.py",
      "image": "github.Auth",
      "is_application": false,
      "line": 213,
      "name": "AppAuth.create_jwt",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Auth.py"
    },
    {
      "file": "jwt/api_jwt.py",
      "image": "jwt.api_jwt",
      "is_application": false,
      "line": 78,
      "name": "PyJWT.encode",
      "path": "/usr/local/lib/python3.11/dist-packages/jwt/api_jwt.py"
    },
    {
      "file": "jwt/api_jws.py",
      "image": "jwt.api_jws",
      "is_application": false,
      "line": 170,
      "name": "PyJWS.encode",
      "path": "/usr/local/lib/python3.11/dist-packages/jwt/api_jws.py"
    },
    {
      "file": "jwt/algorithms.py",
      "image": "jwt.algorithms",
      "is_application": false,
      "line": 343,
      "name": "RSAAlgorithm.prepare_key",
      "path": "/usr/local/lib/python3.11/dist-packages/jwt/algorithms.py"
    },
    {
      "file": "requests/sessions.py",
      "image": "requests.sessions",
      "is_application": false,
      "line": 637,
      "name": "Session.post",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/sessions.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_step.py",
      "image": "seer.automation.codegen.relevant_warnings_step",
      "is_application": true,
      "line": 143,
      "name": "RelevantWarningsStep._invoke.<locals>.<listcomp>",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_step.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_step.py",
        "line": 143,
        "name": "RelevantWarningsStep._invoke.<locals>.<listcomp>",
        "code": "    @ai_track(description=\"Codegen - Relevant Warnings Step\")\n    def _invoke(self, **kwargs) -> None:\n        self.logger.info(\"Executing Codegen - Relevant Warnings Step\")\n        self.context.event_manager.mark_running()\n        diagnostics = []\n\n        # 1. Read the PR.\n        repo_client = self.context.get_repo_client(type=RepoClientType.READ)\n        pr_files = repo_client.repo.get_pull(self.request.pr_id).get_files()\n        pr_files = [\n            PrFile(\n                filename=file.filename,\n                patch=file.patch,\n                status=file.status,\n                changes=file.changes,\n                sha=file.sha,\n            )\n            for file in pr_files\n            if file.patch\n        ]",
        "lineRange": {
          "start": 134,
          "end": 153
        },
        "lines": [
          "import itertools",
          "import json",
          "import logging",
          "from typing import Any",
          "",
          "import requests",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from celery_app.app import celery_app",
          "from integrations.codecov.codecov_auth import get_codecov_auth_header",
          "from seer.automation.autofix.config import (",
          "    AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "from seer.automation.codebase.models import PrFile",
          "from seer.automation.codebase.repo_client import RepoClientType",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodegenRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          ")",
          "from seer.automation.codegen.relevant_warnings_component import (",
          "    AreIssuesFixableComponent,",
          "    AssociateWarningsWithIssuesComponent,",
          "    FetchIssuesComponent,",
          "    FilterWarningsComponent,",
          "    StaticAnalysisSuggestionsComponent,",
          ")",
          "from seer.automation.codegen.step import CodegenStep",
          "from seer.automation.pipeline import PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "@celery_app.task(",
          "    time_limit=AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    soft_time_limit=AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "def relevant_warnings_task(*args, request: dict[str, Any]):",
          "    RelevantWarningsStep(request, DbStateRunTypes.RELEVANT_WARNINGS).invoke()",
          "",
          "",
          "class RelevantWarningsStepRequest(PipelineStepTaskRequest, CodegenRelevantWarningsRequest):",
          "    pass",
          "",
          "",
          "class RelevantWarningsStep(CodegenStep):",
          "    \"\"\"",
          "    Predicts which static analysis warnings in a pull request are relevant to a past Sentry issue.",
          "    \"\"\"",
          "",
          "    name = \"RelevantWarningsStep\"",
          "    request: RelevantWarningsStepRequest",
          "    max_retries = 2",
          "",
          "    @staticmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> RelevantWarningsStepRequest:",
          "        return RelevantWarningsStepRequest.model_validate(request)",
          "",
          "    @staticmethod",
          "    def get_task():",
          "        return relevant_warnings_task",
          "",
          "    @inject",
          "    def _post_results_to_overwatch(",
          "        self,",
          "        llm_suggestions: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "        config: AppConfig = injected,",
          "    ):",
          "",
          "        if not self.request.should_post_to_overwatch:",
          "            self.logger.info(\"Skipping posting relevant warnings results to Overwatch.\")",
          "            return",
          "",
          "        # This should be a temporary solution until we can update",
          "        # Overwatch to accept the new format.",
          "        suggestions_to_overwatch_expected_format = (",
          "            [",
          "                suggestion.to_overwatch_format().model_dump()",
          "                for suggestion in llm_suggestions.suggestions",
          "            ]",
          "            if llm_suggestions",
          "            else []",
          "        )",
          "",
          "        request = {",
          "            \"run_id\": self.context.run_id,",
          "            \"results\": suggestions_to_overwatch_expected_format,",
          "            \"diagnostics\": diagnostics or [],",
          "        }",
          "        request_data = json.dumps(request, separators=(\",\", \":\")).encode(\"utf-8\")",
          "        headers = get_codecov_auth_header(",
          "            request_data,",
          "            signature_header=\"X-GEN-AI-AUTH-SIGNATURE\",",
          "            signature_secret=config.OVERWATCH_OUTGOING_SIGNATURE_SECRET,",
          "        )",
          "        requests.post(",
          "            url=self.request.callback_url,",
          "            headers=headers,",
          "            data=request_data,",
          "        ).raise_for_status()",
          "",
          "    def _complete_run(",
          "        self,",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "    ):",
          "        try:",
          "            self._post_results_to_overwatch(static_analysis_suggestions_output, diagnostics)",
          "        except Exception:",
          "            self.logger.exception(\"Error posting relevant warnings results to Overwatch\")",
          "            raise",
          "        finally:",
          "            self.context.event_manager.mark_completed_and_extend_static_analysis_suggestions(",
          "                static_analysis_suggestions_output.suggestions",
          "                if static_analysis_suggestions_output",
          "                else []",
          "            )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings Step\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings Step\")",
          "    def _invoke(self, **kwargs) -> None:",
          "        self.logger.info(\"Executing Codegen - Relevant Warnings Step\")",
          "        self.context.event_manager.mark_running()",
          "        diagnostics = []",
          "",
          "        # 1. Read the PR.",
          "        repo_client = self.context.get_repo_client(type=RepoClientType.READ)",
          "        pr_files = repo_client.repo.get_pull(self.request.pr_id).get_files()",
          "        pr_files = [",
          "            PrFile(",
          "                filename=file.filename,",
          "                patch=file.patch,",
          "                status=file.status,",
          "                changes=file.changes,",
          "                sha=file.sha,",
          "            )",
          "            for file in pr_files",
          "            if file.patch",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Read PR\",",
          "                \"pr_files\": [pr_file.filename for pr_file in pr_files],",
          "                \"warnings\": [warning.id for warning in self.request.warnings],",
          "            }",
          "        )",
          "",
          "        # 2. Only consider warnings from lines changed in the PR.",
          "        filter_warnings_component = FilterWarningsComponent(self.context)",
          "        filter_warnings_request = FilterWarningsRequest(",
          "            warnings=self.request.warnings, pr_files=pr_files",
          "        )",
          "        filter_warnings_output: FilterWarningsOutput = filter_warnings_component.invoke(",
          "            filter_warnings_request",
          "        )",
          "        warning_and_pr_files = filter_warnings_output.warning_and_pr_files",
          "",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Filter Warnings Component\",",
          "                \"filtered_warning_and_pr_files\": [",
          "                    [item.warning.id, item.pr_file.filename]",
          "                    for item in filter_warnings_output.warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "        if not warning_and_pr_files:  # exit early to avoid unnecessary issue-fetching.",
          "            self.logger.info(\"No warnings to predict relevancy for.\")",
          "            self._complete_run(None, diagnostics)",
          "            return",
          "",
          "        # 3. Fetch issues related to the PR.",
          "        fetch_issues_component = FetchIssuesComponent(self.context)",
          "        fetch_issues_request = CodeFetchIssuesRequest(",
          "            organization_id=self.request.organization_id, pr_files=pr_files",
          "        )",
          "        fetch_issues_output: CodeFetchIssuesOutput = fetch_issues_component.invoke(",
          "            fetch_issues_request",
          "        )",
          "        # Clamp issue to max_num_issues_analyzed",
          "        all_selected_issues = list(",
          "            itertools.chain.from_iterable(fetch_issues_output.filename_to_issues.values())",
          "        )",
          "        all_selected_issues = all_selected_issues[: self.request.max_num_issues_analyzed]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Fetch Issues Component\",",
          "                \"all_selected_issues\": [issue.id for issue in all_selected_issues],",
          "            }",
          "        )",
          "",
          "        # 4. Limit the number of warning-issue associations we analyze to the top",
          "        #    max_num_associations.",
          "        association_component = AssociateWarningsWithIssuesComponent(self.context)",
          "        associations_request = AssociateWarningsWithIssuesRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            filename_to_issues=fetch_issues_output.filename_to_issues,",
          "            max_num_associations=self.request.max_num_associations,",
          "        )",
          "        associations_output: AssociateWarningsWithIssuesOutput = association_component.invoke(",
          "            associations_request",
          "        )",
          "        # Annotate the warnings with potential issues associated",
          "        for association in associations_output.candidate_associations:",
          "            assoc_warning, assoc_issue = association",
          "            warning_from_list = next(",
          "                (w for w in warning_and_pr_files if w.warning.id == assoc_warning.warning.id), None",
          "            )",
          "            if warning_from_list:",
          "                if isinstance(warning_from_list.warning.potentially_related_issue_titles, list):",
          "                    warning_from_list.warning.potentially_related_issue_titles.append(",
          "                        assoc_issue.title",
          "                    )",
          "                else:",
          "                    warning_from_list.warning.potentially_related_issue_titles = [assoc_issue.title]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Associate Warnings With Issues Component\",",
          "                \"candidate_associations\": [",
          "                    {",
          "                        \"warning_id\": association[0].warning.id,",
          "                        \"pr_file\": association[0].pr_file.filename,",
          "                        \"issue_id\": association[1].id,",
          "                    }",
          "                    for association in associations_output.candidate_associations",
          "                ],",
          "                \"potentially_related_issue_titles\": [",
          "                    {",
          "                        \"warning_id\": warning_and_pr_file.warning.id,",
          "                        \"potentially_related_issue_titles\": warning_and_pr_file.warning.potentially_related_issue_titles,",
          "                        \"pr_file\": warning_and_pr_file.pr_file.filename,",
          "                    }",
          "                    for warning_and_pr_file in warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "",
          "        # 5. Filter out unfixable issues b/c it doesn't make much sense to raise suggestions for issues you can't fix.",
          "        are_issues_fixable_component = AreIssuesFixableComponent(self.context)",
          "        are_fixable_output: CodeAreIssuesFixableOutput = are_issues_fixable_component.invoke(",
          "            CodeAreIssuesFixableRequest(",
          "                candidate_issues=all_selected_issues,",
          "                max_num_issues_analyzed=self.request.max_num_issues_analyzed,",
          "            )",
          "        )",
          "        fixable_issues = [",
          "            issue",
          "            for issue, is_fixable in zip(",
          "                all_selected_issues,",
          "                are_fixable_output.are_fixable,",
          "                strict=True,",
          "            )",
          "            if is_fixable",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Are Issues Fixable Component\",",
          "                \"fixable_issues\": [issue.id for issue in fixable_issues],",
          "            }",
          "        )",
          "",
          "        # 6. Suggest issues based on static analysis warnings and fixable issues.",
          "        static_analysis_suggestions_component = StaticAnalysisSuggestionsComponent(self.context)",
          "        static_analysis_suggestions_request = CodePredictStaticAnalysisSuggestionsRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            fixable_issues=fixable_issues,",
          "            pr_files=pr_files,",
          "        )",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput = (",
          "            static_analysis_suggestions_component.invoke(static_analysis_suggestions_request)",
          "        )",
          "",
          "        # 7. Save results.",
          "        self._complete_run(static_analysis_suggestions_output, diagnostics)",
          ""
        ]
      }
    },
    {
      "file": "github/PaginatedList.py",
      "image": "github.PaginatedList",
      "is_application": false,
      "line": 69,
      "name": "PaginatedListBase.__iter__",
      "path": "/usr/local/lib/python3.11/dist-packages/github/PaginatedList.py"
    },
    {
      "file": "github/PaginatedList.py",
      "image": "github.PaginatedList",
      "is_application": false,
      "line": 80,
      "name": "PaginatedListBase._grow",
      "path": "/usr/local/lib/python3.11/dist-packages/github/PaginatedList.py"
    },
    {
      "file": "github/PaginatedList.py",
      "image": "github.PaginatedList",
      "is_application": false,
      "line": 219,
      "name": "PaginatedList._fetchNextPage",
      "path": "/usr/local/lib/python3.11/dist-packages/github/PaginatedList.py"
    },
    {
      "file": "urllib3/util/connection.py",
      "image": "urllib3.util.connection",
      "is_application": false,
      "line": 28,
      "name": "is_connection_dropped",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py"
    },
    {
      "file": "urllib3/util/wait.py",
      "image": "urllib3.util.wait",
      "is_application": false,
      "line": 145,
      "name": "wait_for_read",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/wait.py"
    },
    {
      "file": "urllib3/util/wait.py",
      "image": "urllib3.util.wait",
      "is_application": false,
      "line": 106,
      "name": "poll_wait_for_socket",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/wait.py"
    },
    {
      "file": "urllib3/util/wait.py",
      "image": "urllib3.util.wait",
      "is_application": false,
      "line": 43,
      "name": "_retry_on_intr",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/wait.py"
    },
    {
      "file": "urllib3/util/wait.py",
      "image": "urllib3.util.wait",
      "is_application": false,
      "line": 104,
      "name": "poll_wait_for_socket.<locals>.do_poll",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/util/wait.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_step.py",
      "image": "seer.automation.codegen.relevant_warnings_step",
      "is_application": true,
      "line": 122,
      "name": "RelevantWarningsStep._complete_run",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_step.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_step.py",
        "line": 122,
        "name": "RelevantWarningsStep._complete_run",
        "code": "            data=request_data,\n        ).raise_for_status()\n\n    def _complete_run(\n        self,\n        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput | None,\n        diagnostics: list | None,\n    ):\n        try:\n            self._post_results_to_overwatch(static_analysis_suggestions_output, diagnostics)\n        except Exception:\n            self.logger.exception(\"Error posting relevant warnings results to Overwatch\")\n            raise\n        finally:\n            self.context.event_manager.mark_completed_and_extend_static_analysis_suggestions(\n                static_analysis_suggestions_output.suggestions\n                if static_analysis_suggestions_output\n                else []\n            )\n",
        "lineRange": {
          "start": 113,
          "end": 132
        },
        "lines": [
          "import itertools",
          "import json",
          "import logging",
          "from typing import Any",
          "",
          "import requests",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from celery_app.app import celery_app",
          "from integrations.codecov.codecov_auth import get_codecov_auth_header",
          "from seer.automation.autofix.config import (",
          "    AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "from seer.automation.codebase.models import PrFile",
          "from seer.automation.codebase.repo_client import RepoClientType",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodegenRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          ")",
          "from seer.automation.codegen.relevant_warnings_component import (",
          "    AreIssuesFixableComponent,",
          "    AssociateWarningsWithIssuesComponent,",
          "    FetchIssuesComponent,",
          "    FilterWarningsComponent,",
          "    StaticAnalysisSuggestionsComponent,",
          ")",
          "from seer.automation.codegen.step import CodegenStep",
          "from seer.automation.pipeline import PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "@celery_app.task(",
          "    time_limit=AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    soft_time_limit=AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "def relevant_warnings_task(*args, request: dict[str, Any]):",
          "    RelevantWarningsStep(request, DbStateRunTypes.RELEVANT_WARNINGS).invoke()",
          "",
          "",
          "class RelevantWarningsStepRequest(PipelineStepTaskRequest, CodegenRelevantWarningsRequest):",
          "    pass",
          "",
          "",
          "class RelevantWarningsStep(CodegenStep):",
          "    \"\"\"",
          "    Predicts which static analysis warnings in a pull request are relevant to a past Sentry issue.",
          "    \"\"\"",
          "",
          "    name = \"RelevantWarningsStep\"",
          "    request: RelevantWarningsStepRequest",
          "    max_retries = 2",
          "",
          "    @staticmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> RelevantWarningsStepRequest:",
          "        return RelevantWarningsStepRequest.model_validate(request)",
          "",
          "    @staticmethod",
          "    def get_task():",
          "        return relevant_warnings_task",
          "",
          "    @inject",
          "    def _post_results_to_overwatch(",
          "        self,",
          "        llm_suggestions: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "        config: AppConfig = injected,",
          "    ):",
          "",
          "        if not self.request.should_post_to_overwatch:",
          "            self.logger.info(\"Skipping posting relevant warnings results to Overwatch.\")",
          "            return",
          "",
          "        # This should be a temporary solution until we can update",
          "        # Overwatch to accept the new format.",
          "        suggestions_to_overwatch_expected_format = (",
          "            [",
          "                suggestion.to_overwatch_format().model_dump()",
          "                for suggestion in llm_suggestions.suggestions",
          "            ]",
          "            if llm_suggestions",
          "            else []",
          "        )",
          "",
          "        request = {",
          "            \"run_id\": self.context.run_id,",
          "            \"results\": suggestions_to_overwatch_expected_format,",
          "            \"diagnostics\": diagnostics or [],",
          "        }",
          "        request_data = json.dumps(request, separators=(\",\", \":\")).encode(\"utf-8\")",
          "        headers = get_codecov_auth_header(",
          "            request_data,",
          "            signature_header=\"X-GEN-AI-AUTH-SIGNATURE\",",
          "            signature_secret=config.OVERWATCH_OUTGOING_SIGNATURE_SECRET,",
          "        )",
          "        requests.post(",
          "            url=self.request.callback_url,",
          "            headers=headers,",
          "            data=request_data,",
          "        ).raise_for_status()",
          "",
          "    def _complete_run(",
          "        self,",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "    ):",
          "        try:",
          "            self._post_results_to_overwatch(static_analysis_suggestions_output, diagnostics)",
          "        except Exception:",
          "            self.logger.exception(\"Error posting relevant warnings results to Overwatch\")",
          "            raise",
          "        finally:",
          "            self.context.event_manager.mark_completed_and_extend_static_analysis_suggestions(",
          "                static_analysis_suggestions_output.suggestions",
          "                if static_analysis_suggestions_output",
          "                else []",
          "            )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings Step\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings Step\")",
          "    def _invoke(self, **kwargs) -> None:",
          "        self.logger.info(\"Executing Codegen - Relevant Warnings Step\")",
          "        self.context.event_manager.mark_running()",
          "        diagnostics = []",
          "",
          "        # 1. Read the PR.",
          "        repo_client = self.context.get_repo_client(type=RepoClientType.READ)",
          "        pr_files = repo_client.repo.get_pull(self.request.pr_id).get_files()",
          "        pr_files = [",
          "            PrFile(",
          "                filename=file.filename,",
          "                patch=file.patch,",
          "                status=file.status,",
          "                changes=file.changes,",
          "                sha=file.sha,",
          "            )",
          "            for file in pr_files",
          "            if file.patch",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Read PR\",",
          "                \"pr_files\": [pr_file.filename for pr_file in pr_files],",
          "                \"warnings\": [warning.id for warning in self.request.warnings],",
          "            }",
          "        )",
          "",
          "        # 2. Only consider warnings from lines changed in the PR.",
          "        filter_warnings_component = FilterWarningsComponent(self.context)",
          "        filter_warnings_request = FilterWarningsRequest(",
          "            warnings=self.request.warnings, pr_files=pr_files",
          "        )",
          "        filter_warnings_output: FilterWarningsOutput = filter_warnings_component.invoke(",
          "            filter_warnings_request",
          "        )",
          "        warning_and_pr_files = filter_warnings_output.warning_and_pr_files",
          "",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Filter Warnings Component\",",
          "                \"filtered_warning_and_pr_files\": [",
          "                    [item.warning.id, item.pr_file.filename]",
          "                    for item in filter_warnings_output.warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "        if not warning_and_pr_files:  # exit early to avoid unnecessary issue-fetching.",
          "            self.logger.info(\"No warnings to predict relevancy for.\")",
          "            self._complete_run(None, diagnostics)",
          "            return",
          "",
          "        # 3. Fetch issues related to the PR.",
          "        fetch_issues_component = FetchIssuesComponent(self.context)",
          "        fetch_issues_request = CodeFetchIssuesRequest(",
          "            organization_id=self.request.organization_id, pr_files=pr_files",
          "        )",
          "        fetch_issues_output: CodeFetchIssuesOutput = fetch_issues_component.invoke(",
          "            fetch_issues_request",
          "        )",
          "        # Clamp issue to max_num_issues_analyzed",
          "        all_selected_issues = list(",
          "            itertools.chain.from_iterable(fetch_issues_output.filename_to_issues.values())",
          "        )",
          "        all_selected_issues = all_selected_issues[: self.request.max_num_issues_analyzed]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Fetch Issues Component\",",
          "                \"all_selected_issues\": [issue.id for issue in all_selected_issues],",
          "            }",
          "        )",
          "",
          "        # 4. Limit the number of warning-issue associations we analyze to the top",
          "        #    max_num_associations.",
          "        association_component = AssociateWarningsWithIssuesComponent(self.context)",
          "        associations_request = AssociateWarningsWithIssuesRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            filename_to_issues=fetch_issues_output.filename_to_issues,",
          "            max_num_associations=self.request.max_num_associations,",
          "        )",
          "        associations_output: AssociateWarningsWithIssuesOutput = association_component.invoke(",
          "            associations_request",
          "        )",
          "        # Annotate the warnings with potential issues associated",
          "        for association in associations_output.candidate_associations:",
          "            assoc_warning, assoc_issue = association",
          "            warning_from_list = next(",
          "                (w for w in warning_and_pr_files if w.warning.id == assoc_warning.warning.id), None",
          "            )",
          "            if warning_from_list:",
          "                if isinstance(warning_from_list.warning.potentially_related_issue_titles, list):",
          "                    warning_from_list.warning.potentially_related_issue_titles.append(",
          "                        assoc_issue.title",
          "                    )",
          "                else:",
          "                    warning_from_list.warning.potentially_related_issue_titles = [assoc_issue.title]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Associate Warnings With Issues Component\",",
          "                \"candidate_associations\": [",
          "                    {",
          "                        \"warning_id\": association[0].warning.id,",
          "                        \"pr_file\": association[0].pr_file.filename,",
          "                        \"issue_id\": association[1].id,",
          "                    }",
          "                    for association in associations_output.candidate_associations",
          "                ],",
          "                \"potentially_related_issue_titles\": [",
          "                    {",
          "                        \"warning_id\": warning_and_pr_file.warning.id,",
          "                        \"potentially_related_issue_titles\": warning_and_pr_file.warning.potentially_related_issue_titles,",
          "                        \"pr_file\": warning_and_pr_file.pr_file.filename,",
          "                    }",
          "                    for warning_and_pr_file in warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "",
          "        # 5. Filter out unfixable issues b/c it doesn't make much sense to raise suggestions for issues you can't fix.",
          "        are_issues_fixable_component = AreIssuesFixableComponent(self.context)",
          "        are_fixable_output: CodeAreIssuesFixableOutput = are_issues_fixable_component.invoke(",
          "            CodeAreIssuesFixableRequest(",
          "                candidate_issues=all_selected_issues,",
          "                max_num_issues_analyzed=self.request.max_num_issues_analyzed,",
          "            )",
          "        )",
          "        fixable_issues = [",
          "            issue",
          "            for issue, is_fixable in zip(",
          "                all_selected_issues,",
          "                are_fixable_output.are_fixable,",
          "                strict=True,",
          "            )",
          "            if is_fixable",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Are Issues Fixable Component\",",
          "                \"fixable_issues\": [issue.id for issue in fixable_issues],",
          "            }",
          "        )",
          "",
          "        # 6. Suggest issues based on static analysis warnings and fixable issues.",
          "        static_analysis_suggestions_component = StaticAnalysisSuggestionsComponent(self.context)",
          "        static_analysis_suggestions_request = CodePredictStaticAnalysisSuggestionsRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            fixable_issues=fixable_issues,",
          "            pr_files=pr_files,",
          "        )",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput = (",
          "            static_analysis_suggestions_component.invoke(static_analysis_suggestions_request)",
          "        )",
          "",
          "        # 7. Save results.",
          "        self._complete_run(static_analysis_suggestions_output, diagnostics)",
          ""
        ]
      }
    },
    {
      "file": "seer/dependency_injection.py",
      "image": "seer.dependency_injection",
      "is_application": true,
      "line": 227,
      "name": "inject.<locals>.wrapper",
      "path": "/app/src/seer/dependency_injection.py",
      "codeContext": {
        "file": "seer/dependency_injection.py",
        "line": 227,
        "name": "inject.<locals>.wrapper",
        "code": "\n        if argspec.kwonlydefaults:\n            for k, v in argspec.kwonlydefaults.items():\n                if v is injected and k not in new_kwds:\n                    try:\n                        new_kwds[k] = resolve(argspec.annotations[k])\n                    except KeyError:\n                        raise AssertionError(f\"Cannot inject argument {k} as it lacks annotations\")\n\n        return c(*args, **new_kwds)  # type: ignore\n\n    if inspect.isclass(original_type):\n        return type(original_type.__name__, (original_type,), dict(__init__=wrapper))  # type: ignore\n\n    return wrapper  # type: ignore\n\n\ndef resolve(source: type[_A]) -> _A:\n    key = FactoryAnnotation.from_annotation(source)\n    return cast(_A, resolve_annotation(key, source))",
        "lineRange": {
          "start": 218,
          "end": 237
        },
        "lines": [
          "\"\"\"",
          "Provides a basic dependency injection framework that uses callable annotations",
          "to decide how and when to inject.",
          "",
          "You can inject classes and values and lists of either with some basic constructs:",
          "",
          "@module.provider",
          "@dataclass",
          "class MyService:",
          "  other_service: OtherService = injected",
          "",
          "MyService() # other_service will be instantiated and cached",
          "",
          "Overrides/Stubs and tests can be provided via the `stub_module` or creating a `test_module` fixture",
          "(see conftest.py).",
          "",
          "You can also inject normal functions, like so:",
          "",
          "@inject",
          "def do_setup(a: int, b: MyService = injected):",
          "   ...",
          "",
          "do_setup(100) # b will be injected automatically.",
          "\"\"\"",
          "",
          "import dataclasses",
          "import functools",
          "import inspect",
          "import threading",
          "from typing import Annotated, Any, Callable, TypeVar, cast",
          "",
          "from johen.generators.annotations import AnnotationProcessingContext",
          "from pydantic import BaseModel",
          "from pydantic.fields import FieldInfo",
          "",
          "_A = TypeVar(\"_A\")",
          "_C = TypeVar(\"_C\", bound=Callable[[], Any])",
          "_CK = TypeVar(\"_CK\", bound=Callable)",
          "_T = TypeVar(\"_T\", bound=type)",
          "",
          "",
          "@dataclasses.dataclass",
          "class Labeled:",
          "    \"\"\"",
          "    Used to 'label' a type so as to have a unique provider when the type itself is not unique.",
          "    eg:",
          "",
          "    @inject",
          "    @dataclass",
          "    class Config:",
          "      host: Annotated[str, Labeled(\"host\")]",
          "      protocol: Annotated[str, Labeled(\"protocol\")]",
          "    \"\"\"",
          "",
          "    label: str",
          "",
          "",
          "@dataclasses.dataclass(frozen=True)",
          "class FactoryAnnotation:",
          "    concrete_type: type",
          "    is_collection: bool",
          "    is_type: bool",
          "    label: str",
          "",
          "    @classmethod",
          "    def from_annotation(cls, source: Any) -> \"FactoryAnnotation\":",
          "        annotation = AnnotationProcessingContext.from_source(source)",
          "        if annotation.origin is Annotated:",
          "            label = next((arg.label for arg in annotation.args[1:] if isinstance(arg, Labeled)), \"\")",
          "            inner = FactoryAnnotation.from_annotation(annotation.args[0])",
          "            assert not inner.label, f\"Cannot get_factory {source}: Annotated has embedded Labeled\"",
          "            return dataclasses.replace(inner, label=label)",
          "        elif annotation.concretely_implements(list):",
          "            assert (",
          "                len(annotation.args) == 1",
          "            ), f\"Cannot get_factory {source}: list requires at least one argument\"",
          "            inner = FactoryAnnotation.from_annotation(annotation.args[0])",
          "            assert not inner.label, f\"Cannot get_factory {source}: list has embedded Labeled\"",
          "            assert (",
          "                not inner.is_collection",
          "            ), f\"Cannot get_factory {source}: collections must be of concrete types, not other lists\"",
          "            return dataclasses.replace(inner, is_collection=True)",
          "        elif annotation.origin is type:",
          "            assert (",
          "                len(annotation.args) == 1",
          "            ), f\"Cannot get_factory {source}: type requires at least one argument\"",
          "            inner = FactoryAnnotation.from_annotation(annotation.args[0])",
          "            assert not inner.label, f\"Cannot get_factory {source}: type has embedded Labeled\"",
          "            assert (",
          "                not inner.is_collection and not inner.is_type",
          "            ), f\"Cannot get_factory {source}: type factories must be of concrete types, not lists or other types\"",
          "            return dataclasses.replace(inner, is_type=True)",
          "",
          "        assert (",
          "            annotation.origin is None",
          "        ), f\"Cannot get_factory {source}, only concrete types, type annotations, or lists of concrete types are supported\"",
          "        return FactoryAnnotation(",
          "            concrete_type=annotation.source, is_collection=False, is_type=False, label=\"\"",
          "        )",
          "",
          "    @classmethod",
          "    def from_factory(cls, c: Callable) -> \"FactoryAnnotation\":",
          "        argspec = inspect.getfullargspec(c)",
          "        num_arg_defaults = len(argspec.defaults) if argspec.defaults is not None else 0",
          "        num_kwd_defaults = len(argspec.kwonlydefaults) if argspec.kwonlydefaults is not None else 0",
          "",
          "        # Constructors have implicit self reference and return annotations -- themselves",
          "        if inspect.isclass(c):",
          "            num_arg_defaults += 1",
          "            rv = c",
          "        else:",
          "            rv = argspec.annotations.get(\"return\", None)",
          "            assert rv is not None, \"Cannot decorate function without return annotation\"",
          "",
          "        assert num_arg_defaults >= len(",
          "            argspec.args",
          "        ), \"Cannot decorate function with required positional args\"",
          "        assert num_kwd_defaults >= len(",
          "            argspec.kwonlyargs",
          "        ), \"Cannot decorate function with required kwd args\"",
          "        return FactoryAnnotation.from_annotation(rv)",
          "",
          "    @classmethod",
          "    def from_field(cls, f: FieldInfo) -> \"FactoryAnnotation\":",
          "        base = FactoryAnnotation.from_annotation(f.annotation)",
          "        label = next((arg.label for arg in f.metadata if isinstance(arg, Labeled)), \"\")",
          "        return dataclasses.replace(base, label=label)",
          "",
          "",
          "class FactoryNotFound(Exception):",
          "    pass",
          "",
          "",
          "@dataclasses.dataclass",
          "class Module:",
          "    registry: dict[FactoryAnnotation, Callable] = dataclasses.field(default_factory=dict)",
          "",
          "    def provider(self, c: _C) -> _C:",
          "        c = inject(c)",
          "",
          "        key = FactoryAnnotation.from_factory(c)",
          "        assert (",
          "            key not in self.registry",
          "        ), f\"{key.concrete_type} is already registered for this injector\"",
          "        self.registry[key] = c",
          "        return c",
          "",
          "    def constant(self, annotation: type[_A], val: _A) -> \"Module\":",
          "        key = FactoryAnnotation.from_annotation(annotation)",
          "        self.registry[key] = lambda: val",
          "        return self",
          "",
          "    def enable(self):",
          "        injector = Injector(self, _cur.injector)",
          "        _cur.injector = injector",
          "        return injector",
          "",
          "    def __enter__(self):",
          "        return self.enable()",
          "",
          "    def __exit__(self, exc_type, exc_val, exc_tb):",
          "        assert _cur.injector, \"Injector state was tampered with, or __exit__ invoked prematurely\"",
          "        assert (",
          "            _cur.injector.module is self",
          "        ), \"Injector state was tampered with, or __exit__ invoked prematurely\"",
          "        _cur.injector = _cur.injector.parent",
          "",
          "",
          "class _Injected:",
          "    \"\"\"",
          "    Magical variable indicating that a parameter should be injected when constructed",
          "    by an Injector object.  Invoking a method that uses an `injected` value directly",
          "    will use the currently available injector instance to fill in the default value.",
          "    \"\"\"",
          "",
          "    pass",
          "",
          "",
          "# Marked as Any so it can be a stand in value for any annotation.",
          "injected: Any = _Injected()",
          "",
          "",
          "def inject(c: _A) -> _A:",
          "    original_type = c",
          "    if inspect.isclass(c):",
          "        c = c.__init__",
          "",
          "    argspec = inspect.getfullargspec(c)",
          "",
          "    @functools.wraps(c)  # type: ignore",
          "    def wrapper(*args: Any, **kwargs: Any) -> Any:",
          "        new_kwds = {**kwargs}",
          "",
          "        if inspect.isclass(original_type) and issubclass(original_type, BaseModel):",
          "            for k, f in original_type.model_fields.items():",
          "                if f.default is injected or f.default_factory is injected:",
          "                    annotation = f.annotation",
          "                    if not annotation:",
          "                        raise AssertionError(f\"Cannot inject argument {k} as it lacks annotations\")",
          "                    resolved = resolve_annotation(FactoryAnnotation.from_field(f), f)",
          "                    new_kwds[k] = resolved",
          "",
          "        if argspec.defaults:",
          "            offset = len(argspec.args) - len(argspec.defaults)",
          "            for i, d in enumerate(argspec.defaults):",
          "                arg_idx = offset + i",
          "                arg_name = argspec.args[arg_idx]",
          "",
          "                if d is injected and len(args) <= arg_idx and arg_name not in new_kwds:",
          "                    try:",
          "                        resolved = resolve(argspec.annotations[arg_name])",
          "                    except KeyError:",
          "                        raise AssertionError(",
          "                            f\"Cannot inject argument {arg_name} as it lacks annotations\"",
          "                        )",
          "",
          "                    new_kwds[arg_name] = resolved",
          "",
          "        if argspec.kwonlydefaults:",
          "            for k, v in argspec.kwonlydefaults.items():",
          "                if v is injected and k not in new_kwds:",
          "                    try:",
          "                        new_kwds[k] = resolve(argspec.annotations[k])",
          "                    except KeyError:",
          "                        raise AssertionError(f\"Cannot inject argument {k} as it lacks annotations\")",
          "",
          "        return c(*args, **new_kwds)  # type: ignore",
          "",
          "    if inspect.isclass(original_type):",
          "        return type(original_type.__name__, (original_type,), dict(__init__=wrapper))  # type: ignore",
          "",
          "    return wrapper  # type: ignore",
          "",
          "",
          "def resolve(source: type[_A]) -> _A:",
          "    key = FactoryAnnotation.from_annotation(source)",
          "    return cast(_A, resolve_annotation(key, source))",
          "",
          "",
          "def resolve_annotation(key: FactoryAnnotation, source: Any) -> Any:",
          "    if _cur.injector is None:",
          "        raise FactoryNotFound(f\"Cannot resolve '{source}', no module injector is currently active.\")",
          "    if _cur.seen is None:",
          "        _cur.seen = []",
          "    try:",
          "        if key in _cur.seen:",
          "            raise FactoryNotFound(",
          "                f\"Circular dependency: {' -> '.join(str(k) for k in _cur.seen)} -> {key}\"",
          "            )",
          "        _cur.seen.append(key)",
          "        return _cur.injector.get(source, key=key)",
          "    finally:",
          "        _cur.seen.clear()",
          "",
          "",
          "@dataclasses.dataclass",
          "class Injector:",
          "    module: Module",
          "    parent: \"Injector | None\"",
          "    _cache: dict[FactoryAnnotation, Any] = dataclasses.field(default_factory=dict)",
          "",
          "    @property",
          "    def cache(self) -> dict[FactoryAnnotation, Any]:",
          "        if _cur.injector is not None:",
          "            return _cur.injector._cache",
          "        return self._cache",
          "",
          "    def get(self, source: type[_A], key: FactoryAnnotation | None = None) -> _A:",
          "        if key is None:",
          "            key = FactoryAnnotation.from_annotation(source)",
          "",
          "        if key in self.cache:",
          "            return cast(_A, self.cache[key])",
          "",
          "        try:",
          "            f = self.module.registry[key]",
          "        except KeyError:",
          "            if self.parent is not None:",
          "                return self.parent.get(source, key=key)",
          "            raise FactoryNotFound(f\"No registered factory for {source}\")",
          "",
          "        rv = self.cache[key] = f()",
          "        return cast(_A, rv)",
          "",
          "",
          "class _Cur(threading.local):",
          "    injector: Injector | None = None",
          "    seen: list[FactoryAnnotation] | None = None",
          "",
          "",
          "def copy_modules_initializer() -> Callable[[], None]:",
          "    \"\"\"",
          "    Creates an 'initializer' for use with ThreadPoolExecutor that will enable",
          "    modules that are currently enabled at the time this method was originally",
          "    called.",
          "    \"\"\"",
          "    modules: list[Module] = []",
          "    injector = _cur.injector",
          "    while injector:",
          "        modules.insert(0, injector.module)",
          "        injector = injector.parent",
          "",
          "    def initializer():",
          "        for module in modules:",
          "            module.enable()",
          "",
          "    return initializer",
          "",
          "",
          "_cur = _Cur()",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_step.py",
      "image": "seer.automation.codegen.relevant_warnings_step",
      "is_application": true,
      "line": 100,
      "name": "RelevantWarningsStep._post_results_to_overwatch",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_step.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_step.py",
        "line": 100,
        "name": "RelevantWarningsStep._post_results_to_overwatch",
        "code": "            [\n                suggestion.to_overwatch_format().model_dump()\n                for suggestion in llm_suggestions.suggestions\n            ]\n            if llm_suggestions\n            else []\n        )\n\n        request = {\n            \"run_id\": self.context.run_id,\n            \"results\": suggestions_to_overwatch_expected_format,\n            \"diagnostics\": diagnostics or [],\n        }\n        request_data = json.dumps(request, separators=(\",\", \":\")).encode(\"utf-8\")\n        headers = get_codecov_auth_header(\n            request_data,\n            signature_header=\"X-GEN-AI-AUTH-SIGNATURE\",\n            signature_secret=config.OVERWATCH_OUTGOING_SIGNATURE_SECRET,\n        )\n        requests.post(",
        "lineRange": {
          "start": 91,
          "end": 110
        },
        "lines": [
          "import itertools",
          "import json",
          "import logging",
          "from typing import Any",
          "",
          "import requests",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from celery_app.app import celery_app",
          "from integrations.codecov.codecov_auth import get_codecov_auth_header",
          "from seer.automation.autofix.config import (",
          "    AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "from seer.automation.codebase.models import PrFile",
          "from seer.automation.codebase.repo_client import RepoClientType",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodegenRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          ")",
          "from seer.automation.codegen.relevant_warnings_component import (",
          "    AreIssuesFixableComponent,",
          "    AssociateWarningsWithIssuesComponent,",
          "    FetchIssuesComponent,",
          "    FilterWarningsComponent,",
          "    StaticAnalysisSuggestionsComponent,",
          ")",
          "from seer.automation.codegen.step import CodegenStep",
          "from seer.automation.pipeline import PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "@celery_app.task(",
          "    time_limit=AUTOFIX_EXECUTION_HARD_TIME_LIMIT_SECS,",
          "    soft_time_limit=AUTOFIX_EXECUTION_SOFT_TIME_LIMIT_SECS,",
          ")",
          "def relevant_warnings_task(*args, request: dict[str, Any]):",
          "    RelevantWarningsStep(request, DbStateRunTypes.RELEVANT_WARNINGS).invoke()",
          "",
          "",
          "class RelevantWarningsStepRequest(PipelineStepTaskRequest, CodegenRelevantWarningsRequest):",
          "    pass",
          "",
          "",
          "class RelevantWarningsStep(CodegenStep):",
          "    \"\"\"",
          "    Predicts which static analysis warnings in a pull request are relevant to a past Sentry issue.",
          "    \"\"\"",
          "",
          "    name = \"RelevantWarningsStep\"",
          "    request: RelevantWarningsStepRequest",
          "    max_retries = 2",
          "",
          "    @staticmethod",
          "    def _instantiate_request(request: dict[str, Any]) -> RelevantWarningsStepRequest:",
          "        return RelevantWarningsStepRequest.model_validate(request)",
          "",
          "    @staticmethod",
          "    def get_task():",
          "        return relevant_warnings_task",
          "",
          "    @inject",
          "    def _post_results_to_overwatch(",
          "        self,",
          "        llm_suggestions: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "        config: AppConfig = injected,",
          "    ):",
          "",
          "        if not self.request.should_post_to_overwatch:",
          "            self.logger.info(\"Skipping posting relevant warnings results to Overwatch.\")",
          "            return",
          "",
          "        # This should be a temporary solution until we can update",
          "        # Overwatch to accept the new format.",
          "        suggestions_to_overwatch_expected_format = (",
          "            [",
          "                suggestion.to_overwatch_format().model_dump()",
          "                for suggestion in llm_suggestions.suggestions",
          "            ]",
          "            if llm_suggestions",
          "            else []",
          "        )",
          "",
          "        request = {",
          "            \"run_id\": self.context.run_id,",
          "            \"results\": suggestions_to_overwatch_expected_format,",
          "            \"diagnostics\": diagnostics or [],",
          "        }",
          "        request_data = json.dumps(request, separators=(\",\", \":\")).encode(\"utf-8\")",
          "        headers = get_codecov_auth_header(",
          "            request_data,",
          "            signature_header=\"X-GEN-AI-AUTH-SIGNATURE\",",
          "            signature_secret=config.OVERWATCH_OUTGOING_SIGNATURE_SECRET,",
          "        )",
          "        requests.post(",
          "            url=self.request.callback_url,",
          "            headers=headers,",
          "            data=request_data,",
          "        ).raise_for_status()",
          "",
          "    def _complete_run(",
          "        self,",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput | None,",
          "        diagnostics: list | None,",
          "    ):",
          "        try:",
          "            self._post_results_to_overwatch(static_analysis_suggestions_output, diagnostics)",
          "        except Exception:",
          "            self.logger.exception(\"Error posting relevant warnings results to Overwatch\")",
          "            raise",
          "        finally:",
          "            self.context.event_manager.mark_completed_and_extend_static_analysis_suggestions(",
          "                static_analysis_suggestions_output.suggestions",
          "                if static_analysis_suggestions_output",
          "                else []",
          "            )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings Step\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings Step\")",
          "    def _invoke(self, **kwargs) -> None:",
          "        self.logger.info(\"Executing Codegen - Relevant Warnings Step\")",
          "        self.context.event_manager.mark_running()",
          "        diagnostics = []",
          "",
          "        # 1. Read the PR.",
          "        repo_client = self.context.get_repo_client(type=RepoClientType.READ)",
          "        pr_files = repo_client.repo.get_pull(self.request.pr_id).get_files()",
          "        pr_files = [",
          "            PrFile(",
          "                filename=file.filename,",
          "                patch=file.patch,",
          "                status=file.status,",
          "                changes=file.changes,",
          "                sha=file.sha,",
          "            )",
          "            for file in pr_files",
          "            if file.patch",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Read PR\",",
          "                \"pr_files\": [pr_file.filename for pr_file in pr_files],",
          "                \"warnings\": [warning.id for warning in self.request.warnings],",
          "            }",
          "        )",
          "",
          "        # 2. Only consider warnings from lines changed in the PR.",
          "        filter_warnings_component = FilterWarningsComponent(self.context)",
          "        filter_warnings_request = FilterWarningsRequest(",
          "            warnings=self.request.warnings, pr_files=pr_files",
          "        )",
          "        filter_warnings_output: FilterWarningsOutput = filter_warnings_component.invoke(",
          "            filter_warnings_request",
          "        )",
          "        warning_and_pr_files = filter_warnings_output.warning_and_pr_files",
          "",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Filter Warnings Component\",",
          "                \"filtered_warning_and_pr_files\": [",
          "                    [item.warning.id, item.pr_file.filename]",
          "                    for item in filter_warnings_output.warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "        if not warning_and_pr_files:  # exit early to avoid unnecessary issue-fetching.",
          "            self.logger.info(\"No warnings to predict relevancy for.\")",
          "            self._complete_run(None, diagnostics)",
          "            return",
          "",
          "        # 3. Fetch issues related to the PR.",
          "        fetch_issues_component = FetchIssuesComponent(self.context)",
          "        fetch_issues_request = CodeFetchIssuesRequest(",
          "            organization_id=self.request.organization_id, pr_files=pr_files",
          "        )",
          "        fetch_issues_output: CodeFetchIssuesOutput = fetch_issues_component.invoke(",
          "            fetch_issues_request",
          "        )",
          "        # Clamp issue to max_num_issues_analyzed",
          "        all_selected_issues = list(",
          "            itertools.chain.from_iterable(fetch_issues_output.filename_to_issues.values())",
          "        )",
          "        all_selected_issues = all_selected_issues[: self.request.max_num_issues_analyzed]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Fetch Issues Component\",",
          "                \"all_selected_issues\": [issue.id for issue in all_selected_issues],",
          "            }",
          "        )",
          "",
          "        # 4. Limit the number of warning-issue associations we analyze to the top",
          "        #    max_num_associations.",
          "        association_component = AssociateWarningsWithIssuesComponent(self.context)",
          "        associations_request = AssociateWarningsWithIssuesRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            filename_to_issues=fetch_issues_output.filename_to_issues,",
          "            max_num_associations=self.request.max_num_associations,",
          "        )",
          "        associations_output: AssociateWarningsWithIssuesOutput = association_component.invoke(",
          "            associations_request",
          "        )",
          "        # Annotate the warnings with potential issues associated",
          "        for association in associations_output.candidate_associations:",
          "            assoc_warning, assoc_issue = association",
          "            warning_from_list = next(",
          "                (w for w in warning_and_pr_files if w.warning.id == assoc_warning.warning.id), None",
          "            )",
          "            if warning_from_list:",
          "                if isinstance(warning_from_list.warning.potentially_related_issue_titles, list):",
          "                    warning_from_list.warning.potentially_related_issue_titles.append(",
          "                        assoc_issue.title",
          "                    )",
          "                else:",
          "                    warning_from_list.warning.potentially_related_issue_titles = [assoc_issue.title]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Associate Warnings With Issues Component\",",
          "                \"candidate_associations\": [",
          "                    {",
          "                        \"warning_id\": association[0].warning.id,",
          "                        \"pr_file\": association[0].pr_file.filename,",
          "                        \"issue_id\": association[1].id,",
          "                    }",
          "                    for association in associations_output.candidate_associations",
          "                ],",
          "                \"potentially_related_issue_titles\": [",
          "                    {",
          "                        \"warning_id\": warning_and_pr_file.warning.id,",
          "                        \"potentially_related_issue_titles\": warning_and_pr_file.warning.potentially_related_issue_titles,",
          "                        \"pr_file\": warning_and_pr_file.pr_file.filename,",
          "                    }",
          "                    for warning_and_pr_file in warning_and_pr_files",
          "                ],",
          "            }",
          "        )",
          "",
          "        # 5. Filter out unfixable issues b/c it doesn't make much sense to raise suggestions for issues you can't fix.",
          "        are_issues_fixable_component = AreIssuesFixableComponent(self.context)",
          "        are_fixable_output: CodeAreIssuesFixableOutput = are_issues_fixable_component.invoke(",
          "            CodeAreIssuesFixableRequest(",
          "                candidate_issues=all_selected_issues,",
          "                max_num_issues_analyzed=self.request.max_num_issues_analyzed,",
          "            )",
          "        )",
          "        fixable_issues = [",
          "            issue",
          "            for issue, is_fixable in zip(",
          "                all_selected_issues,",
          "                are_fixable_output.are_fixable,",
          "                strict=True,",
          "            )",
          "            if is_fixable",
          "        ]",
          "        diagnostics.append(",
          "            {",
          "                \"component\": \"Relevant Warnings - Are Issues Fixable Component\",",
          "                \"fixable_issues\": [issue.id for issue in fixable_issues],",
          "            }",
          "        )",
          "",
          "        # 6. Suggest issues based on static analysis warnings and fixable issues.",
          "        static_analysis_suggestions_component = StaticAnalysisSuggestionsComponent(self.context)",
          "        static_analysis_suggestions_request = CodePredictStaticAnalysisSuggestionsRequest(",
          "            warning_and_pr_files=warning_and_pr_files,",
          "            fixable_issues=fixable_issues,",
          "            pr_files=pr_files,",
          "        )",
          "        static_analysis_suggestions_output: CodePredictStaticAnalysisSuggestionsOutput = (",
          "            static_analysis_suggestions_component.invoke(static_analysis_suggestions_request)",
          "        )",
          "",
          "        # 7. Save results.",
          "        self._complete_run(static_analysis_suggestions_output, diagnostics)",
          ""
        ]
      }
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 794,
      "name": "PGresult.nfields",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    },
    {
      "file": "psycopg/_transform.py",
      "image": "psycopg._transform",
      "is_application": false,
      "line": 353,
      "name": "Transformer.get_loader",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/_transform.py"
    },
    {
      "file": "psycopg/types/datetime.py",
      "image": "psycopg.types.datetime",
      "is_application": false,
      "line": 414,
      "name": "TimestampLoader.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/types/datetime.py"
    },
    {
      "file": "psycopg/types/datetime.py",
      "image": "psycopg.types.datetime",
      "is_application": false,
      "line": 670,
      "name": "_get_datestyle",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/types/datetime.py"
    },
    {
      "file": "psycopg/cursor.py",
      "image": "psycopg.cursor",
      "is_application": false,
      "line": 497,
      "name": "BaseCursor._check_results",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py"
    },
    {
      "file": "requests/api.py",
      "image": "requests.api",
      "is_application": false,
      "line": 115,
      "name": "post",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/api.py"
    },
    {
      "file": "requests/api.py",
      "image": "requests.api",
      "is_application": false,
      "line": 59,
      "name": "request",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/api.py"
    },
    {
      "file": "requests/sessions.py",
      "image": "requests.sessions",
      "is_application": false,
      "line": 481,
      "name": "Session.prepare_request",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/sessions.py"
    },
    {
      "file": "requests/utils.py",
      "image": "requests.utils",
      "is_application": false,
      "line": 227,
      "name": "get_netrc_auth",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/utils.py"
    },
    {
      "file": "<frozen genericpath>",
      "image": "genericpath",
      "is_application": true,
      "line": 19,
      "name": "exists",
      "path": "/app/<frozen genericpath>"
    },
    {
      "file": "requests/models.py",
      "image": "requests.models",
      "is_application": false,
      "line": 1023,
      "name": "Response.raise_for_status",
      "path": "/usr/local/lib/python3.11/dist-packages/requests/models.py"
    },
    {
      "file": "weakref.py",
      "image": "weakref",
      "is_application": false,
      "line": 590,
      "name": "finalize.__call__",
      "path": "/usr/lib/python3.11/weakref.py"
    },
    {
      "file": "urllib3/connectionpool.py",
      "image": "urllib3.connectionpool",
      "is_application": false,
      "line": 1137,
      "name": "_close_pool_connections",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py"
    },
    {
      "file": "seer/automation/codegen/codegen_event_manager.py",
      "image": "seer.automation.codegen.codegen_event_manager",
      "is_application": true,
      "line": 33,
      "name": "CodegenEventManager.mark_completed_and_extend_static_analysis_suggestions",
      "path": "/app/src/seer/automation/codegen/codegen_event_manager.py",
      "codeContext": {
        "file": "seer/automation/codegen/codegen_event_manager.py",
        "line": 33,
        "name": "CodegenEventManager.mark_completed_and_extend_static_analysis_suggestions",
        "code": "        pass\n\n    def append_file_change(self, file_change: FileChange):\n        with self.state.update() as current_state:\n            current_state.file_changes.append(file_change)\n\n    def mark_completed_and_extend_static_analysis_suggestions(\n        self, static_analysis_suggestions: list[StaticAnalysisSuggestion]\n    ):\n        with self.state.update() as cur:\n            cur.static_analysis_suggestions.extend(static_analysis_suggestions)\n            cur.completed_at = datetime.now()\n            cur.status = CodegenStatus.COMPLETED\n\n    def on_error(\n        self, error_msg: str = \"Something went wrong\", should_completely_error: bool = True\n    ):\n        with self.state.update() as cur:\n            cur.status = CodegenStatus.ERRORED\n",
        "lineRange": {
          "start": 24,
          "end": 43
        },
        "lines": [
          "import dataclasses",
          "from datetime import datetime",
          "",
          "from seer.automation.autofix.components.insight_sharing.models import InsightSharingOutput",
          "from seer.automation.codegen.models import CodegenStatus, StaticAnalysisSuggestion",
          "from seer.automation.codegen.state import CodegenContinuationState",
          "from seer.automation.models import FileChange",
          "",
          "",
          "@dataclasses.dataclass",
          "class CodegenEventManager:",
          "    state: CodegenContinuationState",
          "",
          "    def mark_running(self):",
          "        with self.state.update() as cur:",
          "            cur.status = CodegenStatus.IN_PROGRESS",
          "",
          "    def mark_completed(self):",
          "        with self.state.update() as cur:",
          "            cur.completed_at = datetime.now()",
          "            cur.status = CodegenStatus.COMPLETED",
          "",
          "    def add_log(self, message: str):",
          "        pass",
          "",
          "    def append_file_change(self, file_change: FileChange):",
          "        with self.state.update() as current_state:",
          "            current_state.file_changes.append(file_change)",
          "",
          "    def mark_completed_and_extend_static_analysis_suggestions(",
          "        self, static_analysis_suggestions: list[StaticAnalysisSuggestion]",
          "    ):",
          "        with self.state.update() as cur:",
          "            cur.static_analysis_suggestions.extend(static_analysis_suggestions)",
          "            cur.completed_at = datetime.now()",
          "            cur.status = CodegenStatus.COMPLETED",
          "",
          "    def on_error(",
          "        self, error_msg: str = \"Something went wrong\", should_completely_error: bool = True",
          "    ):",
          "        with self.state.update() as cur:",
          "            cur.status = CodegenStatus.ERRORED",
          "",
          "    def send_insight(self, insight: InsightSharingOutput):",
          "        # Do nothing for now, this is only used for autofix",
          "        pass",
          ""
        ]
      }
    },
    {
      "file": "sqlalchemy/engine/events.py",
      "image": "sqlalchemy.engine.events",
      "is_application": false,
      "line": 176,
      "name": "ConnectionEvents._listen.<locals>.wrap_before_cursor_execute",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/events.py"
    },
    {
      "file": "sentry_sdk/integrations/sqlalchemy.py",
      "image": "sentry_sdk.integrations.sqlalchemy",
      "is_application": false,
      "line": 57,
      "name": "_before_cursor_execute",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/integrations/sqlalchemy.py"
    },
    {
      "file": "sentry_sdk/tracing_utils.py",
      "image": "sentry_sdk.tracing_utils",
      "is_application": false,
      "line": 151,
      "name": "record_sql_queries",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/tracing_utils.py"
    },
    {
      "file": "sentry_sdk/api.py",
      "image": "sentry_sdk.api",
      "is_application": false,
      "line": 342,
      "name": "start_span",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/api.py"
    },
    {
      "file": "sentry_sdk/scope.py",
      "image": "sentry_sdk.scope",
      "is_application": false,
      "line": 1137,
      "name": "Scope.start_span",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/scope.py"
    },
    {
      "file": "sentry_sdk/tracing.py",
      "image": "sentry_sdk.tracing",
      "is_application": false,
      "line": 434,
      "name": "Span.start_child",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/tracing.py"
    },
    {
      "file": "sentry_sdk/tracing.py",
      "image": "sentry_sdk.tracing",
      "is_application": false,
      "line": 302,
      "name": "Span.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/tracing.py"
    },
    {
      "file": "uuid.py",
      "image": "uuid",
      "is_application": false,
      "line": 718,
      "name": "uuid4",
      "path": "/usr/lib/python3.11/uuid.py"
    },
    {
      "file": "seer/automation/codegen/codegen_context.py",
      "image": "seer.automation.codegen.codegen_context",
      "is_application": true,
      "line": 66,
      "name": "CodegenContext.get_repo_client",
      "path": "/app/src/seer/automation/codegen/codegen_context.py",
      "codeContext": {
        "file": "seer/automation/codegen/codegen_context.py",
        "line": 66,
        "name": "CodegenContext.get_repo_client",
        "code": "            state.signals = value\n\n    def get_repo_client(\n        self, repo_name: str | None = None, type: RepoClientType = RepoClientType.READ\n    ):\n        \"\"\"\n        Gets a repo client for the current single repo or for a given repo name.\n        If there are more than 1 repos, a repo name must be provided.\n        \"\"\"\n        return RepoClient.from_repo_definition(self.repo, type)\n\n    def get_file_contents(\n        self, path: str, repo_name: str | None = None, ignore_local_changes: bool = False\n    ) -> str | None:\n        repo_client = self.get_repo_client()\n\n        file_contents, _ = repo_client.get_file_content(path)\n\n        if not ignore_local_changes:\n            cur_state = self.state.get()",
        "lineRange": {
          "start": 57,
          "end": 76
        },
        "lines": [
          "import logging",
          "",
          "from seer.automation.agent.models import Message",
          "from seer.automation.codebase.repo_client import RepoClient, RepoClientType",
          "from seer.automation.codegen.codegen_event_manager import CodegenEventManager",
          "from seer.automation.codegen.models import CodegenContinuation, UnitTestRunMemory",
          "from seer.automation.codegen.state import CodegenContinuationState",
          "from seer.automation.models import RepoDefinition",
          "from seer.automation.pipeline import PipelineContext",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.db import DbPrContextToUnitTestGenerationRunIdMapping, DbRunMemory, Session",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "RepoExternalId = str",
          "RepoInternalId = int",
          "RepoKey = RepoExternalId | RepoInternalId",
          "RepoIdentifiers = tuple[RepoExternalId, RepoInternalId]",
          "",
          "",
          "class CodegenContext(PipelineContext):",
          "    state: CodegenContinuationState",
          "    event_manager: CodegenEventManager",
          "    repo: RepoDefinition",
          "",
          "    def __init__(",
          "        self,",
          "        state: CodegenContinuationState,",
          "    ):",
          "        request = state.get().request",
          "",
          "        self.repo = request.repo",
          "        self.state = state",
          "        self.event_manager = CodegenEventManager(state)",
          "",
          "        logger.info(f\"CodegenContext initialized with run_id {self.run_id}\")",
          "",
          "    @classmethod",
          "    def from_run_id(cls, run_id: int, type: DbStateRunTypes = DbStateRunTypes.UNIT_TEST):",
          "        state = CodegenContinuationState(run_id, model=CodegenContinuation, type=type)",
          "        with state.update() as cur:",
          "            cur.mark_triggered()",
          "",
          "        return cls(state)",
          "",
          "    @property",
          "    def run_id(self) -> int:",
          "        return self.state.get().run_id",
          "",
          "    @property",
          "    def signals(self) -> list[str]:",
          "        return self.state.get().signals",
          "",
          "    @signals.setter",
          "    def signals(self, value: list[str]):",
          "        with self.state.update() as state:",
          "            state.signals = value",
          "",
          "    def get_repo_client(",
          "        self, repo_name: str | None = None, type: RepoClientType = RepoClientType.READ",
          "    ):",
          "        \"\"\"",
          "        Gets a repo client for the current single repo or for a given repo name.",
          "        If there are more than 1 repos, a repo name must be provided.",
          "        \"\"\"",
          "        return RepoClient.from_repo_definition(self.repo, type)",
          "",
          "    def get_file_contents(",
          "        self, path: str, repo_name: str | None = None, ignore_local_changes: bool = False",
          "    ) -> str | None:",
          "        repo_client = self.get_repo_client()",
          "",
          "        file_contents, _ = repo_client.get_file_content(path)",
          "",
          "        if not ignore_local_changes:",
          "            cur_state = self.state.get()",
          "            current_file_changes = list(filter(lambda x: x.path == path, cur_state.file_changes))",
          "            for file_change in current_file_changes:",
          "                file_contents = file_change.apply(file_contents)",
          "",
          "        return file_contents",
          "",
          "    def store_memory(self, key: str, memory: list[Message]):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == self.run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                memory_model = UnitTestRunMemory(run_id=self.run_id)",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def update_stored_memory(self, key: str, memory: list[Message], original_run_id: int):",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory)",
          "                .where(DbRunMemory.run_id == original_run_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                raise RuntimeError(",
          "                    f\"No memory record found for run_id {original_run_id}. Cannot update stored memory.\"",
          "                )",
          "            else:",
          "                memory_model = UnitTestRunMemory.from_db_model(memory_record)",
          "",
          "            memory_model.memory[key] = memory",
          "            memory_record = memory_model.to_db_model()",
          "",
          "            session.merge(memory_record)",
          "            session.commit()",
          "",
          "    def get_memory(self, key: str, past_run_id: int) -> list[Message]:",
          "        with Session() as session:",
          "            memory_record = (",
          "                session.query(DbRunMemory).where(DbRunMemory.run_id == past_run_id).one_or_none()",
          "            )",
          "",
          "            if not memory_record:",
          "                return []",
          "",
          "            return UnitTestRunMemory.from_db_model(memory_record).memory.get(key, [])",
          "",
          "    def get_previous_run_context(",
          "        self, owner: str, repo: str, pr_id: int",
          "    ) -> DbPrContextToUnitTestGenerationRunIdMapping | None:",
          "        with Session() as session:",
          "            previous_context = (",
          "                session.query(DbPrContextToUnitTestGenerationRunIdMapping)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.owner == owner)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.repo == repo)",
          "                .where(DbPrContextToUnitTestGenerationRunIdMapping.pr_id == pr_id)",
          "                .one_or_none()",
          "            )",
          "",
          "            return previous_context",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codebase/repo_client.py",
      "image": "seer.automation.codebase.repo_client",
      "is_application": true,
      "line": 262,
      "name": "RepoClient.from_repo_definition",
      "path": "/app/src/seer/automation/codebase/repo_client.py",
      "codeContext": {
        "file": "seer/automation/codebase/repo_client.py",
        "line": 262,
        "name": "RepoClient.from_repo_definition",
        "code": "    @functools.lru_cache(maxsize=8)\n    def from_repo_definition(cls, repo_def: RepoDefinition, type: RepoClientType):\n        if type == RepoClientType.WRITE:\n            return cls(*get_write_app_credentials(), repo_def)\n        elif type == RepoClientType.CODECOV_UNIT_TEST:\n            return cls(*get_codecov_unit_test_app_credentials(), repo_def)\n        elif type in (RepoClientType.CODECOV_PR_REVIEW, RepoClientType.CODECOV_PR_CLOSED):\n            return cls(*get_codecov_pr_review_app_credentials(), repo_def)\n\n        return cls(*get_read_app_credentials(), repo_def)\n\n    @property\n    def repo_full_name(self):\n        return self.repo.full_name\n\n    def get_default_branch(self) -> str:\n        return self.repo.default_branch\n\n    def get_branch_head_sha(self, branch: str):\n        return self.repo.get_branch(branch).commit.sha",
        "lineRange": {
          "start": 253,
          "end": 272
        },
        "lines": [
          "import functools",
          "import logging",
          "import os",
          "import shutil",
          "import tarfile",
          "import tempfile",
          "import textwrap",
          "from concurrent.futures import ThreadPoolExecutor",
          "from enum import Enum",
          "from typing import Any, Dict, List, Literal",
          "",
          "import requests",
          "import sentry_sdk",
          "from github import (",
          "    Auth,",
          "    Github,",
          "    GithubException,",
          "    GithubIntegration,",
          "    GithubObject,",
          "    InputGitTreeElement,",
          "    UnknownObjectException,",
          ")",
          "from github.GitRef import GitRef",
          "from github.GitTree import GitTree",
          "from github.GitTreeElement import GitTreeElement",
          "from github.PullRequest import PullRequest",
          "from github.Repository import Repository",
          "",
          "from seer.automation.autofix.utils import generate_random_string, sanitize_branch_name",
          "from seer.automation.codebase.models import GithubPrReviewComment",
          "from seer.automation.codebase.utils import get_all_supported_extensions, get_language_from_path",
          "from seer.automation.models import FileChange, FilePatch, InitializationError, RepoDefinition",
          "from seer.automation.utils import detect_encoding",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "def get_github_app_auth_and_installation(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          "):",
          "    app_auth = Auth.AppAuth(app_id, private_key=private_key)",
          "    gi = GithubIntegration(auth=app_auth)",
          "    installation = gi.get_repo_installation(repo_owner, repo_name)",
          "    github_auth = app_auth.get_installation_auth(installation.id)",
          "",
          "    return github_auth, installation",
          "",
          "",
          "def get_repo_app_permissions(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          ") -> dict[str, str] | None:",
          "    try:",
          "        _, installation = get_github_app_auth_and_installation(",
          "            app_id, private_key, repo_owner, repo_name",
          "        )",
          "",
          "        return installation.raw_data.get(\"permissions\", {})",
          "    except UnknownObjectException:",
          "        return None",
          "",
          "",
          "@inject",
          "def get_github_token_auth(config: AppConfig = injected) -> Auth.Token | None:",
          "    github_token = config.GITHUB_TOKEN",
          "    if github_token is None:",
          "        return None",
          "",
          "    return Auth.Token(github_token)",
          "",
          "",
          "@inject",
          "def get_write_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_APP_ID",
          "    private_key = config.GITHUB_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return None, None",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_read_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_SENTRY_APP_ID",
          "    private_key = config.GITHUB_SENTRY_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_unit_test_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_UNIT_TEST_APP_ID",
          "    private_key = config.GITHUB_CODECOV_UNIT_TEST_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov unit test app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_pr_review_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_PR_REVIEW_APP_ID",
          "    private_key = config.GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY",
          "",
          "    if not app_id:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_APP_ID\")",
          "    if not private_key:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY\")",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov pr review app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "class RepoClientType(str, Enum):",
          "    READ = \"read\"",
          "    WRITE = \"write\"",
          "    CODECOV_UNIT_TEST = \"codecov_unit_test\"",
          "    CODECOV_PR_REVIEW = \"codecov_pr_review\"",
          "    CODECOV_PR_CLOSED = \"codecov_pr_closed\"",
          "",
          "",
          "class CompleteGitTree:",
          "    \"\"\"",
          "    A custom class that mimics the interface of github.GitTree",
          "    but allows combining multiple trees into one complete representation.",
          "    \"\"\"",
          "",
          "    def __init__(self, github_tree: GitTree | None = None) -> None:",
          "        self.tree: List[GitTreeElement] = []",
          "        self.raw_data: Dict[str, Any] = {\"truncated\": False}",
          "",
          "        if github_tree:",
          "            self.add_items(github_tree.tree)",
          "            for key, value in github_tree.raw_data.items():",
          "                if key != \"truncated\":  # We always set truncated to False for our complete tree",
          "                    self.raw_data[key] = value",
          "",
          "    def add_item(self, item: GitTreeElement) -> None:",
          "        \"\"\"Add a tree item to this collection\"\"\"",
          "        self.tree.append(item)",
          "",
          "    def add_items(self, items: List[GitTreeElement]) -> None:",
          "        \"\"\"Add multiple tree items to this collection\"\"\"",
          "        self.tree.extend(items)",
          "",
          "",
          "class RepoClient:",
          "    # TODO: Support other git providers later",
          "    github_auth: Auth.Token | Auth.AppInstallationAuth",
          "    github: Github",
          "    repo: Repository",
          "",
          "    provider: str",
          "    repo_owner: str",
          "    repo_name: str",
          "    repo_external_id: str",
          "    base_commit_sha: str",
          "    base_branch: str",
          "",
          "    supported_providers = [\"github\"]",
          "",
          "    def __init__(",
          "        self, app_id: int | str | None, private_key: str | None, repo_definition: RepoDefinition",
          "    ):",
          "        if repo_definition.provider not in self.supported_providers:",
          "            # This should never get here, the repo provider should be checked on the Sentry side but this will make debugging",
          "            # easier if it does",
          "            raise InitializationError(",
          "                f\"Unsupported repo provider: {repo_definition.provider}, only {', '.join(self.supported_providers)} are supported.\"",
          "            )",
          "",
          "        if app_id and private_key:",
          "            self.github = Github(",
          "                auth=get_github_app_auth_and_installation(",
          "                    app_id, private_key, repo_definition.owner, repo_definition.name",
          "                )[0]",
          "            )",
          "        else:",
          "            self.github = Github(auth=get_github_token_auth())",
          "",
          "        self.repo = self.github.get_repo(",
          "            int(repo_definition.external_id)",
          "            if repo_definition.external_id.isdigit()",
          "            else repo_definition.full_name",
          "        )",
          "",
          "        self.provider = repo_definition.provider",
          "        self.repo_owner = repo_definition.owner",
          "        self.repo_name = repo_definition.name",
          "        self.repo_external_id = repo_definition.external_id",
          "        self.base_branch = repo_definition.branch_name or self.get_default_branch()",
          "        self.base_commit_sha = repo_definition.base_commit_sha or self.get_branch_head_sha(",
          "            self.base_branch",
          "        )",
          "",
          "    @staticmethod",
          "    def check_repo_write_access(repo: RepoDefinition):",
          "        app_id, pk = get_write_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if (",
          "            permissions",
          "            and permissions.get(\"contents\") == \"write\"",
          "            and permissions.get(\"pull_requests\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def check_repo_read_access(repo: RepoDefinition):",
          "        app_id, pk = get_read_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if permissions and (",
          "            permissions.get(\"contents\") == \"read\" or permissions.get(\"contents\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def _extract_id_from_pr_url(pr_url: str):",
          "        \"\"\"",
          "        Extracts the repository path and PR/issue ID from the provided URL.",
          "        \"\"\"",
          "        pr_id = int(pr_url.split(\"/\")[-1])",
          "        return pr_id",
          "",
          "    @classmethod",
          "    @functools.lru_cache(maxsize=8)",
          "    def from_repo_definition(cls, repo_def: RepoDefinition, type: RepoClientType):",
          "        if type == RepoClientType.WRITE:",
          "            return cls(*get_write_app_credentials(), repo_def)",
          "        elif type == RepoClientType.CODECOV_UNIT_TEST:",
          "            return cls(*get_codecov_unit_test_app_credentials(), repo_def)",
          "        elif type in (RepoClientType.CODECOV_PR_REVIEW, RepoClientType.CODECOV_PR_CLOSED):",
          "            return cls(*get_codecov_pr_review_app_credentials(), repo_def)",
          "",
          "        return cls(*get_read_app_credentials(), repo_def)",
          "",
          "    @property",
          "    def repo_full_name(self):",
          "        return self.repo.full_name",
          "",
          "    def get_default_branch(self) -> str:",
          "        return self.repo.default_branch",
          "",
          "    def get_branch_head_sha(self, branch: str):",
          "        return self.repo.get_branch(branch).commit.sha",
          "",
          "    def compare(self, base: str, head: str):",
          "        return self.repo.compare(base, head)",
          "",
          "    def load_repo_to_tmp_dir(self, sha: str | None = None) -> tuple[str, str]:",
          "        sha = sha or self.base_commit_sha",
          "",
          "        # Check if output directory exists, if not create it",
          "        tmp_dir = tempfile.mkdtemp(prefix=f\"{self.repo_owner}-{self.repo_name}_{sha}\")",
          "        tmp_repo_dir = os.path.join(tmp_dir, \"repo\")",
          "",
          "        logger.debug(f\"Loading repository to {tmp_repo_dir}\")",
          "",
          "        # Create a temporary directory to store the repository",
          "        os.makedirs(tmp_repo_dir, exist_ok=True)",
          "",
          "        # Clean the directory",
          "        for root, dirs, files in os.walk(tmp_repo_dir, topdown=False):",
          "            for name in files:",
          "                os.remove(os.path.join(root, name))",
          "            for name in dirs:",
          "                os.rmdir(os.path.join(root, name))",
          "",
          "        tarball_url = self.repo.get_archive_link(\"tarball\", ref=sha)",
          "        tarfile_path = os.path.join(tmp_dir, f\"{sha}.tar.gz\")",
          "",
          "        response = requests.get(tarball_url, stream=True)",
          "        if response.status_code == 200:",
          "            with open(tarfile_path, \"wb\") as f:",
          "                f.write(response.content)",
          "        else:",
          "            logger.error(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "            logger.error(",
          "                f\"Response status code: {response.status_code}, response text: {response.text}\"",
          "            )",
          "            raise Exception(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "",
          "        # Extract tarball into the output directory",
          "        with tarfile.open(tarfile_path, \"r:gz\") as tar:",
          "            tar.extractall(path=tmp_repo_dir)  # extract all members normally",
          "            extracted_folders = [",
          "                name",
          "                for name in os.listdir(tmp_repo_dir)",
          "                if os.path.isdir(os.path.join(tmp_repo_dir, name))",
          "            ]",
          "            if extracted_folders:",
          "                root_folder = extracted_folders[0]  # assuming the first folder is the root folder",
          "                root_folder_path = os.path.join(tmp_repo_dir, root_folder)",
          "                for item in os.listdir(root_folder_path):",
          "                    s = os.path.join(root_folder_path, item)",
          "                    d = os.path.join(tmp_repo_dir, item)",
          "                    # TODO: Consider a strategy for handling symlinks more appropriately in the future, possibly by resolving them or copying as symlinks to maintain the original structure.",
          "                    if os.path.isdir(s):",
          "                        shutil.move(",
          "                            s, d",
          "                        )  # move all directories from the root folder to the output directory",
          "                    else:",
          "                        # Skipping symlinks to prevent FileNotFoundError.",
          "                        if not os.path.islink(s):",
          "                            shutil.copy2(",
          "                                s, d",
          "                            )  # copy all files from the root folder to the output directory",
          "",
          "                shutil.rmtree(root_folder_path)  # remove the root folder",
          "",
          "        return tmp_dir, tmp_repo_dir",
          "",
          "    def _autocorrect_path(self, path: str, sha: str | None = None) -> tuple[str, bool]:",
          "        \"\"\"",
          "        Attempts to autocorrect a file path by finding the closest match in the repository.",
          "",
          "        Args:",
          "            path: The path to autocorrect",
          "            sha: The commit SHA to use for finding valid paths",
          "",
          "        Returns:",
          "            A tuple of (corrected_path, was_autocorrected)",
          "        \"\"\"",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        path = path.lstrip(\"/\")",
          "        valid_paths = self.get_valid_file_paths(sha)",
          "",
          "        # If path is valid, return it unchanged",
          "        if path in valid_paths:",
          "            return path, False",
          "",
          "        # Check for partial matches if no exact match and path is long enough",
          "        if len(path) > 3:",
          "            path_lower = path.lower()",
          "            partial_matches = [",
          "                valid_path for valid_path in valid_paths if path_lower in valid_path.lower()",
          "            ]",
          "            if partial_matches:",
          "                # Sort by length to get closest match (shortest containing path)",
          "                closest_match = sorted(partial_matches, key=len)[0]",
          "                logger.warning(",
          "                    f\"Path '{path}' not found exactly, using closest match: '{closest_match}'\"",
          "                )",
          "                return closest_match, True",
          "",
          "        # No match found",
          "        logger.warning(\"No matching file found for provided file path\", extra={\"path\": path})",
          "        return path, False",
          "",
          "    def get_file_content(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False",
          "    ) -> tuple[str | None, str]:",
          "        logger.debug(f\"Getting file contents for {path} in {self.repo.full_name} on sha {sha}\")",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        autocorrected_path = False",
          "        if autocorrect:",
          "            path, autocorrected_path = self._autocorrect_path(path, sha)",
          "            if not autocorrected_path and path not in self.get_valid_file_paths(sha):",
          "                return None, \"utf-8\"",
          "",
          "        try:",
          "            contents = self.repo.get_contents(path, ref=sha)",
          "",
          "            if isinstance(contents, list):",
          "                raise Exception(f\"Expected a single ContentFile but got a list for path {path}\")",
          "",
          "            detected_encoding = detect_encoding(contents.decoded_content) if contents else \"utf-8\"",
          "            content = contents.decoded_content.decode(detected_encoding)",
          "            if autocorrected_path:",
          "                content = f\"Showing results instead for {path}\\n=====\\n{content}\"",
          "            return content, detected_encoding",
          "        except Exception as e:",
          "            logger.exception(f\"Error getting file contents: {e}\")",
          "            return None, \"utf-8\"",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_valid_file_paths(self, sha: str | None = None) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        valid_file_paths: set[str] = set()",
          "        valid_file_extensions = get_all_supported_extensions()",
          "",
          "        for file in tree.tree:",
          "            if file.type == \"blob\" and any(",
          "                file.path.endswith(ext) for ext in valid_file_extensions",
          "            ):",
          "                valid_file_paths.add(file.path)",
          "",
          "        return valid_file_paths",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_history(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False, max_commits: int = 10",
          "    ) -> list[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(sha):",
          "                return []",
          "",
          "        commits = self.repo.get_commits(sha=sha, path=path)",
          "        commit_list = list(commits[:max_commits])",
          "        commit_strs = []",
          "",
          "        def process_commit(commit):",
          "            short_sha = commit.sha[:7]",
          "            message = commit.commit.message",
          "            files_touched = [",
          "                {\"path\": file.filename, \"status\": file.status} for file in commit.files[:20]",
          "            ]",
          "",
          "            # Build a file tree representation instead of a flat list",
          "            file_tree_str = self._build_file_tree_string(files_touched)",
          "",
          "            # Add a note about additional files if needed",
          "            additional_files_note = \"\"",
          "            if len(files_touched) < len(commit.files):",
          "                additional_files_note = (",
          "                    f\"\\n[and {len(commit.files) - len(files_touched)} more files were changed...]\"",
          "                )",
          "",
          "            string = textwrap.dedent(",
          "                \"\"\"\\",
          "                ----------------",
          "                {short_sha} - {message}",
          "                Files touched:",
          "                {file_tree}{additional_files}",
          "                \"\"\"",
          "            ).format(",
          "                short_sha=short_sha,",
          "                message=message,",
          "                file_tree=file_tree_str,",
          "                additional_files=additional_files_note,",
          "            )",
          "            return string",
          "",
          "        with ThreadPoolExecutor() as executor:",
          "            results = list(executor.map(process_commit, commit_list))",
          "",
          "        for result in results:",
          "            commit_strs.append(result)",
          "",
          "        return commit_strs",
          "",
          "    def _build_file_tree_string(",
          "        self, files: list[dict], only_immediate_children_of_path: str | None = None",
          "    ) -> str:",
          "        \"\"\"",
          "        Builds a tree representation of files to save tokens when many files share the same directories.",
          "        The output is similar to the 'tree' command in terminal.",
          "",
          "        Args:",
          "            files: List of dictionaries with 'path' and 'status' keys",
          "            only_immediate_children_of_path: If provided, only include files and directories that are immediate children of this path",
          "",
          "        Returns:",
          "            A string representation of the file tree",
          "        \"\"\"",
          "        if not files:",
          "            return \"No files changed\"",
          "",
          "        if only_immediate_children_of_path is not None:",
          "            only_immediate_children_of_path = only_immediate_children_of_path.rstrip(\"/\")",
          "",
          "        # First, build a nested dictionary structure representing the file tree",
          "        tree: dict = {}",
          "        for file in files:",
          "            path = file[\"path\"]",
          "            status = file[\"status\"]",
          "",
          "            # Split the path into components",
          "            parts = path.split(\"/\")",
          "",
          "            # Navigate through the tree, creating nodes as needed",
          "            current = tree",
          "            for i, part in enumerate(parts):",
          "                # If this is the last part (the filename)",
          "                if i == len(parts) - 1:",
          "                    current[part] = {\"__status__\": status}",
          "                else:",
          "                    if part not in current:",
          "                        current[part] = {}",
          "                    current = current[part]",
          "",
          "        # Now build the tree string recursively",
          "        lines = []",
          "",
          "        def _build_tree(node, previous_parts=[], prefix=\"\", is_last=True, is_root=True):",
          "            items = list(node.items())",
          "",
          "            # Process each item",
          "            for i, (key, value) in enumerate(sorted(items)):",
          "                if key == \"__status__\":",
          "                    continue",
          "",
          "                # Determine if this is the last item at this level",
          "                is_last_item = i == len(items) - 1 or (i == len(items) - 2 and \"__status__\" in node)",
          "",
          "                # Create the appropriate prefix for this line",
          "                if is_root:",
          "                    current_prefix = \"\"",
          "                    next_prefix = \"\"",
          "                else:",
          "                    current_prefix = prefix + (\"└── \" if is_last else \"├── \")",
          "                    next_prefix = prefix + (\"    \" if is_last else \"│   \")",
          "",
          "                # If this is a file (has a status)",
          "                if \"__status__\" in value:",
          "                    if (",
          "                        only_immediate_children_of_path is not None",
          "                        and not only_immediate_children_of_path == (\"/\".join(previous_parts))",
          "                    ):",
          "                        continue",
          "",
          "                    status = value[\"__status__\"]",
          "                    status_str = f\" ({status})\" if status else \"\"",
          "                    lines.append(f\"{current_prefix}{key}{status_str}\")",
          "                # If this is a directory",
          "                else:",
          "                    # If this is within the specified path",
          "                    if (",
          "                        only_immediate_children_of_path is None",
          "                        or only_immediate_children_of_path.startswith(",
          "                            \"/\".join(previous_parts + [key])",
          "                        )",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "                        _build_tree(value, previous_parts + [key], next_prefix, is_last_item, False)",
          "                    elif (",
          "                        only_immediate_children_of_path is not None",
          "                        and only_immediate_children_of_path == \"/\".join(previous_parts)",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "",
          "        # Start building the tree from the root",
          "        _build_tree(tree)",
          "",
          "        return \"\\n\".join(lines)",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_patch_for_file(",
          "        self, path: str, commit_sha: str, autocorrect: bool = False",
          "    ) -> str | None:",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, commit_sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(commit_sha):",
          "                return None",
          "",
          "        commit = self.repo.get_commit(commit_sha)",
          "        matching_file = next((file for file in commit.files if file.filename == path), None)",
          "        if not matching_file:",
          "            return None",
          "",
          "        return matching_file.patch",
          "",
          "    def _create_branch(self, branch_name, from_base_sha=False):",
          "        ref = self.repo.create_git_ref(",
          "            ref=f\"refs/heads/{branch_name}\",",
          "            sha=(",
          "                self.base_commit_sha",
          "                if from_base_sha",
          "                else self.get_branch_head_sha(self.base_branch)",
          "            ),",
          "        )",
          "        return ref",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_git_tree(self, sha: str) -> CompleteGitTree:",
          "        \"\"\"",
          "        Get the git tree for a specific sha, handling truncation with divide and conquer.",
          "        Always returns a CompleteGitTree instance for consistent interface.",
          "",
          "        First tries to get the complete tree recursively. If truncated, it uses a",
          "        divide and conquer approach to fetch all subtrees individually and combine them.",
          "",
          "        Args:",
          "            sha: The commit SHA to get the tree for",
          "",
          "        Returns:",
          "            A CompleteGitTree with all items from all subtrees",
          "        \"\"\"",
          "        tree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not tree.raw_data.get(\"truncated\", False):",
          "            return CompleteGitTree(tree)",
          "",
          "        complete_tree = CompleteGitTree()",
          "        root_tree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        for key, value in root_tree.raw_data.items():",
          "            if key != \"tree\" and key != \"truncated\":",
          "                complete_tree.raw_data[key] = value",
          "",
          "        for item in root_tree.tree:",
          "            complete_tree.add_item(item)",
          "",
          "        tree_items = [item for item in root_tree.tree if item.type == \"tree\"]",
          "        with ThreadPoolExecutor() as executor:",
          "            subtree_results = []",
          "            for item in tree_items:",
          "                subtree_results.append(executor.submit(self._get_git_subtree, item.sha))",
          "",
          "            for future in subtree_results:",
          "                subtree_items = future.result()",
          "                complete_tree.add_items(subtree_items)",
          "",
          "        return complete_tree",
          "",
          "    def _get_git_subtree(self, sha: str) -> list:",
          "        \"\"\"",
          "        Process a subtree and return all its items for parallel execution.",
          "",
          "        Args:",
          "            sha: The SHA of the subtree",
          "",
          "        Returns:",
          "            A list of all tree items from this subtree and its nested subtrees",
          "        \"\"\"",
          "        items = []",
          "        subtree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not subtree.raw_data.get(\"truncated\", False):",
          "            return subtree.tree",
          "",
          "        non_recursive_subtree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        nested_tree_items = [item for item in non_recursive_subtree.tree if item.type == \"tree\"]",
          "        non_tree_items = [item for item in non_recursive_subtree.tree if item.type != \"tree\"]",
          "",
          "        items.extend(non_tree_items)",
          "",
          "        if nested_tree_items:",
          "            with ThreadPoolExecutor() as executor:",
          "                subtree_futures = [",
          "                    executor.submit(self._get_git_subtree, item.sha) for item in nested_tree_items",
          "                ]",
          "",
          "                for future in subtree_futures:",
          "                    items.extend(future.result())",
          "",
          "        return items",
          "",
          "    def process_one_file_for_git_commit(",
          "        self, *, branch_ref: str, patch: FilePatch | None = None, change: FileChange | None = None",
          "    ) -> InputGitTreeElement | None:",
          "        \"\"\"",
          "        This method is used to get a single change to be committed by to github.",
          "        It processes a FilePatch/FileChange object and converts it into an InputGitTreeElement which can be commited",
          "        It supports both FilePatch and FileChange objects.",
          "        \"\"\"",
          "        path = patch.path if patch else (change.path if change else None)",
          "        patch_type = patch.type if patch else (change.change_type if change else None)",
          "        if not path:",
          "            raise ValueError(\"Path must be provided\")",
          "",
          "        if not patch_type:",
          "            raise ValueError(\"Patch type must be provided\")",
          "        if patch_type == \"create\":",
          "            patch_type = \"A\"",
          "        elif patch_type == \"delete\":",
          "            patch_type = \"D\"",
          "        elif patch_type == \"edit\":",
          "            patch_type = \"M\"",
          "",
          "        to_apply = None",
          "        detected_encoding = \"utf-8\"",
          "        if patch_type != \"A\":",
          "            to_apply, detected_encoding = self.get_file_content(path, sha=branch_ref)",
          "",
          "        new_contents = (",
          "            patch.apply(to_apply) if patch else (change.apply(to_apply) if change else None)",
          "        )",
          "",
          "        # Remove leading slash if it exists, the github api will reject paths with leading slashes.",
          "        if path.startswith(\"/\"):",
          "            path = path[1:]",
          "",
          "        # don't create a blob if the file is being deleted",
          "        blob = self.repo.create_git_blob(new_contents, detected_encoding) if new_contents else None",
          "",
          "        # Prevent creating tree elements with None SHA for file additions",
          "        if patch_type == \"A\" and blob is None:",
          "            return None",
          "",
          "        # 100644 is the git code for creating a Regular non-executable file",
          "        # https://stackoverflow.com/questions/737673/how-to-read-the-mode-field-of-git-ls-trees-output",
          "        return InputGitTreeElement(",
          "            path=path, mode=\"100644\", type=\"blob\", sha=blob.sha if blob else None",
          "        )",
          "",
          "    def get_branch_ref(self, branch_name: str) -> GitRef | None:",
          "        try:",
          "            return self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        except GithubException as e:",
          "            if e.status == 404:",
          "                return None",
          "            raise e",
          "",
          "    def create_branch_from_changes(",
          "        self,",
          "        *,",
          "        pr_title: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "        branch_name: str | None = None,",
          "        from_base_sha: bool = False,",
          "    ) -> GitRef | None:",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Either file_patches or file_changes must be provided\")",
          "",
          "        new_branch_name = sanitize_branch_name(branch_name or pr_title)",
          "",
          "        try:",
          "            branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "        except GithubException as e:",
          "            # only use the random suffix if the branch already exists",
          "            if e.status == 409 or e.status == 422:",
          "                new_branch_name = f\"{new_branch_name}-{generate_random_string(n=6)}\"",
          "                branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "            else:",
          "                raise e",
          "",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, patch=patch",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file patch: {e}\")",
          "",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, change=change",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file change: {e}\")",
          "        # latest commit is the head of new branch",
          "        latest_commit = self.repo.get_git_commit(self.get_branch_head_sha(new_branch_name))",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "",
          "        new_commit = self.repo.create_git_commit(",
          "            message=pr_title, tree=new_tree, parents=[latest_commit]",
          "        )",
          "",
          "        branch_ref.edit(sha=new_commit.sha)",
          "",
          "        # Check that the changes were made",
          "        comparison = self.repo.compare(",
          "            self.get_branch_head_sha(self.base_branch), branch_ref.object.sha",
          "        )",
          "",
          "        if comparison.ahead_by < 1:",
          "            # Remove the branch if there are no changes",
          "            try:",
          "                branch_ref.delete()",
          "            except UnknownObjectException:",
          "                logger.warning(\"Attempted to delete a branch or reference that does not exist.\")",
          "            sentry_sdk.capture_message(",
          "                f\"Failed to create branch from changes. Comparison is ahead by {comparison.ahead_by}\"",
          "            )",
          "            return None",
          "",
          "        return branch_ref",
          "",
          "    def create_pr_from_branch(",
          "        self,",
          "        branch: GitRef,",
          "        title: str,",
          "        description: str,",
          "        provided_base: str | None = None,",
          "    ) -> PullRequest:",
          "        pulls = self.repo.get_pulls(state=\"open\", head=f\"{self.repo_owner}:{branch.ref}\")",
          "",
          "        if pulls.totalCount > 0:",
          "            logger.error(",
          "                f\"Branch {branch.ref} already has an open PR.\",",
          "                extra={",
          "                    \"branch_ref\": branch.ref,",
          "                    \"title\": title,",
          "                    \"description\": description,",
          "                    \"provided_base\": provided_base,",
          "                },",
          "            )",
          "",
          "            return pulls[0]",
          "",
          "        try:",
          "            return self.repo.create_pull(",
          "                title=title,",
          "                body=description,",
          "                base=provided_base or self.base_branch or self.get_default_branch(),",
          "                head=branch.ref,",
          "                draft=True,",
          "            )",
          "        except GithubException as e:",
          "            if e.status == 422 and \"Draft pull requests are not supported\" in str(e):",
          "                # fallback to creating a regular PR if draft PR is not supported",
          "                return self.repo.create_pull(",
          "                    title=title,",
          "                    body=description,",
          "                    base=provided_base or self.base_branch or self.get_default_branch(),",
          "                    head=branch.ref,",
          "                    draft=False,",
          "                )",
          "            else:",
          "                logger.exception(\"Error creating PR\")",
          "                raise e",
          "",
          "    def get_index_file_set(",
          "        self, sha: str | None = None, max_file_size_bytes=2 * 1024 * 1024, skip_empty_files=False",
          "    ) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        file_set = set()",
          "        for file in tree.tree:",
          "            if (",
          "                file.type == \"blob\"",
          "                and file.size < max_file_size_bytes",
          "                and file.mode",
          "                in [\"100644\", \"100755\"]  # 100644 is a regular file, 100755 is an executable file",
          "                and get_language_from_path(file.path) is not None",
          "                and (not skip_empty_files or file.size > 0)",
          "            ):",
          "                file_set.add(file.path)",
          "",
          "        return file_set",
          "",
          "    def get_pr_diff_content(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"diff\"))",
          "",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.text",
          "",
          "    def _get_auth_headers(self, accept_type: Literal[\"json\", \"diff\"] = \"json\"):",
          "        requester = self.repo._requester",
          "        if requester.auth is None:",
          "            raise Exception(\"No auth token found for GitHub API\")",
          "        headers = {",
          "            \"Accept\": (",
          "                \"application/vnd.github.diff\"",
          "                if accept_type == \"diff\"",
          "                else \"application/vnd.github+json\"",
          "            ),",
          "            \"Authorization\": f\"Bearer {requester.auth.token}\",",
          "            \"X-GitHub-Api-Version\": \"2022-11-28\",",
          "        }",
          "        return headers",
          "",
          "    def comment_root_cause_on_pr_for_copilot(",
          "        self, pr_url: str, run_id: int, issue_id: int, comment: str",
          "    ):",
          "        pull_id = int(pr_url.split(\"/\")[-1])",
          "        repo_name = pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "        params = {",
          "            \"body\": comment,",
          "            \"actions\": [",
          "                {",
          "                    \"name\": \"Fix with Sentry\",",
          "                    \"type\": \"copilot-chat\",",
          "                    \"prompt\": f\"@sentry find a fix for issue {issue_id} with run ID {run_id}\",",
          "                }",
          "            ],",
          "        }",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def comment_pr_generated_for_copilot(",
          "        self, pr_to_comment_on_url: str, new_pr_url: str, run_id: int",
          "    ):",
          "        pull_id = int(pr_to_comment_on_url.split(\"/\")[-1])",
          "        repo_name = pr_to_comment_on_url.split(\"github.com/\")[1].split(\"/pull\")[",
          "            0",
          "        ]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "",
          "        comment = f\"A fix has been generated and is available [here]({new_pr_url}) for your review. Autofix Run ID: {run_id}\"",
          "",
          "        params = {\"body\": comment}",
          "",
          "        headers = self._get_auth_headers()",
          "",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def get_pr_head_sha(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"json\"))",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.json()[\"head\"][\"sha\"]",
          "",
          "    def post_unit_test_reference_to_original_pr(self, original_pr_url: str, unit_test_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Sentry has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_reference_to_original_pr_codecov_app(",
          "        self, original_pr_url: str, unit_test_pr_url: str",
          "    ):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Codecov has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_not_generated_message_to_original_pr(self, original_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = \"Sentry has determined that unit tests are not necessary for this PR.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_issue_comment(self, pr_url: str, comment: str):",
          "        \"\"\"",
          "        Create an issue comment on a GitHub issue (all pull requests are issues).",
          "        This can be used to create an overall PR comment instead of associated with a specific line.",
          "        See https://docs.github.com/en/rest/issues/comments?apiVersion=2022-11-28#create-an-issue-comment",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        issue = self.repo.get_issue(number=pr_id)",
          "        comment_obj = issue.create_comment(body=comment)",
          "        return comment_obj.html_url",
          "",
          "    def post_pr_review_comment(self, pr_url: str, comment: GithubPrReviewComment):",
          "        \"\"\"",
          "        Create a review comment on a GitHub pull request.",
          "        See https://docs.github.com/en/rest/pulls/comments?apiVersion=2022-11-28#create-a-review-comment-for-a-pull-request",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        pr = self.repo.get_pull(number=pr_id)",
          "        commit = self.repo.get_commit(comment[\"commit_id\"])",
          "",
          "        review_comment = pr.create_review_comment(",
          "            body=comment[\"body\"],",
          "            commit=commit,",
          "            path=comment[\"path\"],",
          "            line=comment.get(\"line\", GithubObject.NotSet),",
          "            side=comment.get(\"side\", GithubObject.NotSet),",
          "            start_line=comment.get(\"start_line\", GithubObject.NotSet),",
          "        )",
          "        return review_comment.html_url",
          "",
          "    def push_new_commit_to_pr(",
          "        self,",
          "        pr,",
          "        commit_message: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "    ):",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Must provide file_patches or file_changes\")",
          "        branch_name = pr.head.ref",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                element = self.process_one_file_for_git_commit(branch_ref=branch_name, patch=patch)",
          "                if element:",
          "                    tree_elements.append(element)",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                element = self.process_one_file_for_git_commit(",
          "                    branch_ref=branch_name, change=change",
          "                )",
          "                if element:",
          "                    tree_elements.append(element)",
          "        if not tree_elements:",
          "            logger.warning(\"No valid changes to commit\")",
          "            return None",
          "        latest_sha = self.get_branch_head_sha(branch_name)",
          "        latest_commit = self.repo.get_git_commit(latest_sha)",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "        new_commit = self.repo.create_git_commit(",
          "            message=commit_message, tree=new_tree, parents=[latest_commit]",
          "        )",
          "        branch_ref = self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        branch_ref.edit(sha=new_commit.sha)",
          "        return new_commit",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codebase/repo_client.py",
      "image": "seer.automation.codebase.repo_client",
      "is_application": true,
      "line": 188,
      "name": "RepoClient.__init__",
      "path": "/app/src/seer/automation/codebase/repo_client.py",
      "codeContext": {
        "file": "seer/automation/codebase/repo_client.py",
        "line": 188,
        "name": "RepoClient.__init__",
        "code": "        if repo_definition.provider not in self.supported_providers:\n            # This should never get here, the repo provider should be checked on the Sentry side but this will make debugging\n            # easier if it does\n            raise InitializationError(\n                f\"Unsupported repo provider: {repo_definition.provider}, only {', '.join(self.supported_providers)} are supported.\"\n            )\n\n        if app_id and private_key:\n            self.github = Github(\n                auth=get_github_app_auth_and_installation(\n                    app_id, private_key, repo_definition.owner, repo_definition.name\n                )[0]\n            )\n        else:\n            self.github = Github(auth=get_github_token_auth())\n\n        self.repo = self.github.get_repo(\n            int(repo_definition.external_id)\n            if repo_definition.external_id.isdigit()\n            else repo_definition.full_name",
        "lineRange": {
          "start": 179,
          "end": 198
        },
        "lines": [
          "import functools",
          "import logging",
          "import os",
          "import shutil",
          "import tarfile",
          "import tempfile",
          "import textwrap",
          "from concurrent.futures import ThreadPoolExecutor",
          "from enum import Enum",
          "from typing import Any, Dict, List, Literal",
          "",
          "import requests",
          "import sentry_sdk",
          "from github import (",
          "    Auth,",
          "    Github,",
          "    GithubException,",
          "    GithubIntegration,",
          "    GithubObject,",
          "    InputGitTreeElement,",
          "    UnknownObjectException,",
          ")",
          "from github.GitRef import GitRef",
          "from github.GitTree import GitTree",
          "from github.GitTreeElement import GitTreeElement",
          "from github.PullRequest import PullRequest",
          "from github.Repository import Repository",
          "",
          "from seer.automation.autofix.utils import generate_random_string, sanitize_branch_name",
          "from seer.automation.codebase.models import GithubPrReviewComment",
          "from seer.automation.codebase.utils import get_all_supported_extensions, get_language_from_path",
          "from seer.automation.models import FileChange, FilePatch, InitializationError, RepoDefinition",
          "from seer.automation.utils import detect_encoding",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "def get_github_app_auth_and_installation(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          "):",
          "    app_auth = Auth.AppAuth(app_id, private_key=private_key)",
          "    gi = GithubIntegration(auth=app_auth)",
          "    installation = gi.get_repo_installation(repo_owner, repo_name)",
          "    github_auth = app_auth.get_installation_auth(installation.id)",
          "",
          "    return github_auth, installation",
          "",
          "",
          "def get_repo_app_permissions(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          ") -> dict[str, str] | None:",
          "    try:",
          "        _, installation = get_github_app_auth_and_installation(",
          "            app_id, private_key, repo_owner, repo_name",
          "        )",
          "",
          "        return installation.raw_data.get(\"permissions\", {})",
          "    except UnknownObjectException:",
          "        return None",
          "",
          "",
          "@inject",
          "def get_github_token_auth(config: AppConfig = injected) -> Auth.Token | None:",
          "    github_token = config.GITHUB_TOKEN",
          "    if github_token is None:",
          "        return None",
          "",
          "    return Auth.Token(github_token)",
          "",
          "",
          "@inject",
          "def get_write_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_APP_ID",
          "    private_key = config.GITHUB_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return None, None",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_read_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_SENTRY_APP_ID",
          "    private_key = config.GITHUB_SENTRY_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_unit_test_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_UNIT_TEST_APP_ID",
          "    private_key = config.GITHUB_CODECOV_UNIT_TEST_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov unit test app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_pr_review_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_PR_REVIEW_APP_ID",
          "    private_key = config.GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY",
          "",
          "    if not app_id:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_APP_ID\")",
          "    if not private_key:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY\")",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov pr review app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "class RepoClientType(str, Enum):",
          "    READ = \"read\"",
          "    WRITE = \"write\"",
          "    CODECOV_UNIT_TEST = \"codecov_unit_test\"",
          "    CODECOV_PR_REVIEW = \"codecov_pr_review\"",
          "    CODECOV_PR_CLOSED = \"codecov_pr_closed\"",
          "",
          "",
          "class CompleteGitTree:",
          "    \"\"\"",
          "    A custom class that mimics the interface of github.GitTree",
          "    but allows combining multiple trees into one complete representation.",
          "    \"\"\"",
          "",
          "    def __init__(self, github_tree: GitTree | None = None) -> None:",
          "        self.tree: List[GitTreeElement] = []",
          "        self.raw_data: Dict[str, Any] = {\"truncated\": False}",
          "",
          "        if github_tree:",
          "            self.add_items(github_tree.tree)",
          "            for key, value in github_tree.raw_data.items():",
          "                if key != \"truncated\":  # We always set truncated to False for our complete tree",
          "                    self.raw_data[key] = value",
          "",
          "    def add_item(self, item: GitTreeElement) -> None:",
          "        \"\"\"Add a tree item to this collection\"\"\"",
          "        self.tree.append(item)",
          "",
          "    def add_items(self, items: List[GitTreeElement]) -> None:",
          "        \"\"\"Add multiple tree items to this collection\"\"\"",
          "        self.tree.extend(items)",
          "",
          "",
          "class RepoClient:",
          "    # TODO: Support other git providers later",
          "    github_auth: Auth.Token | Auth.AppInstallationAuth",
          "    github: Github",
          "    repo: Repository",
          "",
          "    provider: str",
          "    repo_owner: str",
          "    repo_name: str",
          "    repo_external_id: str",
          "    base_commit_sha: str",
          "    base_branch: str",
          "",
          "    supported_providers = [\"github\"]",
          "",
          "    def __init__(",
          "        self, app_id: int | str | None, private_key: str | None, repo_definition: RepoDefinition",
          "    ):",
          "        if repo_definition.provider not in self.supported_providers:",
          "            # This should never get here, the repo provider should be checked on the Sentry side but this will make debugging",
          "            # easier if it does",
          "            raise InitializationError(",
          "                f\"Unsupported repo provider: {repo_definition.provider}, only {', '.join(self.supported_providers)} are supported.\"",
          "            )",
          "",
          "        if app_id and private_key:",
          "            self.github = Github(",
          "                auth=get_github_app_auth_and_installation(",
          "                    app_id, private_key, repo_definition.owner, repo_definition.name",
          "                )[0]",
          "            )",
          "        else:",
          "            self.github = Github(auth=get_github_token_auth())",
          "",
          "        self.repo = self.github.get_repo(",
          "            int(repo_definition.external_id)",
          "            if repo_definition.external_id.isdigit()",
          "            else repo_definition.full_name",
          "        )",
          "",
          "        self.provider = repo_definition.provider",
          "        self.repo_owner = repo_definition.owner",
          "        self.repo_name = repo_definition.name",
          "        self.repo_external_id = repo_definition.external_id",
          "        self.base_branch = repo_definition.branch_name or self.get_default_branch()",
          "        self.base_commit_sha = repo_definition.base_commit_sha or self.get_branch_head_sha(",
          "            self.base_branch",
          "        )",
          "",
          "    @staticmethod",
          "    def check_repo_write_access(repo: RepoDefinition):",
          "        app_id, pk = get_write_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if (",
          "            permissions",
          "            and permissions.get(\"contents\") == \"write\"",
          "            and permissions.get(\"pull_requests\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def check_repo_read_access(repo: RepoDefinition):",
          "        app_id, pk = get_read_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if permissions and (",
          "            permissions.get(\"contents\") == \"read\" or permissions.get(\"contents\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def _extract_id_from_pr_url(pr_url: str):",
          "        \"\"\"",
          "        Extracts the repository path and PR/issue ID from the provided URL.",
          "        \"\"\"",
          "        pr_id = int(pr_url.split(\"/\")[-1])",
          "        return pr_id",
          "",
          "    @classmethod",
          "    @functools.lru_cache(maxsize=8)",
          "    def from_repo_definition(cls, repo_def: RepoDefinition, type: RepoClientType):",
          "        if type == RepoClientType.WRITE:",
          "            return cls(*get_write_app_credentials(), repo_def)",
          "        elif type == RepoClientType.CODECOV_UNIT_TEST:",
          "            return cls(*get_codecov_unit_test_app_credentials(), repo_def)",
          "        elif type in (RepoClientType.CODECOV_PR_REVIEW, RepoClientType.CODECOV_PR_CLOSED):",
          "            return cls(*get_codecov_pr_review_app_credentials(), repo_def)",
          "",
          "        return cls(*get_read_app_credentials(), repo_def)",
          "",
          "    @property",
          "    def repo_full_name(self):",
          "        return self.repo.full_name",
          "",
          "    def get_default_branch(self) -> str:",
          "        return self.repo.default_branch",
          "",
          "    def get_branch_head_sha(self, branch: str):",
          "        return self.repo.get_branch(branch).commit.sha",
          "",
          "    def compare(self, base: str, head: str):",
          "        return self.repo.compare(base, head)",
          "",
          "    def load_repo_to_tmp_dir(self, sha: str | None = None) -> tuple[str, str]:",
          "        sha = sha or self.base_commit_sha",
          "",
          "        # Check if output directory exists, if not create it",
          "        tmp_dir = tempfile.mkdtemp(prefix=f\"{self.repo_owner}-{self.repo_name}_{sha}\")",
          "        tmp_repo_dir = os.path.join(tmp_dir, \"repo\")",
          "",
          "        logger.debug(f\"Loading repository to {tmp_repo_dir}\")",
          "",
          "        # Create a temporary directory to store the repository",
          "        os.makedirs(tmp_repo_dir, exist_ok=True)",
          "",
          "        # Clean the directory",
          "        for root, dirs, files in os.walk(tmp_repo_dir, topdown=False):",
          "            for name in files:",
          "                os.remove(os.path.join(root, name))",
          "            for name in dirs:",
          "                os.rmdir(os.path.join(root, name))",
          "",
          "        tarball_url = self.repo.get_archive_link(\"tarball\", ref=sha)",
          "        tarfile_path = os.path.join(tmp_dir, f\"{sha}.tar.gz\")",
          "",
          "        response = requests.get(tarball_url, stream=True)",
          "        if response.status_code == 200:",
          "            with open(tarfile_path, \"wb\") as f:",
          "                f.write(response.content)",
          "        else:",
          "            logger.error(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "            logger.error(",
          "                f\"Response status code: {response.status_code}, response text: {response.text}\"",
          "            )",
          "            raise Exception(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "",
          "        # Extract tarball into the output directory",
          "        with tarfile.open(tarfile_path, \"r:gz\") as tar:",
          "            tar.extractall(path=tmp_repo_dir)  # extract all members normally",
          "            extracted_folders = [",
          "                name",
          "                for name in os.listdir(tmp_repo_dir)",
          "                if os.path.isdir(os.path.join(tmp_repo_dir, name))",
          "            ]",
          "            if extracted_folders:",
          "                root_folder = extracted_folders[0]  # assuming the first folder is the root folder",
          "                root_folder_path = os.path.join(tmp_repo_dir, root_folder)",
          "                for item in os.listdir(root_folder_path):",
          "                    s = os.path.join(root_folder_path, item)",
          "                    d = os.path.join(tmp_repo_dir, item)",
          "                    # TODO: Consider a strategy for handling symlinks more appropriately in the future, possibly by resolving them or copying as symlinks to maintain the original structure.",
          "                    if os.path.isdir(s):",
          "                        shutil.move(",
          "                            s, d",
          "                        )  # move all directories from the root folder to the output directory",
          "                    else:",
          "                        # Skipping symlinks to prevent FileNotFoundError.",
          "                        if not os.path.islink(s):",
          "                            shutil.copy2(",
          "                                s, d",
          "                            )  # copy all files from the root folder to the output directory",
          "",
          "                shutil.rmtree(root_folder_path)  # remove the root folder",
          "",
          "        return tmp_dir, tmp_repo_dir",
          "",
          "    def _autocorrect_path(self, path: str, sha: str | None = None) -> tuple[str, bool]:",
          "        \"\"\"",
          "        Attempts to autocorrect a file path by finding the closest match in the repository.",
          "",
          "        Args:",
          "            path: The path to autocorrect",
          "            sha: The commit SHA to use for finding valid paths",
          "",
          "        Returns:",
          "            A tuple of (corrected_path, was_autocorrected)",
          "        \"\"\"",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        path = path.lstrip(\"/\")",
          "        valid_paths = self.get_valid_file_paths(sha)",
          "",
          "        # If path is valid, return it unchanged",
          "        if path in valid_paths:",
          "            return path, False",
          "",
          "        # Check for partial matches if no exact match and path is long enough",
          "        if len(path) > 3:",
          "            path_lower = path.lower()",
          "            partial_matches = [",
          "                valid_path for valid_path in valid_paths if path_lower in valid_path.lower()",
          "            ]",
          "            if partial_matches:",
          "                # Sort by length to get closest match (shortest containing path)",
          "                closest_match = sorted(partial_matches, key=len)[0]",
          "                logger.warning(",
          "                    f\"Path '{path}' not found exactly, using closest match: '{closest_match}'\"",
          "                )",
          "                return closest_match, True",
          "",
          "        # No match found",
          "        logger.warning(\"No matching file found for provided file path\", extra={\"path\": path})",
          "        return path, False",
          "",
          "    def get_file_content(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False",
          "    ) -> tuple[str | None, str]:",
          "        logger.debug(f\"Getting file contents for {path} in {self.repo.full_name} on sha {sha}\")",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        autocorrected_path = False",
          "        if autocorrect:",
          "            path, autocorrected_path = self._autocorrect_path(path, sha)",
          "            if not autocorrected_path and path not in self.get_valid_file_paths(sha):",
          "                return None, \"utf-8\"",
          "",
          "        try:",
          "            contents = self.repo.get_contents(path, ref=sha)",
          "",
          "            if isinstance(contents, list):",
          "                raise Exception(f\"Expected a single ContentFile but got a list for path {path}\")",
          "",
          "            detected_encoding = detect_encoding(contents.decoded_content) if contents else \"utf-8\"",
          "            content = contents.decoded_content.decode(detected_encoding)",
          "            if autocorrected_path:",
          "                content = f\"Showing results instead for {path}\\n=====\\n{content}\"",
          "            return content, detected_encoding",
          "        except Exception as e:",
          "            logger.exception(f\"Error getting file contents: {e}\")",
          "            return None, \"utf-8\"",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_valid_file_paths(self, sha: str | None = None) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        valid_file_paths: set[str] = set()",
          "        valid_file_extensions = get_all_supported_extensions()",
          "",
          "        for file in tree.tree:",
          "            if file.type == \"blob\" and any(",
          "                file.path.endswith(ext) for ext in valid_file_extensions",
          "            ):",
          "                valid_file_paths.add(file.path)",
          "",
          "        return valid_file_paths",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_history(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False, max_commits: int = 10",
          "    ) -> list[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(sha):",
          "                return []",
          "",
          "        commits = self.repo.get_commits(sha=sha, path=path)",
          "        commit_list = list(commits[:max_commits])",
          "        commit_strs = []",
          "",
          "        def process_commit(commit):",
          "            short_sha = commit.sha[:7]",
          "            message = commit.commit.message",
          "            files_touched = [",
          "                {\"path\": file.filename, \"status\": file.status} for file in commit.files[:20]",
          "            ]",
          "",
          "            # Build a file tree representation instead of a flat list",
          "            file_tree_str = self._build_file_tree_string(files_touched)",
          "",
          "            # Add a note about additional files if needed",
          "            additional_files_note = \"\"",
          "            if len(files_touched) < len(commit.files):",
          "                additional_files_note = (",
          "                    f\"\\n[and {len(commit.files) - len(files_touched)} more files were changed...]\"",
          "                )",
          "",
          "            string = textwrap.dedent(",
          "                \"\"\"\\",
          "                ----------------",
          "                {short_sha} - {message}",
          "                Files touched:",
          "                {file_tree}{additional_files}",
          "                \"\"\"",
          "            ).format(",
          "                short_sha=short_sha,",
          "                message=message,",
          "                file_tree=file_tree_str,",
          "                additional_files=additional_files_note,",
          "            )",
          "            return string",
          "",
          "        with ThreadPoolExecutor() as executor:",
          "            results = list(executor.map(process_commit, commit_list))",
          "",
          "        for result in results:",
          "            commit_strs.append(result)",
          "",
          "        return commit_strs",
          "",
          "    def _build_file_tree_string(",
          "        self, files: list[dict], only_immediate_children_of_path: str | None = None",
          "    ) -> str:",
          "        \"\"\"",
          "        Builds a tree representation of files to save tokens when many files share the same directories.",
          "        The output is similar to the 'tree' command in terminal.",
          "",
          "        Args:",
          "            files: List of dictionaries with 'path' and 'status' keys",
          "            only_immediate_children_of_path: If provided, only include files and directories that are immediate children of this path",
          "",
          "        Returns:",
          "            A string representation of the file tree",
          "        \"\"\"",
          "        if not files:",
          "            return \"No files changed\"",
          "",
          "        if only_immediate_children_of_path is not None:",
          "            only_immediate_children_of_path = only_immediate_children_of_path.rstrip(\"/\")",
          "",
          "        # First, build a nested dictionary structure representing the file tree",
          "        tree: dict = {}",
          "        for file in files:",
          "            path = file[\"path\"]",
          "            status = file[\"status\"]",
          "",
          "            # Split the path into components",
          "            parts = path.split(\"/\")",
          "",
          "            # Navigate through the tree, creating nodes as needed",
          "            current = tree",
          "            for i, part in enumerate(parts):",
          "                # If this is the last part (the filename)",
          "                if i == len(parts) - 1:",
          "                    current[part] = {\"__status__\": status}",
          "                else:",
          "                    if part not in current:",
          "                        current[part] = {}",
          "                    current = current[part]",
          "",
          "        # Now build the tree string recursively",
          "        lines = []",
          "",
          "        def _build_tree(node, previous_parts=[], prefix=\"\", is_last=True, is_root=True):",
          "            items = list(node.items())",
          "",
          "            # Process each item",
          "            for i, (key, value) in enumerate(sorted(items)):",
          "                if key == \"__status__\":",
          "                    continue",
          "",
          "                # Determine if this is the last item at this level",
          "                is_last_item = i == len(items) - 1 or (i == len(items) - 2 and \"__status__\" in node)",
          "",
          "                # Create the appropriate prefix for this line",
          "                if is_root:",
          "                    current_prefix = \"\"",
          "                    next_prefix = \"\"",
          "                else:",
          "                    current_prefix = prefix + (\"└── \" if is_last else \"├── \")",
          "                    next_prefix = prefix + (\"    \" if is_last else \"│   \")",
          "",
          "                # If this is a file (has a status)",
          "                if \"__status__\" in value:",
          "                    if (",
          "                        only_immediate_children_of_path is not None",
          "                        and not only_immediate_children_of_path == (\"/\".join(previous_parts))",
          "                    ):",
          "                        continue",
          "",
          "                    status = value[\"__status__\"]",
          "                    status_str = f\" ({status})\" if status else \"\"",
          "                    lines.append(f\"{current_prefix}{key}{status_str}\")",
          "                # If this is a directory",
          "                else:",
          "                    # If this is within the specified path",
          "                    if (",
          "                        only_immediate_children_of_path is None",
          "                        or only_immediate_children_of_path.startswith(",
          "                            \"/\".join(previous_parts + [key])",
          "                        )",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "                        _build_tree(value, previous_parts + [key], next_prefix, is_last_item, False)",
          "                    elif (",
          "                        only_immediate_children_of_path is not None",
          "                        and only_immediate_children_of_path == \"/\".join(previous_parts)",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "",
          "        # Start building the tree from the root",
          "        _build_tree(tree)",
          "",
          "        return \"\\n\".join(lines)",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_patch_for_file(",
          "        self, path: str, commit_sha: str, autocorrect: bool = False",
          "    ) -> str | None:",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, commit_sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(commit_sha):",
          "                return None",
          "",
          "        commit = self.repo.get_commit(commit_sha)",
          "        matching_file = next((file for file in commit.files if file.filename == path), None)",
          "        if not matching_file:",
          "            return None",
          "",
          "        return matching_file.patch",
          "",
          "    def _create_branch(self, branch_name, from_base_sha=False):",
          "        ref = self.repo.create_git_ref(",
          "            ref=f\"refs/heads/{branch_name}\",",
          "            sha=(",
          "                self.base_commit_sha",
          "                if from_base_sha",
          "                else self.get_branch_head_sha(self.base_branch)",
          "            ),",
          "        )",
          "        return ref",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_git_tree(self, sha: str) -> CompleteGitTree:",
          "        \"\"\"",
          "        Get the git tree for a specific sha, handling truncation with divide and conquer.",
          "        Always returns a CompleteGitTree instance for consistent interface.",
          "",
          "        First tries to get the complete tree recursively. If truncated, it uses a",
          "        divide and conquer approach to fetch all subtrees individually and combine them.",
          "",
          "        Args:",
          "            sha: The commit SHA to get the tree for",
          "",
          "        Returns:",
          "            A CompleteGitTree with all items from all subtrees",
          "        \"\"\"",
          "        tree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not tree.raw_data.get(\"truncated\", False):",
          "            return CompleteGitTree(tree)",
          "",
          "        complete_tree = CompleteGitTree()",
          "        root_tree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        for key, value in root_tree.raw_data.items():",
          "            if key != \"tree\" and key != \"truncated\":",
          "                complete_tree.raw_data[key] = value",
          "",
          "        for item in root_tree.tree:",
          "            complete_tree.add_item(item)",
          "",
          "        tree_items = [item for item in root_tree.tree if item.type == \"tree\"]",
          "        with ThreadPoolExecutor() as executor:",
          "            subtree_results = []",
          "            for item in tree_items:",
          "                subtree_results.append(executor.submit(self._get_git_subtree, item.sha))",
          "",
          "            for future in subtree_results:",
          "                subtree_items = future.result()",
          "                complete_tree.add_items(subtree_items)",
          "",
          "        return complete_tree",
          "",
          "    def _get_git_subtree(self, sha: str) -> list:",
          "        \"\"\"",
          "        Process a subtree and return all its items for parallel execution.",
          "",
          "        Args:",
          "            sha: The SHA of the subtree",
          "",
          "        Returns:",
          "            A list of all tree items from this subtree and its nested subtrees",
          "        \"\"\"",
          "        items = []",
          "        subtree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not subtree.raw_data.get(\"truncated\", False):",
          "            return subtree.tree",
          "",
          "        non_recursive_subtree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        nested_tree_items = [item for item in non_recursive_subtree.tree if item.type == \"tree\"]",
          "        non_tree_items = [item for item in non_recursive_subtree.tree if item.type != \"tree\"]",
          "",
          "        items.extend(non_tree_items)",
          "",
          "        if nested_tree_items:",
          "            with ThreadPoolExecutor() as executor:",
          "                subtree_futures = [",
          "                    executor.submit(self._get_git_subtree, item.sha) for item in nested_tree_items",
          "                ]",
          "",
          "                for future in subtree_futures:",
          "                    items.extend(future.result())",
          "",
          "        return items",
          "",
          "    def process_one_file_for_git_commit(",
          "        self, *, branch_ref: str, patch: FilePatch | None = None, change: FileChange | None = None",
          "    ) -> InputGitTreeElement | None:",
          "        \"\"\"",
          "        This method is used to get a single change to be committed by to github.",
          "        It processes a FilePatch/FileChange object and converts it into an InputGitTreeElement which can be commited",
          "        It supports both FilePatch and FileChange objects.",
          "        \"\"\"",
          "        path = patch.path if patch else (change.path if change else None)",
          "        patch_type = patch.type if patch else (change.change_type if change else None)",
          "        if not path:",
          "            raise ValueError(\"Path must be provided\")",
          "",
          "        if not patch_type:",
          "            raise ValueError(\"Patch type must be provided\")",
          "        if patch_type == \"create\":",
          "            patch_type = \"A\"",
          "        elif patch_type == \"delete\":",
          "            patch_type = \"D\"",
          "        elif patch_type == \"edit\":",
          "            patch_type = \"M\"",
          "",
          "        to_apply = None",
          "        detected_encoding = \"utf-8\"",
          "        if patch_type != \"A\":",
          "            to_apply, detected_encoding = self.get_file_content(path, sha=branch_ref)",
          "",
          "        new_contents = (",
          "            patch.apply(to_apply) if patch else (change.apply(to_apply) if change else None)",
          "        )",
          "",
          "        # Remove leading slash if it exists, the github api will reject paths with leading slashes.",
          "        if path.startswith(\"/\"):",
          "            path = path[1:]",
          "",
          "        # don't create a blob if the file is being deleted",
          "        blob = self.repo.create_git_blob(new_contents, detected_encoding) if new_contents else None",
          "",
          "        # Prevent creating tree elements with None SHA for file additions",
          "        if patch_type == \"A\" and blob is None:",
          "            return None",
          "",
          "        # 100644 is the git code for creating a Regular non-executable file",
          "        # https://stackoverflow.com/questions/737673/how-to-read-the-mode-field-of-git-ls-trees-output",
          "        return InputGitTreeElement(",
          "            path=path, mode=\"100644\", type=\"blob\", sha=blob.sha if blob else None",
          "        )",
          "",
          "    def get_branch_ref(self, branch_name: str) -> GitRef | None:",
          "        try:",
          "            return self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        except GithubException as e:",
          "            if e.status == 404:",
          "                return None",
          "            raise e",
          "",
          "    def create_branch_from_changes(",
          "        self,",
          "        *,",
          "        pr_title: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "        branch_name: str | None = None,",
          "        from_base_sha: bool = False,",
          "    ) -> GitRef | None:",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Either file_patches or file_changes must be provided\")",
          "",
          "        new_branch_name = sanitize_branch_name(branch_name or pr_title)",
          "",
          "        try:",
          "            branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "        except GithubException as e:",
          "            # only use the random suffix if the branch already exists",
          "            if e.status == 409 or e.status == 422:",
          "                new_branch_name = f\"{new_branch_name}-{generate_random_string(n=6)}\"",
          "                branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "            else:",
          "                raise e",
          "",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, patch=patch",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file patch: {e}\")",
          "",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, change=change",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file change: {e}\")",
          "        # latest commit is the head of new branch",
          "        latest_commit = self.repo.get_git_commit(self.get_branch_head_sha(new_branch_name))",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "",
          "        new_commit = self.repo.create_git_commit(",
          "            message=pr_title, tree=new_tree, parents=[latest_commit]",
          "        )",
          "",
          "        branch_ref.edit(sha=new_commit.sha)",
          "",
          "        # Check that the changes were made",
          "        comparison = self.repo.compare(",
          "            self.get_branch_head_sha(self.base_branch), branch_ref.object.sha",
          "        )",
          "",
          "        if comparison.ahead_by < 1:",
          "            # Remove the branch if there are no changes",
          "            try:",
          "                branch_ref.delete()",
          "            except UnknownObjectException:",
          "                logger.warning(\"Attempted to delete a branch or reference that does not exist.\")",
          "            sentry_sdk.capture_message(",
          "                f\"Failed to create branch from changes. Comparison is ahead by {comparison.ahead_by}\"",
          "            )",
          "            return None",
          "",
          "        return branch_ref",
          "",
          "    def create_pr_from_branch(",
          "        self,",
          "        branch: GitRef,",
          "        title: str,",
          "        description: str,",
          "        provided_base: str | None = None,",
          "    ) -> PullRequest:",
          "        pulls = self.repo.get_pulls(state=\"open\", head=f\"{self.repo_owner}:{branch.ref}\")",
          "",
          "        if pulls.totalCount > 0:",
          "            logger.error(",
          "                f\"Branch {branch.ref} already has an open PR.\",",
          "                extra={",
          "                    \"branch_ref\": branch.ref,",
          "                    \"title\": title,",
          "                    \"description\": description,",
          "                    \"provided_base\": provided_base,",
          "                },",
          "            )",
          "",
          "            return pulls[0]",
          "",
          "        try:",
          "            return self.repo.create_pull(",
          "                title=title,",
          "                body=description,",
          "                base=provided_base or self.base_branch or self.get_default_branch(),",
          "                head=branch.ref,",
          "                draft=True,",
          "            )",
          "        except GithubException as e:",
          "            if e.status == 422 and \"Draft pull requests are not supported\" in str(e):",
          "                # fallback to creating a regular PR if draft PR is not supported",
          "                return self.repo.create_pull(",
          "                    title=title,",
          "                    body=description,",
          "                    base=provided_base or self.base_branch or self.get_default_branch(),",
          "                    head=branch.ref,",
          "                    draft=False,",
          "                )",
          "            else:",
          "                logger.exception(\"Error creating PR\")",
          "                raise e",
          "",
          "    def get_index_file_set(",
          "        self, sha: str | None = None, max_file_size_bytes=2 * 1024 * 1024, skip_empty_files=False",
          "    ) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        file_set = set()",
          "        for file in tree.tree:",
          "            if (",
          "                file.type == \"blob\"",
          "                and file.size < max_file_size_bytes",
          "                and file.mode",
          "                in [\"100644\", \"100755\"]  # 100644 is a regular file, 100755 is an executable file",
          "                and get_language_from_path(file.path) is not None",
          "                and (not skip_empty_files or file.size > 0)",
          "            ):",
          "                file_set.add(file.path)",
          "",
          "        return file_set",
          "",
          "    def get_pr_diff_content(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"diff\"))",
          "",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.text",
          "",
          "    def _get_auth_headers(self, accept_type: Literal[\"json\", \"diff\"] = \"json\"):",
          "        requester = self.repo._requester",
          "        if requester.auth is None:",
          "            raise Exception(\"No auth token found for GitHub API\")",
          "        headers = {",
          "            \"Accept\": (",
          "                \"application/vnd.github.diff\"",
          "                if accept_type == \"diff\"",
          "                else \"application/vnd.github+json\"",
          "            ),",
          "            \"Authorization\": f\"Bearer {requester.auth.token}\",",
          "            \"X-GitHub-Api-Version\": \"2022-11-28\",",
          "        }",
          "        return headers",
          "",
          "    def comment_root_cause_on_pr_for_copilot(",
          "        self, pr_url: str, run_id: int, issue_id: int, comment: str",
          "    ):",
          "        pull_id = int(pr_url.split(\"/\")[-1])",
          "        repo_name = pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "        params = {",
          "            \"body\": comment,",
          "            \"actions\": [",
          "                {",
          "                    \"name\": \"Fix with Sentry\",",
          "                    \"type\": \"copilot-chat\",",
          "                    \"prompt\": f\"@sentry find a fix for issue {issue_id} with run ID {run_id}\",",
          "                }",
          "            ],",
          "        }",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def comment_pr_generated_for_copilot(",
          "        self, pr_to_comment_on_url: str, new_pr_url: str, run_id: int",
          "    ):",
          "        pull_id = int(pr_to_comment_on_url.split(\"/\")[-1])",
          "        repo_name = pr_to_comment_on_url.split(\"github.com/\")[1].split(\"/pull\")[",
          "            0",
          "        ]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "",
          "        comment = f\"A fix has been generated and is available [here]({new_pr_url}) for your review. Autofix Run ID: {run_id}\"",
          "",
          "        params = {\"body\": comment}",
          "",
          "        headers = self._get_auth_headers()",
          "",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def get_pr_head_sha(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"json\"))",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.json()[\"head\"][\"sha\"]",
          "",
          "    def post_unit_test_reference_to_original_pr(self, original_pr_url: str, unit_test_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Sentry has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_reference_to_original_pr_codecov_app(",
          "        self, original_pr_url: str, unit_test_pr_url: str",
          "    ):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Codecov has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_not_generated_message_to_original_pr(self, original_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = \"Sentry has determined that unit tests are not necessary for this PR.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_issue_comment(self, pr_url: str, comment: str):",
          "        \"\"\"",
          "        Create an issue comment on a GitHub issue (all pull requests are issues).",
          "        This can be used to create an overall PR comment instead of associated with a specific line.",
          "        See https://docs.github.com/en/rest/issues/comments?apiVersion=2022-11-28#create-an-issue-comment",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        issue = self.repo.get_issue(number=pr_id)",
          "        comment_obj = issue.create_comment(body=comment)",
          "        return comment_obj.html_url",
          "",
          "    def post_pr_review_comment(self, pr_url: str, comment: GithubPrReviewComment):",
          "        \"\"\"",
          "        Create a review comment on a GitHub pull request.",
          "        See https://docs.github.com/en/rest/pulls/comments?apiVersion=2022-11-28#create-a-review-comment-for-a-pull-request",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        pr = self.repo.get_pull(number=pr_id)",
          "        commit = self.repo.get_commit(comment[\"commit_id\"])",
          "",
          "        review_comment = pr.create_review_comment(",
          "            body=comment[\"body\"],",
          "            commit=commit,",
          "            path=comment[\"path\"],",
          "            line=comment.get(\"line\", GithubObject.NotSet),",
          "            side=comment.get(\"side\", GithubObject.NotSet),",
          "            start_line=comment.get(\"start_line\", GithubObject.NotSet),",
          "        )",
          "        return review_comment.html_url",
          "",
          "    def push_new_commit_to_pr(",
          "        self,",
          "        pr,",
          "        commit_message: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "    ):",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Must provide file_patches or file_changes\")",
          "        branch_name = pr.head.ref",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                element = self.process_one_file_for_git_commit(branch_ref=branch_name, patch=patch)",
          "                if element:",
          "                    tree_elements.append(element)",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                element = self.process_one_file_for_git_commit(",
          "                    branch_ref=branch_name, change=change",
          "                )",
          "                if element:",
          "                    tree_elements.append(element)",
          "        if not tree_elements:",
          "            logger.warning(\"No valid changes to commit\")",
          "            return None",
          "        latest_sha = self.get_branch_head_sha(branch_name)",
          "        latest_commit = self.repo.get_git_commit(latest_sha)",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "        new_commit = self.repo.create_git_commit(",
          "            message=commit_message, tree=new_tree, parents=[latest_commit]",
          "        )",
          "        branch_ref = self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        branch_ref.edit(sha=new_commit.sha)",
          "        return new_commit",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codebase/repo_client.py",
      "image": "seer.automation.codebase.repo_client",
      "is_application": true,
      "line": 45,
      "name": "get_github_app_auth_and_installation",
      "path": "/app/src/seer/automation/codebase/repo_client.py",
      "codeContext": {
        "file": "seer/automation/codebase/repo_client.py",
        "line": 45,
        "name": "get_github_app_auth_and_installation",
        "code": "\nlogger = logging.getLogger(__name__)\n\n\ndef get_github_app_auth_and_installation(\n    app_id: int | str, private_key: str, repo_owner: str, repo_name: str\n):\n    app_auth = Auth.AppAuth(app_id, private_key=private_key)\n    gi = GithubIntegration(auth=app_auth)\n    installation = gi.get_repo_installation(repo_owner, repo_name)\n    github_auth = app_auth.get_installation_auth(installation.id)\n\n    return github_auth, installation\n\n\ndef get_repo_app_permissions(\n    app_id: int | str, private_key: str, repo_owner: str, repo_name: str\n) -> dict[str, str] | None:\n    try:\n        _, installation = get_github_app_auth_and_installation(",
        "lineRange": {
          "start": 36,
          "end": 55
        },
        "lines": [
          "import functools",
          "import logging",
          "import os",
          "import shutil",
          "import tarfile",
          "import tempfile",
          "import textwrap",
          "from concurrent.futures import ThreadPoolExecutor",
          "from enum import Enum",
          "from typing import Any, Dict, List, Literal",
          "",
          "import requests",
          "import sentry_sdk",
          "from github import (",
          "    Auth,",
          "    Github,",
          "    GithubException,",
          "    GithubIntegration,",
          "    GithubObject,",
          "    InputGitTreeElement,",
          "    UnknownObjectException,",
          ")",
          "from github.GitRef import GitRef",
          "from github.GitTree import GitTree",
          "from github.GitTreeElement import GitTreeElement",
          "from github.PullRequest import PullRequest",
          "from github.Repository import Repository",
          "",
          "from seer.automation.autofix.utils import generate_random_string, sanitize_branch_name",
          "from seer.automation.codebase.models import GithubPrReviewComment",
          "from seer.automation.codebase.utils import get_all_supported_extensions, get_language_from_path",
          "from seer.automation.models import FileChange, FilePatch, InitializationError, RepoDefinition",
          "from seer.automation.utils import detect_encoding",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "def get_github_app_auth_and_installation(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          "):",
          "    app_auth = Auth.AppAuth(app_id, private_key=private_key)",
          "    gi = GithubIntegration(auth=app_auth)",
          "    installation = gi.get_repo_installation(repo_owner, repo_name)",
          "    github_auth = app_auth.get_installation_auth(installation.id)",
          "",
          "    return github_auth, installation",
          "",
          "",
          "def get_repo_app_permissions(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          ") -> dict[str, str] | None:",
          "    try:",
          "        _, installation = get_github_app_auth_and_installation(",
          "            app_id, private_key, repo_owner, repo_name",
          "        )",
          "",
          "        return installation.raw_data.get(\"permissions\", {})",
          "    except UnknownObjectException:",
          "        return None",
          "",
          "",
          "@inject",
          "def get_github_token_auth(config: AppConfig = injected) -> Auth.Token | None:",
          "    github_token = config.GITHUB_TOKEN",
          "    if github_token is None:",
          "        return None",
          "",
          "    return Auth.Token(github_token)",
          "",
          "",
          "@inject",
          "def get_write_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_APP_ID",
          "    private_key = config.GITHUB_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return None, None",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_read_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_SENTRY_APP_ID",
          "    private_key = config.GITHUB_SENTRY_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_unit_test_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_UNIT_TEST_APP_ID",
          "    private_key = config.GITHUB_CODECOV_UNIT_TEST_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov unit test app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_pr_review_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_PR_REVIEW_APP_ID",
          "    private_key = config.GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY",
          "",
          "    if not app_id:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_APP_ID\")",
          "    if not private_key:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY\")",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov pr review app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "class RepoClientType(str, Enum):",
          "    READ = \"read\"",
          "    WRITE = \"write\"",
          "    CODECOV_UNIT_TEST = \"codecov_unit_test\"",
          "    CODECOV_PR_REVIEW = \"codecov_pr_review\"",
          "    CODECOV_PR_CLOSED = \"codecov_pr_closed\"",
          "",
          "",
          "class CompleteGitTree:",
          "    \"\"\"",
          "    A custom class that mimics the interface of github.GitTree",
          "    but allows combining multiple trees into one complete representation.",
          "    \"\"\"",
          "",
          "    def __init__(self, github_tree: GitTree | None = None) -> None:",
          "        self.tree: List[GitTreeElement] = []",
          "        self.raw_data: Dict[str, Any] = {\"truncated\": False}",
          "",
          "        if github_tree:",
          "            self.add_items(github_tree.tree)",
          "            for key, value in github_tree.raw_data.items():",
          "                if key != \"truncated\":  # We always set truncated to False for our complete tree",
          "                    self.raw_data[key] = value",
          "",
          "    def add_item(self, item: GitTreeElement) -> None:",
          "        \"\"\"Add a tree item to this collection\"\"\"",
          "        self.tree.append(item)",
          "",
          "    def add_items(self, items: List[GitTreeElement]) -> None:",
          "        \"\"\"Add multiple tree items to this collection\"\"\"",
          "        self.tree.extend(items)",
          "",
          "",
          "class RepoClient:",
          "    # TODO: Support other git providers later",
          "    github_auth: Auth.Token | Auth.AppInstallationAuth",
          "    github: Github",
          "    repo: Repository",
          "",
          "    provider: str",
          "    repo_owner: str",
          "    repo_name: str",
          "    repo_external_id: str",
          "    base_commit_sha: str",
          "    base_branch: str",
          "",
          "    supported_providers = [\"github\"]",
          "",
          "    def __init__(",
          "        self, app_id: int | str | None, private_key: str | None, repo_definition: RepoDefinition",
          "    ):",
          "        if repo_definition.provider not in self.supported_providers:",
          "            # This should never get here, the repo provider should be checked on the Sentry side but this will make debugging",
          "            # easier if it does",
          "            raise InitializationError(",
          "                f\"Unsupported repo provider: {repo_definition.provider}, only {', '.join(self.supported_providers)} are supported.\"",
          "            )",
          "",
          "        if app_id and private_key:",
          "            self.github = Github(",
          "                auth=get_github_app_auth_and_installation(",
          "                    app_id, private_key, repo_definition.owner, repo_definition.name",
          "                )[0]",
          "            )",
          "        else:",
          "            self.github = Github(auth=get_github_token_auth())",
          "",
          "        self.repo = self.github.get_repo(",
          "            int(repo_definition.external_id)",
          "            if repo_definition.external_id.isdigit()",
          "            else repo_definition.full_name",
          "        )",
          "",
          "        self.provider = repo_definition.provider",
          "        self.repo_owner = repo_definition.owner",
          "        self.repo_name = repo_definition.name",
          "        self.repo_external_id = repo_definition.external_id",
          "        self.base_branch = repo_definition.branch_name or self.get_default_branch()",
          "        self.base_commit_sha = repo_definition.base_commit_sha or self.get_branch_head_sha(",
          "            self.base_branch",
          "        )",
          "",
          "    @staticmethod",
          "    def check_repo_write_access(repo: RepoDefinition):",
          "        app_id, pk = get_write_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if (",
          "            permissions",
          "            and permissions.get(\"contents\") == \"write\"",
          "            and permissions.get(\"pull_requests\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def check_repo_read_access(repo: RepoDefinition):",
          "        app_id, pk = get_read_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if permissions and (",
          "            permissions.get(\"contents\") == \"read\" or permissions.get(\"contents\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def _extract_id_from_pr_url(pr_url: str):",
          "        \"\"\"",
          "        Extracts the repository path and PR/issue ID from the provided URL.",
          "        \"\"\"",
          "        pr_id = int(pr_url.split(\"/\")[-1])",
          "        return pr_id",
          "",
          "    @classmethod",
          "    @functools.lru_cache(maxsize=8)",
          "    def from_repo_definition(cls, repo_def: RepoDefinition, type: RepoClientType):",
          "        if type == RepoClientType.WRITE:",
          "            return cls(*get_write_app_credentials(), repo_def)",
          "        elif type == RepoClientType.CODECOV_UNIT_TEST:",
          "            return cls(*get_codecov_unit_test_app_credentials(), repo_def)",
          "        elif type in (RepoClientType.CODECOV_PR_REVIEW, RepoClientType.CODECOV_PR_CLOSED):",
          "            return cls(*get_codecov_pr_review_app_credentials(), repo_def)",
          "",
          "        return cls(*get_read_app_credentials(), repo_def)",
          "",
          "    @property",
          "    def repo_full_name(self):",
          "        return self.repo.full_name",
          "",
          "    def get_default_branch(self) -> str:",
          "        return self.repo.default_branch",
          "",
          "    def get_branch_head_sha(self, branch: str):",
          "        return self.repo.get_branch(branch).commit.sha",
          "",
          "    def compare(self, base: str, head: str):",
          "        return self.repo.compare(base, head)",
          "",
          "    def load_repo_to_tmp_dir(self, sha: str | None = None) -> tuple[str, str]:",
          "        sha = sha or self.base_commit_sha",
          "",
          "        # Check if output directory exists, if not create it",
          "        tmp_dir = tempfile.mkdtemp(prefix=f\"{self.repo_owner}-{self.repo_name}_{sha}\")",
          "        tmp_repo_dir = os.path.join(tmp_dir, \"repo\")",
          "",
          "        logger.debug(f\"Loading repository to {tmp_repo_dir}\")",
          "",
          "        # Create a temporary directory to store the repository",
          "        os.makedirs(tmp_repo_dir, exist_ok=True)",
          "",
          "        # Clean the directory",
          "        for root, dirs, files in os.walk(tmp_repo_dir, topdown=False):",
          "            for name in files:",
          "                os.remove(os.path.join(root, name))",
          "            for name in dirs:",
          "                os.rmdir(os.path.join(root, name))",
          "",
          "        tarball_url = self.repo.get_archive_link(\"tarball\", ref=sha)",
          "        tarfile_path = os.path.join(tmp_dir, f\"{sha}.tar.gz\")",
          "",
          "        response = requests.get(tarball_url, stream=True)",
          "        if response.status_code == 200:",
          "            with open(tarfile_path, \"wb\") as f:",
          "                f.write(response.content)",
          "        else:",
          "            logger.error(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "            logger.error(",
          "                f\"Response status code: {response.status_code}, response text: {response.text}\"",
          "            )",
          "            raise Exception(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "",
          "        # Extract tarball into the output directory",
          "        with tarfile.open(tarfile_path, \"r:gz\") as tar:",
          "            tar.extractall(path=tmp_repo_dir)  # extract all members normally",
          "            extracted_folders = [",
          "                name",
          "                for name in os.listdir(tmp_repo_dir)",
          "                if os.path.isdir(os.path.join(tmp_repo_dir, name))",
          "            ]",
          "            if extracted_folders:",
          "                root_folder = extracted_folders[0]  # assuming the first folder is the root folder",
          "                root_folder_path = os.path.join(tmp_repo_dir, root_folder)",
          "                for item in os.listdir(root_folder_path):",
          "                    s = os.path.join(root_folder_path, item)",
          "                    d = os.path.join(tmp_repo_dir, item)",
          "                    # TODO: Consider a strategy for handling symlinks more appropriately in the future, possibly by resolving them or copying as symlinks to maintain the original structure.",
          "                    if os.path.isdir(s):",
          "                        shutil.move(",
          "                            s, d",
          "                        )  # move all directories from the root folder to the output directory",
          "                    else:",
          "                        # Skipping symlinks to prevent FileNotFoundError.",
          "                        if not os.path.islink(s):",
          "                            shutil.copy2(",
          "                                s, d",
          "                            )  # copy all files from the root folder to the output directory",
          "",
          "                shutil.rmtree(root_folder_path)  # remove the root folder",
          "",
          "        return tmp_dir, tmp_repo_dir",
          "",
          "    def _autocorrect_path(self, path: str, sha: str | None = None) -> tuple[str, bool]:",
          "        \"\"\"",
          "        Attempts to autocorrect a file path by finding the closest match in the repository.",
          "",
          "        Args:",
          "            path: The path to autocorrect",
          "            sha: The commit SHA to use for finding valid paths",
          "",
          "        Returns:",
          "            A tuple of (corrected_path, was_autocorrected)",
          "        \"\"\"",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        path = path.lstrip(\"/\")",
          "        valid_paths = self.get_valid_file_paths(sha)",
          "",
          "        # If path is valid, return it unchanged",
          "        if path in valid_paths:",
          "            return path, False",
          "",
          "        # Check for partial matches if no exact match and path is long enough",
          "        if len(path) > 3:",
          "            path_lower = path.lower()",
          "            partial_matches = [",
          "                valid_path for valid_path in valid_paths if path_lower in valid_path.lower()",
          "            ]",
          "            if partial_matches:",
          "                # Sort by length to get closest match (shortest containing path)",
          "                closest_match = sorted(partial_matches, key=len)[0]",
          "                logger.warning(",
          "                    f\"Path '{path}' not found exactly, using closest match: '{closest_match}'\"",
          "                )",
          "                return closest_match, True",
          "",
          "        # No match found",
          "        logger.warning(\"No matching file found for provided file path\", extra={\"path\": path})",
          "        return path, False",
          "",
          "    def get_file_content(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False",
          "    ) -> tuple[str | None, str]:",
          "        logger.debug(f\"Getting file contents for {path} in {self.repo.full_name} on sha {sha}\")",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        autocorrected_path = False",
          "        if autocorrect:",
          "            path, autocorrected_path = self._autocorrect_path(path, sha)",
          "            if not autocorrected_path and path not in self.get_valid_file_paths(sha):",
          "                return None, \"utf-8\"",
          "",
          "        try:",
          "            contents = self.repo.get_contents(path, ref=sha)",
          "",
          "            if isinstance(contents, list):",
          "                raise Exception(f\"Expected a single ContentFile but got a list for path {path}\")",
          "",
          "            detected_encoding = detect_encoding(contents.decoded_content) if contents else \"utf-8\"",
          "            content = contents.decoded_content.decode(detected_encoding)",
          "            if autocorrected_path:",
          "                content = f\"Showing results instead for {path}\\n=====\\n{content}\"",
          "            return content, detected_encoding",
          "        except Exception as e:",
          "            logger.exception(f\"Error getting file contents: {e}\")",
          "            return None, \"utf-8\"",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_valid_file_paths(self, sha: str | None = None) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        valid_file_paths: set[str] = set()",
          "        valid_file_extensions = get_all_supported_extensions()",
          "",
          "        for file in tree.tree:",
          "            if file.type == \"blob\" and any(",
          "                file.path.endswith(ext) for ext in valid_file_extensions",
          "            ):",
          "                valid_file_paths.add(file.path)",
          "",
          "        return valid_file_paths",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_history(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False, max_commits: int = 10",
          "    ) -> list[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(sha):",
          "                return []",
          "",
          "        commits = self.repo.get_commits(sha=sha, path=path)",
          "        commit_list = list(commits[:max_commits])",
          "        commit_strs = []",
          "",
          "        def process_commit(commit):",
          "            short_sha = commit.sha[:7]",
          "            message = commit.commit.message",
          "            files_touched = [",
          "                {\"path\": file.filename, \"status\": file.status} for file in commit.files[:20]",
          "            ]",
          "",
          "            # Build a file tree representation instead of a flat list",
          "            file_tree_str = self._build_file_tree_string(files_touched)",
          "",
          "            # Add a note about additional files if needed",
          "            additional_files_note = \"\"",
          "            if len(files_touched) < len(commit.files):",
          "                additional_files_note = (",
          "                    f\"\\n[and {len(commit.files) - len(files_touched)} more files were changed...]\"",
          "                )",
          "",
          "            string = textwrap.dedent(",
          "                \"\"\"\\",
          "                ----------------",
          "                {short_sha} - {message}",
          "                Files touched:",
          "                {file_tree}{additional_files}",
          "                \"\"\"",
          "            ).format(",
          "                short_sha=short_sha,",
          "                message=message,",
          "                file_tree=file_tree_str,",
          "                additional_files=additional_files_note,",
          "            )",
          "            return string",
          "",
          "        with ThreadPoolExecutor() as executor:",
          "            results = list(executor.map(process_commit, commit_list))",
          "",
          "        for result in results:",
          "            commit_strs.append(result)",
          "",
          "        return commit_strs",
          "",
          "    def _build_file_tree_string(",
          "        self, files: list[dict], only_immediate_children_of_path: str | None = None",
          "    ) -> str:",
          "        \"\"\"",
          "        Builds a tree representation of files to save tokens when many files share the same directories.",
          "        The output is similar to the 'tree' command in terminal.",
          "",
          "        Args:",
          "            files: List of dictionaries with 'path' and 'status' keys",
          "            only_immediate_children_of_path: If provided, only include files and directories that are immediate children of this path",
          "",
          "        Returns:",
          "            A string representation of the file tree",
          "        \"\"\"",
          "        if not files:",
          "            return \"No files changed\"",
          "",
          "        if only_immediate_children_of_path is not None:",
          "            only_immediate_children_of_path = only_immediate_children_of_path.rstrip(\"/\")",
          "",
          "        # First, build a nested dictionary structure representing the file tree",
          "        tree: dict = {}",
          "        for file in files:",
          "            path = file[\"path\"]",
          "            status = file[\"status\"]",
          "",
          "            # Split the path into components",
          "            parts = path.split(\"/\")",
          "",
          "            # Navigate through the tree, creating nodes as needed",
          "            current = tree",
          "            for i, part in enumerate(parts):",
          "                # If this is the last part (the filename)",
          "                if i == len(parts) - 1:",
          "                    current[part] = {\"__status__\": status}",
          "                else:",
          "                    if part not in current:",
          "                        current[part] = {}",
          "                    current = current[part]",
          "",
          "        # Now build the tree string recursively",
          "        lines = []",
          "",
          "        def _build_tree(node, previous_parts=[], prefix=\"\", is_last=True, is_root=True):",
          "            items = list(node.items())",
          "",
          "            # Process each item",
          "            for i, (key, value) in enumerate(sorted(items)):",
          "                if key == \"__status__\":",
          "                    continue",
          "",
          "                # Determine if this is the last item at this level",
          "                is_last_item = i == len(items) - 1 or (i == len(items) - 2 and \"__status__\" in node)",
          "",
          "                # Create the appropriate prefix for this line",
          "                if is_root:",
          "                    current_prefix = \"\"",
          "                    next_prefix = \"\"",
          "                else:",
          "                    current_prefix = prefix + (\"└── \" if is_last else \"├── \")",
          "                    next_prefix = prefix + (\"    \" if is_last else \"│   \")",
          "",
          "                # If this is a file (has a status)",
          "                if \"__status__\" in value:",
          "                    if (",
          "                        only_immediate_children_of_path is not None",
          "                        and not only_immediate_children_of_path == (\"/\".join(previous_parts))",
          "                    ):",
          "                        continue",
          "",
          "                    status = value[\"__status__\"]",
          "                    status_str = f\" ({status})\" if status else \"\"",
          "                    lines.append(f\"{current_prefix}{key}{status_str}\")",
          "                # If this is a directory",
          "                else:",
          "                    # If this is within the specified path",
          "                    if (",
          "                        only_immediate_children_of_path is None",
          "                        or only_immediate_children_of_path.startswith(",
          "                            \"/\".join(previous_parts + [key])",
          "                        )",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "                        _build_tree(value, previous_parts + [key], next_prefix, is_last_item, False)",
          "                    elif (",
          "                        only_immediate_children_of_path is not None",
          "                        and only_immediate_children_of_path == \"/\".join(previous_parts)",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "",
          "        # Start building the tree from the root",
          "        _build_tree(tree)",
          "",
          "        return \"\\n\".join(lines)",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_patch_for_file(",
          "        self, path: str, commit_sha: str, autocorrect: bool = False",
          "    ) -> str | None:",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, commit_sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(commit_sha):",
          "                return None",
          "",
          "        commit = self.repo.get_commit(commit_sha)",
          "        matching_file = next((file for file in commit.files if file.filename == path), None)",
          "        if not matching_file:",
          "            return None",
          "",
          "        return matching_file.patch",
          "",
          "    def _create_branch(self, branch_name, from_base_sha=False):",
          "        ref = self.repo.create_git_ref(",
          "            ref=f\"refs/heads/{branch_name}\",",
          "            sha=(",
          "                self.base_commit_sha",
          "                if from_base_sha",
          "                else self.get_branch_head_sha(self.base_branch)",
          "            ),",
          "        )",
          "        return ref",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_git_tree(self, sha: str) -> CompleteGitTree:",
          "        \"\"\"",
          "        Get the git tree for a specific sha, handling truncation with divide and conquer.",
          "        Always returns a CompleteGitTree instance for consistent interface.",
          "",
          "        First tries to get the complete tree recursively. If truncated, it uses a",
          "        divide and conquer approach to fetch all subtrees individually and combine them.",
          "",
          "        Args:",
          "            sha: The commit SHA to get the tree for",
          "",
          "        Returns:",
          "            A CompleteGitTree with all items from all subtrees",
          "        \"\"\"",
          "        tree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not tree.raw_data.get(\"truncated\", False):",
          "            return CompleteGitTree(tree)",
          "",
          "        complete_tree = CompleteGitTree()",
          "        root_tree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        for key, value in root_tree.raw_data.items():",
          "            if key != \"tree\" and key != \"truncated\":",
          "                complete_tree.raw_data[key] = value",
          "",
          "        for item in root_tree.tree:",
          "            complete_tree.add_item(item)",
          "",
          "        tree_items = [item for item in root_tree.tree if item.type == \"tree\"]",
          "        with ThreadPoolExecutor() as executor:",
          "            subtree_results = []",
          "            for item in tree_items:",
          "                subtree_results.append(executor.submit(self._get_git_subtree, item.sha))",
          "",
          "            for future in subtree_results:",
          "                subtree_items = future.result()",
          "                complete_tree.add_items(subtree_items)",
          "",
          "        return complete_tree",
          "",
          "    def _get_git_subtree(self, sha: str) -> list:",
          "        \"\"\"",
          "        Process a subtree and return all its items for parallel execution.",
          "",
          "        Args:",
          "            sha: The SHA of the subtree",
          "",
          "        Returns:",
          "            A list of all tree items from this subtree and its nested subtrees",
          "        \"\"\"",
          "        items = []",
          "        subtree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not subtree.raw_data.get(\"truncated\", False):",
          "            return subtree.tree",
          "",
          "        non_recursive_subtree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        nested_tree_items = [item for item in non_recursive_subtree.tree if item.type == \"tree\"]",
          "        non_tree_items = [item for item in non_recursive_subtree.tree if item.type != \"tree\"]",
          "",
          "        items.extend(non_tree_items)",
          "",
          "        if nested_tree_items:",
          "            with ThreadPoolExecutor() as executor:",
          "                subtree_futures = [",
          "                    executor.submit(self._get_git_subtree, item.sha) for item in nested_tree_items",
          "                ]",
          "",
          "                for future in subtree_futures:",
          "                    items.extend(future.result())",
          "",
          "        return items",
          "",
          "    def process_one_file_for_git_commit(",
          "        self, *, branch_ref: str, patch: FilePatch | None = None, change: FileChange | None = None",
          "    ) -> InputGitTreeElement | None:",
          "        \"\"\"",
          "        This method is used to get a single change to be committed by to github.",
          "        It processes a FilePatch/FileChange object and converts it into an InputGitTreeElement which can be commited",
          "        It supports both FilePatch and FileChange objects.",
          "        \"\"\"",
          "        path = patch.path if patch else (change.path if change else None)",
          "        patch_type = patch.type if patch else (change.change_type if change else None)",
          "        if not path:",
          "            raise ValueError(\"Path must be provided\")",
          "",
          "        if not patch_type:",
          "            raise ValueError(\"Patch type must be provided\")",
          "        if patch_type == \"create\":",
          "            patch_type = \"A\"",
          "        elif patch_type == \"delete\":",
          "            patch_type = \"D\"",
          "        elif patch_type == \"edit\":",
          "            patch_type = \"M\"",
          "",
          "        to_apply = None",
          "        detected_encoding = \"utf-8\"",
          "        if patch_type != \"A\":",
          "            to_apply, detected_encoding = self.get_file_content(path, sha=branch_ref)",
          "",
          "        new_contents = (",
          "            patch.apply(to_apply) if patch else (change.apply(to_apply) if change else None)",
          "        )",
          "",
          "        # Remove leading slash if it exists, the github api will reject paths with leading slashes.",
          "        if path.startswith(\"/\"):",
          "            path = path[1:]",
          "",
          "        # don't create a blob if the file is being deleted",
          "        blob = self.repo.create_git_blob(new_contents, detected_encoding) if new_contents else None",
          "",
          "        # Prevent creating tree elements with None SHA for file additions",
          "        if patch_type == \"A\" and blob is None:",
          "            return None",
          "",
          "        # 100644 is the git code for creating a Regular non-executable file",
          "        # https://stackoverflow.com/questions/737673/how-to-read-the-mode-field-of-git-ls-trees-output",
          "        return InputGitTreeElement(",
          "            path=path, mode=\"100644\", type=\"blob\", sha=blob.sha if blob else None",
          "        )",
          "",
          "    def get_branch_ref(self, branch_name: str) -> GitRef | None:",
          "        try:",
          "            return self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        except GithubException as e:",
          "            if e.status == 404:",
          "                return None",
          "            raise e",
          "",
          "    def create_branch_from_changes(",
          "        self,",
          "        *,",
          "        pr_title: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "        branch_name: str | None = None,",
          "        from_base_sha: bool = False,",
          "    ) -> GitRef | None:",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Either file_patches or file_changes must be provided\")",
          "",
          "        new_branch_name = sanitize_branch_name(branch_name or pr_title)",
          "",
          "        try:",
          "            branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "        except GithubException as e:",
          "            # only use the random suffix if the branch already exists",
          "            if e.status == 409 or e.status == 422:",
          "                new_branch_name = f\"{new_branch_name}-{generate_random_string(n=6)}\"",
          "                branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "            else:",
          "                raise e",
          "",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, patch=patch",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file patch: {e}\")",
          "",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, change=change",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file change: {e}\")",
          "        # latest commit is the head of new branch",
          "        latest_commit = self.repo.get_git_commit(self.get_branch_head_sha(new_branch_name))",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "",
          "        new_commit = self.repo.create_git_commit(",
          "            message=pr_title, tree=new_tree, parents=[latest_commit]",
          "        )",
          "",
          "        branch_ref.edit(sha=new_commit.sha)",
          "",
          "        # Check that the changes were made",
          "        comparison = self.repo.compare(",
          "            self.get_branch_head_sha(self.base_branch), branch_ref.object.sha",
          "        )",
          "",
          "        if comparison.ahead_by < 1:",
          "            # Remove the branch if there are no changes",
          "            try:",
          "                branch_ref.delete()",
          "            except UnknownObjectException:",
          "                logger.warning(\"Attempted to delete a branch or reference that does not exist.\")",
          "            sentry_sdk.capture_message(",
          "                f\"Failed to create branch from changes. Comparison is ahead by {comparison.ahead_by}\"",
          "            )",
          "            return None",
          "",
          "        return branch_ref",
          "",
          "    def create_pr_from_branch(",
          "        self,",
          "        branch: GitRef,",
          "        title: str,",
          "        description: str,",
          "        provided_base: str | None = None,",
          "    ) -> PullRequest:",
          "        pulls = self.repo.get_pulls(state=\"open\", head=f\"{self.repo_owner}:{branch.ref}\")",
          "",
          "        if pulls.totalCount > 0:",
          "            logger.error(",
          "                f\"Branch {branch.ref} already has an open PR.\",",
          "                extra={",
          "                    \"branch_ref\": branch.ref,",
          "                    \"title\": title,",
          "                    \"description\": description,",
          "                    \"provided_base\": provided_base,",
          "                },",
          "            )",
          "",
          "            return pulls[0]",
          "",
          "        try:",
          "            return self.repo.create_pull(",
          "                title=title,",
          "                body=description,",
          "                base=provided_base or self.base_branch or self.get_default_branch(),",
          "                head=branch.ref,",
          "                draft=True,",
          "            )",
          "        except GithubException as e:",
          "            if e.status == 422 and \"Draft pull requests are not supported\" in str(e):",
          "                # fallback to creating a regular PR if draft PR is not supported",
          "                return self.repo.create_pull(",
          "                    title=title,",
          "                    body=description,",
          "                    base=provided_base or self.base_branch or self.get_default_branch(),",
          "                    head=branch.ref,",
          "                    draft=False,",
          "                )",
          "            else:",
          "                logger.exception(\"Error creating PR\")",
          "                raise e",
          "",
          "    def get_index_file_set(",
          "        self, sha: str | None = None, max_file_size_bytes=2 * 1024 * 1024, skip_empty_files=False",
          "    ) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        file_set = set()",
          "        for file in tree.tree:",
          "            if (",
          "                file.type == \"blob\"",
          "                and file.size < max_file_size_bytes",
          "                and file.mode",
          "                in [\"100644\", \"100755\"]  # 100644 is a regular file, 100755 is an executable file",
          "                and get_language_from_path(file.path) is not None",
          "                and (not skip_empty_files or file.size > 0)",
          "            ):",
          "                file_set.add(file.path)",
          "",
          "        return file_set",
          "",
          "    def get_pr_diff_content(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"diff\"))",
          "",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.text",
          "",
          "    def _get_auth_headers(self, accept_type: Literal[\"json\", \"diff\"] = \"json\"):",
          "        requester = self.repo._requester",
          "        if requester.auth is None:",
          "            raise Exception(\"No auth token found for GitHub API\")",
          "        headers = {",
          "            \"Accept\": (",
          "                \"application/vnd.github.diff\"",
          "                if accept_type == \"diff\"",
          "                else \"application/vnd.github+json\"",
          "            ),",
          "            \"Authorization\": f\"Bearer {requester.auth.token}\",",
          "            \"X-GitHub-Api-Version\": \"2022-11-28\",",
          "        }",
          "        return headers",
          "",
          "    def comment_root_cause_on_pr_for_copilot(",
          "        self, pr_url: str, run_id: int, issue_id: int, comment: str",
          "    ):",
          "        pull_id = int(pr_url.split(\"/\")[-1])",
          "        repo_name = pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "        params = {",
          "            \"body\": comment,",
          "            \"actions\": [",
          "                {",
          "                    \"name\": \"Fix with Sentry\",",
          "                    \"type\": \"copilot-chat\",",
          "                    \"prompt\": f\"@sentry find a fix for issue {issue_id} with run ID {run_id}\",",
          "                }",
          "            ],",
          "        }",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def comment_pr_generated_for_copilot(",
          "        self, pr_to_comment_on_url: str, new_pr_url: str, run_id: int",
          "    ):",
          "        pull_id = int(pr_to_comment_on_url.split(\"/\")[-1])",
          "        repo_name = pr_to_comment_on_url.split(\"github.com/\")[1].split(\"/pull\")[",
          "            0",
          "        ]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "",
          "        comment = f\"A fix has been generated and is available [here]({new_pr_url}) for your review. Autofix Run ID: {run_id}\"",
          "",
          "        params = {\"body\": comment}",
          "",
          "        headers = self._get_auth_headers()",
          "",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def get_pr_head_sha(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"json\"))",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.json()[\"head\"][\"sha\"]",
          "",
          "    def post_unit_test_reference_to_original_pr(self, original_pr_url: str, unit_test_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Sentry has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_reference_to_original_pr_codecov_app(",
          "        self, original_pr_url: str, unit_test_pr_url: str",
          "    ):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Codecov has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_not_generated_message_to_original_pr(self, original_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = \"Sentry has determined that unit tests are not necessary for this PR.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_issue_comment(self, pr_url: str, comment: str):",
          "        \"\"\"",
          "        Create an issue comment on a GitHub issue (all pull requests are issues).",
          "        This can be used to create an overall PR comment instead of associated with a specific line.",
          "        See https://docs.github.com/en/rest/issues/comments?apiVersion=2022-11-28#create-an-issue-comment",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        issue = self.repo.get_issue(number=pr_id)",
          "        comment_obj = issue.create_comment(body=comment)",
          "        return comment_obj.html_url",
          "",
          "    def post_pr_review_comment(self, pr_url: str, comment: GithubPrReviewComment):",
          "        \"\"\"",
          "        Create a review comment on a GitHub pull request.",
          "        See https://docs.github.com/en/rest/pulls/comments?apiVersion=2022-11-28#create-a-review-comment-for-a-pull-request",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        pr = self.repo.get_pull(number=pr_id)",
          "        commit = self.repo.get_commit(comment[\"commit_id\"])",
          "",
          "        review_comment = pr.create_review_comment(",
          "            body=comment[\"body\"],",
          "            commit=commit,",
          "            path=comment[\"path\"],",
          "            line=comment.get(\"line\", GithubObject.NotSet),",
          "            side=comment.get(\"side\", GithubObject.NotSet),",
          "            start_line=comment.get(\"start_line\", GithubObject.NotSet),",
          "        )",
          "        return review_comment.html_url",
          "",
          "    def push_new_commit_to_pr(",
          "        self,",
          "        pr,",
          "        commit_message: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "    ):",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Must provide file_patches or file_changes\")",
          "        branch_name = pr.head.ref",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                element = self.process_one_file_for_git_commit(branch_ref=branch_name, patch=patch)",
          "                if element:",
          "                    tree_elements.append(element)",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                element = self.process_one_file_for_git_commit(",
          "                    branch_ref=branch_name, change=change",
          "                )",
          "                if element:",
          "                    tree_elements.append(element)",
          "        if not tree_elements:",
          "            logger.warning(\"No valid changes to commit\")",
          "            return None",
          "        latest_sha = self.get_branch_head_sha(branch_name)",
          "        latest_commit = self.repo.get_git_commit(latest_sha)",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "        new_commit = self.repo.create_git_commit(",
          "            message=commit_message, tree=new_tree, parents=[latest_commit]",
          "        )",
          "        branch_ref = self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        branch_ref.edit(sha=new_commit.sha)",
          "        return new_commit",
          ""
        ]
      }
    },
    {
      "file": "github/GithubIntegration.py",
      "image": "github.GithubIntegration",
      "is_application": false,
      "line": 244,
      "name": "GithubIntegration.get_repo_installation",
      "path": "/usr/local/lib/python3.11/dist-packages/github/GithubIntegration.py"
    },
    {
      "file": "github/GithubIntegration.py",
      "image": "github.GithubIntegration",
      "is_application": false,
      "line": 166,
      "name": "GithubIntegration._get_installed_app",
      "path": "/usr/local/lib/python3.11/dist-packages/github/GithubIntegration.py"
    },
    {
      "file": "github/MainClass.py",
      "image": "github.MainClass",
      "is_application": false,
      "line": 380,
      "name": "Github.get_repo",
      "path": "/usr/local/lib/python3.11/dist-packages/github/MainClass.py"
    },
    {
      "file": "seer/automation/codebase/repo_client.py",
      "image": "seer.automation.codebase.repo_client",
      "is_application": true,
      "line": 272,
      "name": "RepoClient.get_branch_head_sha",
      "path": "/app/src/seer/automation/codebase/repo_client.py",
      "codeContext": {
        "file": "seer/automation/codebase/repo_client.py",
        "line": 272,
        "name": "RepoClient.get_branch_head_sha",
        "code": "\n    @property\n    def repo_full_name(self):\n        return self.repo.full_name\n\n    def get_default_branch(self) -> str:\n        return self.repo.default_branch\n\n    def get_branch_head_sha(self, branch: str):\n        return self.repo.get_branch(branch).commit.sha\n\n    def compare(self, base: str, head: str):\n        return self.repo.compare(base, head)\n\n    def load_repo_to_tmp_dir(self, sha: str | None = None) -> tuple[str, str]:\n        sha = sha or self.base_commit_sha\n\n        # Check if output directory exists, if not create it\n        tmp_dir = tempfile.mkdtemp(prefix=f\"{self.repo_owner}-{self.repo_name}_{sha}\")\n        tmp_repo_dir = os.path.join(tmp_dir, \"repo\")",
        "lineRange": {
          "start": 263,
          "end": 282
        },
        "lines": [
          "import functools",
          "import logging",
          "import os",
          "import shutil",
          "import tarfile",
          "import tempfile",
          "import textwrap",
          "from concurrent.futures import ThreadPoolExecutor",
          "from enum import Enum",
          "from typing import Any, Dict, List, Literal",
          "",
          "import requests",
          "import sentry_sdk",
          "from github import (",
          "    Auth,",
          "    Github,",
          "    GithubException,",
          "    GithubIntegration,",
          "    GithubObject,",
          "    InputGitTreeElement,",
          "    UnknownObjectException,",
          ")",
          "from github.GitRef import GitRef",
          "from github.GitTree import GitTree",
          "from github.GitTreeElement import GitTreeElement",
          "from github.PullRequest import PullRequest",
          "from github.Repository import Repository",
          "",
          "from seer.automation.autofix.utils import generate_random_string, sanitize_branch_name",
          "from seer.automation.codebase.models import GithubPrReviewComment",
          "from seer.automation.codebase.utils import get_all_supported_extensions, get_language_from_path",
          "from seer.automation.models import FileChange, FilePatch, InitializationError, RepoDefinition",
          "from seer.automation.utils import detect_encoding",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "def get_github_app_auth_and_installation(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          "):",
          "    app_auth = Auth.AppAuth(app_id, private_key=private_key)",
          "    gi = GithubIntegration(auth=app_auth)",
          "    installation = gi.get_repo_installation(repo_owner, repo_name)",
          "    github_auth = app_auth.get_installation_auth(installation.id)",
          "",
          "    return github_auth, installation",
          "",
          "",
          "def get_repo_app_permissions(",
          "    app_id: int | str, private_key: str, repo_owner: str, repo_name: str",
          ") -> dict[str, str] | None:",
          "    try:",
          "        _, installation = get_github_app_auth_and_installation(",
          "            app_id, private_key, repo_owner, repo_name",
          "        )",
          "",
          "        return installation.raw_data.get(\"permissions\", {})",
          "    except UnknownObjectException:",
          "        return None",
          "",
          "",
          "@inject",
          "def get_github_token_auth(config: AppConfig = injected) -> Auth.Token | None:",
          "    github_token = config.GITHUB_TOKEN",
          "    if github_token is None:",
          "        return None",
          "",
          "    return Auth.Token(github_token)",
          "",
          "",
          "@inject",
          "def get_write_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_APP_ID",
          "    private_key = config.GITHUB_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return None, None",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_read_app_credentials(config: AppConfig = injected) -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_SENTRY_APP_ID",
          "    private_key = config.GITHUB_SENTRY_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_unit_test_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_UNIT_TEST_APP_ID",
          "    private_key = config.GITHUB_CODECOV_UNIT_TEST_PRIVATE_KEY",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov unit test app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "@inject",
          "def get_codecov_pr_review_app_credentials(",
          "    config: AppConfig = injected,",
          ") -> tuple[int | str | None, str | None]:",
          "    app_id = config.GITHUB_CODECOV_PR_REVIEW_APP_ID",
          "    private_key = config.GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY",
          "",
          "    if not app_id:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_APP_ID\")",
          "    if not private_key:",
          "        logger.warning(\"No key set GITHUB_CODECOV_PR_REVIEW_PRIVATE_KEY\")",
          "",
          "    if not app_id or not private_key:",
          "        sentry_sdk.capture_message(\"Invalid credentials for codecov pr review app.\")",
          "        return get_write_app_credentials()",
          "",
          "    return app_id, private_key",
          "",
          "",
          "class RepoClientType(str, Enum):",
          "    READ = \"read\"",
          "    WRITE = \"write\"",
          "    CODECOV_UNIT_TEST = \"codecov_unit_test\"",
          "    CODECOV_PR_REVIEW = \"codecov_pr_review\"",
          "    CODECOV_PR_CLOSED = \"codecov_pr_closed\"",
          "",
          "",
          "class CompleteGitTree:",
          "    \"\"\"",
          "    A custom class that mimics the interface of github.GitTree",
          "    but allows combining multiple trees into one complete representation.",
          "    \"\"\"",
          "",
          "    def __init__(self, github_tree: GitTree | None = None) -> None:",
          "        self.tree: List[GitTreeElement] = []",
          "        self.raw_data: Dict[str, Any] = {\"truncated\": False}",
          "",
          "        if github_tree:",
          "            self.add_items(github_tree.tree)",
          "            for key, value in github_tree.raw_data.items():",
          "                if key != \"truncated\":  # We always set truncated to False for our complete tree",
          "                    self.raw_data[key] = value",
          "",
          "    def add_item(self, item: GitTreeElement) -> None:",
          "        \"\"\"Add a tree item to this collection\"\"\"",
          "        self.tree.append(item)",
          "",
          "    def add_items(self, items: List[GitTreeElement]) -> None:",
          "        \"\"\"Add multiple tree items to this collection\"\"\"",
          "        self.tree.extend(items)",
          "",
          "",
          "class RepoClient:",
          "    # TODO: Support other git providers later",
          "    github_auth: Auth.Token | Auth.AppInstallationAuth",
          "    github: Github",
          "    repo: Repository",
          "",
          "    provider: str",
          "    repo_owner: str",
          "    repo_name: str",
          "    repo_external_id: str",
          "    base_commit_sha: str",
          "    base_branch: str",
          "",
          "    supported_providers = [\"github\"]",
          "",
          "    def __init__(",
          "        self, app_id: int | str | None, private_key: str | None, repo_definition: RepoDefinition",
          "    ):",
          "        if repo_definition.provider not in self.supported_providers:",
          "            # This should never get here, the repo provider should be checked on the Sentry side but this will make debugging",
          "            # easier if it does",
          "            raise InitializationError(",
          "                f\"Unsupported repo provider: {repo_definition.provider}, only {', '.join(self.supported_providers)} are supported.\"",
          "            )",
          "",
          "        if app_id and private_key:",
          "            self.github = Github(",
          "                auth=get_github_app_auth_and_installation(",
          "                    app_id, private_key, repo_definition.owner, repo_definition.name",
          "                )[0]",
          "            )",
          "        else:",
          "            self.github = Github(auth=get_github_token_auth())",
          "",
          "        self.repo = self.github.get_repo(",
          "            int(repo_definition.external_id)",
          "            if repo_definition.external_id.isdigit()",
          "            else repo_definition.full_name",
          "        )",
          "",
          "        self.provider = repo_definition.provider",
          "        self.repo_owner = repo_definition.owner",
          "        self.repo_name = repo_definition.name",
          "        self.repo_external_id = repo_definition.external_id",
          "        self.base_branch = repo_definition.branch_name or self.get_default_branch()",
          "        self.base_commit_sha = repo_definition.base_commit_sha or self.get_branch_head_sha(",
          "            self.base_branch",
          "        )",
          "",
          "    @staticmethod",
          "    def check_repo_write_access(repo: RepoDefinition):",
          "        app_id, pk = get_write_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if (",
          "            permissions",
          "            and permissions.get(\"contents\") == \"write\"",
          "            and permissions.get(\"pull_requests\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def check_repo_read_access(repo: RepoDefinition):",
          "        app_id, pk = get_read_app_credentials()",
          "",
          "        if app_id is None or pk is None:",
          "            return True if get_github_token_auth() else None",
          "",
          "        permissions = get_repo_app_permissions(app_id, pk, repo.owner, repo.name)",
          "",
          "        if permissions and (",
          "            permissions.get(\"contents\") == \"read\" or permissions.get(\"contents\") == \"write\"",
          "        ):",
          "            return True",
          "",
          "        return False",
          "",
          "    @staticmethod",
          "    def _extract_id_from_pr_url(pr_url: str):",
          "        \"\"\"",
          "        Extracts the repository path and PR/issue ID from the provided URL.",
          "        \"\"\"",
          "        pr_id = int(pr_url.split(\"/\")[-1])",
          "        return pr_id",
          "",
          "    @classmethod",
          "    @functools.lru_cache(maxsize=8)",
          "    def from_repo_definition(cls, repo_def: RepoDefinition, type: RepoClientType):",
          "        if type == RepoClientType.WRITE:",
          "            return cls(*get_write_app_credentials(), repo_def)",
          "        elif type == RepoClientType.CODECOV_UNIT_TEST:",
          "            return cls(*get_codecov_unit_test_app_credentials(), repo_def)",
          "        elif type in (RepoClientType.CODECOV_PR_REVIEW, RepoClientType.CODECOV_PR_CLOSED):",
          "            return cls(*get_codecov_pr_review_app_credentials(), repo_def)",
          "",
          "        return cls(*get_read_app_credentials(), repo_def)",
          "",
          "    @property",
          "    def repo_full_name(self):",
          "        return self.repo.full_name",
          "",
          "    def get_default_branch(self) -> str:",
          "        return self.repo.default_branch",
          "",
          "    def get_branch_head_sha(self, branch: str):",
          "        return self.repo.get_branch(branch).commit.sha",
          "",
          "    def compare(self, base: str, head: str):",
          "        return self.repo.compare(base, head)",
          "",
          "    def load_repo_to_tmp_dir(self, sha: str | None = None) -> tuple[str, str]:",
          "        sha = sha or self.base_commit_sha",
          "",
          "        # Check if output directory exists, if not create it",
          "        tmp_dir = tempfile.mkdtemp(prefix=f\"{self.repo_owner}-{self.repo_name}_{sha}\")",
          "        tmp_repo_dir = os.path.join(tmp_dir, \"repo\")",
          "",
          "        logger.debug(f\"Loading repository to {tmp_repo_dir}\")",
          "",
          "        # Create a temporary directory to store the repository",
          "        os.makedirs(tmp_repo_dir, exist_ok=True)",
          "",
          "        # Clean the directory",
          "        for root, dirs, files in os.walk(tmp_repo_dir, topdown=False):",
          "            for name in files:",
          "                os.remove(os.path.join(root, name))",
          "            for name in dirs:",
          "                os.rmdir(os.path.join(root, name))",
          "",
          "        tarball_url = self.repo.get_archive_link(\"tarball\", ref=sha)",
          "        tarfile_path = os.path.join(tmp_dir, f\"{sha}.tar.gz\")",
          "",
          "        response = requests.get(tarball_url, stream=True)",
          "        if response.status_code == 200:",
          "            with open(tarfile_path, \"wb\") as f:",
          "                f.write(response.content)",
          "        else:",
          "            logger.error(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "            logger.error(",
          "                f\"Response status code: {response.status_code}, response text: {response.text}\"",
          "            )",
          "            raise Exception(",
          "                f\"Failed to get tarball url for {tarball_url}. Please check if the repository exists and the provided token is valid.\"",
          "            )",
          "",
          "        # Extract tarball into the output directory",
          "        with tarfile.open(tarfile_path, \"r:gz\") as tar:",
          "            tar.extractall(path=tmp_repo_dir)  # extract all members normally",
          "            extracted_folders = [",
          "                name",
          "                for name in os.listdir(tmp_repo_dir)",
          "                if os.path.isdir(os.path.join(tmp_repo_dir, name))",
          "            ]",
          "            if extracted_folders:",
          "                root_folder = extracted_folders[0]  # assuming the first folder is the root folder",
          "                root_folder_path = os.path.join(tmp_repo_dir, root_folder)",
          "                for item in os.listdir(root_folder_path):",
          "                    s = os.path.join(root_folder_path, item)",
          "                    d = os.path.join(tmp_repo_dir, item)",
          "                    # TODO: Consider a strategy for handling symlinks more appropriately in the future, possibly by resolving them or copying as symlinks to maintain the original structure.",
          "                    if os.path.isdir(s):",
          "                        shutil.move(",
          "                            s, d",
          "                        )  # move all directories from the root folder to the output directory",
          "                    else:",
          "                        # Skipping symlinks to prevent FileNotFoundError.",
          "                        if not os.path.islink(s):",
          "                            shutil.copy2(",
          "                                s, d",
          "                            )  # copy all files from the root folder to the output directory",
          "",
          "                shutil.rmtree(root_folder_path)  # remove the root folder",
          "",
          "        return tmp_dir, tmp_repo_dir",
          "",
          "    def _autocorrect_path(self, path: str, sha: str | None = None) -> tuple[str, bool]:",
          "        \"\"\"",
          "        Attempts to autocorrect a file path by finding the closest match in the repository.",
          "",
          "        Args:",
          "            path: The path to autocorrect",
          "            sha: The commit SHA to use for finding valid paths",
          "",
          "        Returns:",
          "            A tuple of (corrected_path, was_autocorrected)",
          "        \"\"\"",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        path = path.lstrip(\"/\")",
          "        valid_paths = self.get_valid_file_paths(sha)",
          "",
          "        # If path is valid, return it unchanged",
          "        if path in valid_paths:",
          "            return path, False",
          "",
          "        # Check for partial matches if no exact match and path is long enough",
          "        if len(path) > 3:",
          "            path_lower = path.lower()",
          "            partial_matches = [",
          "                valid_path for valid_path in valid_paths if path_lower in valid_path.lower()",
          "            ]",
          "            if partial_matches:",
          "                # Sort by length to get closest match (shortest containing path)",
          "                closest_match = sorted(partial_matches, key=len)[0]",
          "                logger.warning(",
          "                    f\"Path '{path}' not found exactly, using closest match: '{closest_match}'\"",
          "                )",
          "                return closest_match, True",
          "",
          "        # No match found",
          "        logger.warning(\"No matching file found for provided file path\", extra={\"path\": path})",
          "        return path, False",
          "",
          "    def get_file_content(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False",
          "    ) -> tuple[str | None, str]:",
          "        logger.debug(f\"Getting file contents for {path} in {self.repo.full_name} on sha {sha}\")",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        autocorrected_path = False",
          "        if autocorrect:",
          "            path, autocorrected_path = self._autocorrect_path(path, sha)",
          "            if not autocorrected_path and path not in self.get_valid_file_paths(sha):",
          "                return None, \"utf-8\"",
          "",
          "        try:",
          "            contents = self.repo.get_contents(path, ref=sha)",
          "",
          "            if isinstance(contents, list):",
          "                raise Exception(f\"Expected a single ContentFile but got a list for path {path}\")",
          "",
          "            detected_encoding = detect_encoding(contents.decoded_content) if contents else \"utf-8\"",
          "            content = contents.decoded_content.decode(detected_encoding)",
          "            if autocorrected_path:",
          "                content = f\"Showing results instead for {path}\\n=====\\n{content}\"",
          "            return content, detected_encoding",
          "        except Exception as e:",
          "            logger.exception(f\"Error getting file contents: {e}\")",
          "            return None, \"utf-8\"",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_valid_file_paths(self, sha: str | None = None) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        valid_file_paths: set[str] = set()",
          "        valid_file_extensions = get_all_supported_extensions()",
          "",
          "        for file in tree.tree:",
          "            if file.type == \"blob\" and any(",
          "                file.path.endswith(ext) for ext in valid_file_extensions",
          "            ):",
          "                valid_file_paths.add(file.path)",
          "",
          "        return valid_file_paths",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_history(",
          "        self, path: str, sha: str | None = None, autocorrect: bool = False, max_commits: int = 10",
          "    ) -> list[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(sha):",
          "                return []",
          "",
          "        commits = self.repo.get_commits(sha=sha, path=path)",
          "        commit_list = list(commits[:max_commits])",
          "        commit_strs = []",
          "",
          "        def process_commit(commit):",
          "            short_sha = commit.sha[:7]",
          "            message = commit.commit.message",
          "            files_touched = [",
          "                {\"path\": file.filename, \"status\": file.status} for file in commit.files[:20]",
          "            ]",
          "",
          "            # Build a file tree representation instead of a flat list",
          "            file_tree_str = self._build_file_tree_string(files_touched)",
          "",
          "            # Add a note about additional files if needed",
          "            additional_files_note = \"\"",
          "            if len(files_touched) < len(commit.files):",
          "                additional_files_note = (",
          "                    f\"\\n[and {len(commit.files) - len(files_touched)} more files were changed...]\"",
          "                )",
          "",
          "            string = textwrap.dedent(",
          "                \"\"\"\\",
          "                ----------------",
          "                {short_sha} - {message}",
          "                Files touched:",
          "                {file_tree}{additional_files}",
          "                \"\"\"",
          "            ).format(",
          "                short_sha=short_sha,",
          "                message=message,",
          "                file_tree=file_tree_str,",
          "                additional_files=additional_files_note,",
          "            )",
          "            return string",
          "",
          "        with ThreadPoolExecutor() as executor:",
          "            results = list(executor.map(process_commit, commit_list))",
          "",
          "        for result in results:",
          "            commit_strs.append(result)",
          "",
          "        return commit_strs",
          "",
          "    def _build_file_tree_string(",
          "        self, files: list[dict], only_immediate_children_of_path: str | None = None",
          "    ) -> str:",
          "        \"\"\"",
          "        Builds a tree representation of files to save tokens when many files share the same directories.",
          "        The output is similar to the 'tree' command in terminal.",
          "",
          "        Args:",
          "            files: List of dictionaries with 'path' and 'status' keys",
          "            only_immediate_children_of_path: If provided, only include files and directories that are immediate children of this path",
          "",
          "        Returns:",
          "            A string representation of the file tree",
          "        \"\"\"",
          "        if not files:",
          "            return \"No files changed\"",
          "",
          "        if only_immediate_children_of_path is not None:",
          "            only_immediate_children_of_path = only_immediate_children_of_path.rstrip(\"/\")",
          "",
          "        # First, build a nested dictionary structure representing the file tree",
          "        tree: dict = {}",
          "        for file in files:",
          "            path = file[\"path\"]",
          "            status = file[\"status\"]",
          "",
          "            # Split the path into components",
          "            parts = path.split(\"/\")",
          "",
          "            # Navigate through the tree, creating nodes as needed",
          "            current = tree",
          "            for i, part in enumerate(parts):",
          "                # If this is the last part (the filename)",
          "                if i == len(parts) - 1:",
          "                    current[part] = {\"__status__\": status}",
          "                else:",
          "                    if part not in current:",
          "                        current[part] = {}",
          "                    current = current[part]",
          "",
          "        # Now build the tree string recursively",
          "        lines = []",
          "",
          "        def _build_tree(node, previous_parts=[], prefix=\"\", is_last=True, is_root=True):",
          "            items = list(node.items())",
          "",
          "            # Process each item",
          "            for i, (key, value) in enumerate(sorted(items)):",
          "                if key == \"__status__\":",
          "                    continue",
          "",
          "                # Determine if this is the last item at this level",
          "                is_last_item = i == len(items) - 1 or (i == len(items) - 2 and \"__status__\" in node)",
          "",
          "                # Create the appropriate prefix for this line",
          "                if is_root:",
          "                    current_prefix = \"\"",
          "                    next_prefix = \"\"",
          "                else:",
          "                    current_prefix = prefix + (\"└── \" if is_last else \"├── \")",
          "                    next_prefix = prefix + (\"    \" if is_last else \"│   \")",
          "",
          "                # If this is a file (has a status)",
          "                if \"__status__\" in value:",
          "                    if (",
          "                        only_immediate_children_of_path is not None",
          "                        and not only_immediate_children_of_path == (\"/\".join(previous_parts))",
          "                    ):",
          "                        continue",
          "",
          "                    status = value[\"__status__\"]",
          "                    status_str = f\" ({status})\" if status else \"\"",
          "                    lines.append(f\"{current_prefix}{key}{status_str}\")",
          "                # If this is a directory",
          "                else:",
          "                    # If this is within the specified path",
          "                    if (",
          "                        only_immediate_children_of_path is None",
          "                        or only_immediate_children_of_path.startswith(",
          "                            \"/\".join(previous_parts + [key])",
          "                        )",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "                        _build_tree(value, previous_parts + [key], next_prefix, is_last_item, False)",
          "                    elif (",
          "                        only_immediate_children_of_path is not None",
          "                        and only_immediate_children_of_path == \"/\".join(previous_parts)",
          "                    ):",
          "                        lines.append(f\"{current_prefix}{key}/\")",
          "",
          "        # Start building the tree from the root",
          "        _build_tree(tree)",
          "",
          "        return \"\\n\".join(lines)",
          "",
          "    @functools.lru_cache(maxsize=16)",
          "    def get_commit_patch_for_file(",
          "        self, path: str, commit_sha: str, autocorrect: bool = False",
          "    ) -> str | None:",
          "        if autocorrect:",
          "            path, was_autocorrected = self._autocorrect_path(path, commit_sha)",
          "            if not was_autocorrected and path not in self.get_valid_file_paths(commit_sha):",
          "                return None",
          "",
          "        commit = self.repo.get_commit(commit_sha)",
          "        matching_file = next((file for file in commit.files if file.filename == path), None)",
          "        if not matching_file:",
          "            return None",
          "",
          "        return matching_file.patch",
          "",
          "    def _create_branch(self, branch_name, from_base_sha=False):",
          "        ref = self.repo.create_git_ref(",
          "            ref=f\"refs/heads/{branch_name}\",",
          "            sha=(",
          "                self.base_commit_sha",
          "                if from_base_sha",
          "                else self.get_branch_head_sha(self.base_branch)",
          "            ),",
          "        )",
          "        return ref",
          "",
          "    @functools.lru_cache(maxsize=8)",
          "    def get_git_tree(self, sha: str) -> CompleteGitTree:",
          "        \"\"\"",
          "        Get the git tree for a specific sha, handling truncation with divide and conquer.",
          "        Always returns a CompleteGitTree instance for consistent interface.",
          "",
          "        First tries to get the complete tree recursively. If truncated, it uses a",
          "        divide and conquer approach to fetch all subtrees individually and combine them.",
          "",
          "        Args:",
          "            sha: The commit SHA to get the tree for",
          "",
          "        Returns:",
          "            A CompleteGitTree with all items from all subtrees",
          "        \"\"\"",
          "        tree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not tree.raw_data.get(\"truncated\", False):",
          "            return CompleteGitTree(tree)",
          "",
          "        complete_tree = CompleteGitTree()",
          "        root_tree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        for key, value in root_tree.raw_data.items():",
          "            if key != \"tree\" and key != \"truncated\":",
          "                complete_tree.raw_data[key] = value",
          "",
          "        for item in root_tree.tree:",
          "            complete_tree.add_item(item)",
          "",
          "        tree_items = [item for item in root_tree.tree if item.type == \"tree\"]",
          "        with ThreadPoolExecutor() as executor:",
          "            subtree_results = []",
          "            for item in tree_items:",
          "                subtree_results.append(executor.submit(self._get_git_subtree, item.sha))",
          "",
          "            for future in subtree_results:",
          "                subtree_items = future.result()",
          "                complete_tree.add_items(subtree_items)",
          "",
          "        return complete_tree",
          "",
          "    def _get_git_subtree(self, sha: str) -> list:",
          "        \"\"\"",
          "        Process a subtree and return all its items for parallel execution.",
          "",
          "        Args:",
          "            sha: The SHA of the subtree",
          "",
          "        Returns:",
          "            A list of all tree items from this subtree and its nested subtrees",
          "        \"\"\"",
          "        items = []",
          "        subtree = self.repo.get_git_tree(sha=sha, recursive=True)",
          "",
          "        if not subtree.raw_data.get(\"truncated\", False):",
          "            return subtree.tree",
          "",
          "        non_recursive_subtree = self.repo.get_git_tree(sha=sha, recursive=False)",
          "",
          "        nested_tree_items = [item for item in non_recursive_subtree.tree if item.type == \"tree\"]",
          "        non_tree_items = [item for item in non_recursive_subtree.tree if item.type != \"tree\"]",
          "",
          "        items.extend(non_tree_items)",
          "",
          "        if nested_tree_items:",
          "            with ThreadPoolExecutor() as executor:",
          "                subtree_futures = [",
          "                    executor.submit(self._get_git_subtree, item.sha) for item in nested_tree_items",
          "                ]",
          "",
          "                for future in subtree_futures:",
          "                    items.extend(future.result())",
          "",
          "        return items",
          "",
          "    def process_one_file_for_git_commit(",
          "        self, *, branch_ref: str, patch: FilePatch | None = None, change: FileChange | None = None",
          "    ) -> InputGitTreeElement | None:",
          "        \"\"\"",
          "        This method is used to get a single change to be committed by to github.",
          "        It processes a FilePatch/FileChange object and converts it into an InputGitTreeElement which can be commited",
          "        It supports both FilePatch and FileChange objects.",
          "        \"\"\"",
          "        path = patch.path if patch else (change.path if change else None)",
          "        patch_type = patch.type if patch else (change.change_type if change else None)",
          "        if not path:",
          "            raise ValueError(\"Path must be provided\")",
          "",
          "        if not patch_type:",
          "            raise ValueError(\"Patch type must be provided\")",
          "        if patch_type == \"create\":",
          "            patch_type = \"A\"",
          "        elif patch_type == \"delete\":",
          "            patch_type = \"D\"",
          "        elif patch_type == \"edit\":",
          "            patch_type = \"M\"",
          "",
          "        to_apply = None",
          "        detected_encoding = \"utf-8\"",
          "        if patch_type != \"A\":",
          "            to_apply, detected_encoding = self.get_file_content(path, sha=branch_ref)",
          "",
          "        new_contents = (",
          "            patch.apply(to_apply) if patch else (change.apply(to_apply) if change else None)",
          "        )",
          "",
          "        # Remove leading slash if it exists, the github api will reject paths with leading slashes.",
          "        if path.startswith(\"/\"):",
          "            path = path[1:]",
          "",
          "        # don't create a blob if the file is being deleted",
          "        blob = self.repo.create_git_blob(new_contents, detected_encoding) if new_contents else None",
          "",
          "        # Prevent creating tree elements with None SHA for file additions",
          "        if patch_type == \"A\" and blob is None:",
          "            return None",
          "",
          "        # 100644 is the git code for creating a Regular non-executable file",
          "        # https://stackoverflow.com/questions/737673/how-to-read-the-mode-field-of-git-ls-trees-output",
          "        return InputGitTreeElement(",
          "            path=path, mode=\"100644\", type=\"blob\", sha=blob.sha if blob else None",
          "        )",
          "",
          "    def get_branch_ref(self, branch_name: str) -> GitRef | None:",
          "        try:",
          "            return self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        except GithubException as e:",
          "            if e.status == 404:",
          "                return None",
          "            raise e",
          "",
          "    def create_branch_from_changes(",
          "        self,",
          "        *,",
          "        pr_title: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "        branch_name: str | None = None,",
          "        from_base_sha: bool = False,",
          "    ) -> GitRef | None:",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Either file_patches or file_changes must be provided\")",
          "",
          "        new_branch_name = sanitize_branch_name(branch_name or pr_title)",
          "",
          "        try:",
          "            branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "        except GithubException as e:",
          "            # only use the random suffix if the branch already exists",
          "            if e.status == 409 or e.status == 422:",
          "                new_branch_name = f\"{new_branch_name}-{generate_random_string(n=6)}\"",
          "                branch_ref = self._create_branch(new_branch_name, from_base_sha)",
          "            else:",
          "                raise e",
          "",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, patch=patch",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file patch: {e}\")",
          "",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                try:",
          "                    element = self.process_one_file_for_git_commit(",
          "                        branch_ref=branch_ref.ref, change=change",
          "                    )",
          "                    if element:",
          "                        tree_elements.append(element)",
          "                except Exception as e:",
          "                    logger.exception(f\"Error processing file change: {e}\")",
          "        # latest commit is the head of new branch",
          "        latest_commit = self.repo.get_git_commit(self.get_branch_head_sha(new_branch_name))",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "",
          "        new_commit = self.repo.create_git_commit(",
          "            message=pr_title, tree=new_tree, parents=[latest_commit]",
          "        )",
          "",
          "        branch_ref.edit(sha=new_commit.sha)",
          "",
          "        # Check that the changes were made",
          "        comparison = self.repo.compare(",
          "            self.get_branch_head_sha(self.base_branch), branch_ref.object.sha",
          "        )",
          "",
          "        if comparison.ahead_by < 1:",
          "            # Remove the branch if there are no changes",
          "            try:",
          "                branch_ref.delete()",
          "            except UnknownObjectException:",
          "                logger.warning(\"Attempted to delete a branch or reference that does not exist.\")",
          "            sentry_sdk.capture_message(",
          "                f\"Failed to create branch from changes. Comparison is ahead by {comparison.ahead_by}\"",
          "            )",
          "            return None",
          "",
          "        return branch_ref",
          "",
          "    def create_pr_from_branch(",
          "        self,",
          "        branch: GitRef,",
          "        title: str,",
          "        description: str,",
          "        provided_base: str | None = None,",
          "    ) -> PullRequest:",
          "        pulls = self.repo.get_pulls(state=\"open\", head=f\"{self.repo_owner}:{branch.ref}\")",
          "",
          "        if pulls.totalCount > 0:",
          "            logger.error(",
          "                f\"Branch {branch.ref} already has an open PR.\",",
          "                extra={",
          "                    \"branch_ref\": branch.ref,",
          "                    \"title\": title,",
          "                    \"description\": description,",
          "                    \"provided_base\": provided_base,",
          "                },",
          "            )",
          "",
          "            return pulls[0]",
          "",
          "        try:",
          "            return self.repo.create_pull(",
          "                title=title,",
          "                body=description,",
          "                base=provided_base or self.base_branch or self.get_default_branch(),",
          "                head=branch.ref,",
          "                draft=True,",
          "            )",
          "        except GithubException as e:",
          "            if e.status == 422 and \"Draft pull requests are not supported\" in str(e):",
          "                # fallback to creating a regular PR if draft PR is not supported",
          "                return self.repo.create_pull(",
          "                    title=title,",
          "                    body=description,",
          "                    base=provided_base or self.base_branch or self.get_default_branch(),",
          "                    head=branch.ref,",
          "                    draft=False,",
          "                )",
          "            else:",
          "                logger.exception(\"Error creating PR\")",
          "                raise e",
          "",
          "    def get_index_file_set(",
          "        self, sha: str | None = None, max_file_size_bytes=2 * 1024 * 1024, skip_empty_files=False",
          "    ) -> set[str]:",
          "        if sha is None:",
          "            sha = self.base_commit_sha",
          "",
          "        tree = self.get_git_tree(sha)",
          "        file_set = set()",
          "        for file in tree.tree:",
          "            if (",
          "                file.type == \"blob\"",
          "                and file.size < max_file_size_bytes",
          "                and file.mode",
          "                in [\"100644\", \"100755\"]  # 100644 is a regular file, 100755 is an executable file",
          "                and get_language_from_path(file.path) is not None",
          "                and (not skip_empty_files or file.size > 0)",
          "            ):",
          "                file_set.add(file.path)",
          "",
          "        return file_set",
          "",
          "    def get_pr_diff_content(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"diff\"))",
          "",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.text",
          "",
          "    def _get_auth_headers(self, accept_type: Literal[\"json\", \"diff\"] = \"json\"):",
          "        requester = self.repo._requester",
          "        if requester.auth is None:",
          "            raise Exception(\"No auth token found for GitHub API\")",
          "        headers = {",
          "            \"Accept\": (",
          "                \"application/vnd.github.diff\"",
          "                if accept_type == \"diff\"",
          "                else \"application/vnd.github+json\"",
          "            ),",
          "            \"Authorization\": f\"Bearer {requester.auth.token}\",",
          "            \"X-GitHub-Api-Version\": \"2022-11-28\",",
          "        }",
          "        return headers",
          "",
          "    def comment_root_cause_on_pr_for_copilot(",
          "        self, pr_url: str, run_id: int, issue_id: int, comment: str",
          "    ):",
          "        pull_id = int(pr_url.split(\"/\")[-1])",
          "        repo_name = pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "        params = {",
          "            \"body\": comment,",
          "            \"actions\": [",
          "                {",
          "                    \"name\": \"Fix with Sentry\",",
          "                    \"type\": \"copilot-chat\",",
          "                    \"prompt\": f\"@sentry find a fix for issue {issue_id} with run ID {run_id}\",",
          "                }",
          "            ],",
          "        }",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def comment_pr_generated_for_copilot(",
          "        self, pr_to_comment_on_url: str, new_pr_url: str, run_id: int",
          "    ):",
          "        pull_id = int(pr_to_comment_on_url.split(\"/\")[-1])",
          "        repo_name = pr_to_comment_on_url.split(\"github.com/\")[1].split(\"/pull\")[",
          "            0",
          "        ]  # should be \"owner/repo\"",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{pull_id}/comments\"",
          "",
          "        comment = f\"A fix has been generated and is available [here]({new_pr_url}) for your review. Autofix Run ID: {run_id}\"",
          "",
          "        params = {\"body\": comment}",
          "",
          "        headers = self._get_auth_headers()",
          "",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "",
          "    def get_pr_head_sha(self, pr_url: str) -> str:",
          "        data = requests.get(pr_url, headers=self._get_auth_headers(accept_type=\"json\"))",
          "        data.raise_for_status()  # Raise an exception for HTTP errors",
          "        return data.json()[\"head\"][\"sha\"]",
          "",
          "    def post_unit_test_reference_to_original_pr(self, original_pr_url: str, unit_test_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Sentry has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_reference_to_original_pr_codecov_app(",
          "        self, original_pr_url: str, unit_test_pr_url: str",
          "    ):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = f\"Codecov has generated a new [PR]({unit_test_pr_url}) with unit tests for this PR. View the new PR({unit_test_pr_url}) to review the changes.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_unit_test_not_generated_message_to_original_pr(self, original_pr_url: str):",
          "        original_pr_id = int(original_pr_url.split(\"/\")[-1])",
          "        repo_name = original_pr_url.split(\"github.com/\")[1].split(\"/pull\")[0]",
          "        url = f\"https://api.github.com/repos/{repo_name}/issues/{original_pr_id}/comments\"",
          "        comment = \"Sentry has determined that unit tests are not necessary for this PR.\"",
          "        params = {\"body\": comment}",
          "        headers = self._get_auth_headers()",
          "        response = requests.post(url, headers=headers, json=params)",
          "        response.raise_for_status()",
          "        return response.json()[\"html_url\"]",
          "",
          "    def post_issue_comment(self, pr_url: str, comment: str):",
          "        \"\"\"",
          "        Create an issue comment on a GitHub issue (all pull requests are issues).",
          "        This can be used to create an overall PR comment instead of associated with a specific line.",
          "        See https://docs.github.com/en/rest/issues/comments?apiVersion=2022-11-28#create-an-issue-comment",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        issue = self.repo.get_issue(number=pr_id)",
          "        comment_obj = issue.create_comment(body=comment)",
          "        return comment_obj.html_url",
          "",
          "    def post_pr_review_comment(self, pr_url: str, comment: GithubPrReviewComment):",
          "        \"\"\"",
          "        Create a review comment on a GitHub pull request.",
          "        See https://docs.github.com/en/rest/pulls/comments?apiVersion=2022-11-28#create-a-review-comment-for-a-pull-request",
          "        Note that expected input is pr_url NOT pr_html_url",
          "        \"\"\"",
          "        pr_id = self._extract_id_from_pr_url(pr_url)",
          "        pr = self.repo.get_pull(number=pr_id)",
          "        commit = self.repo.get_commit(comment[\"commit_id\"])",
          "",
          "        review_comment = pr.create_review_comment(",
          "            body=comment[\"body\"],",
          "            commit=commit,",
          "            path=comment[\"path\"],",
          "            line=comment.get(\"line\", GithubObject.NotSet),",
          "            side=comment.get(\"side\", GithubObject.NotSet),",
          "            start_line=comment.get(\"start_line\", GithubObject.NotSet),",
          "        )",
          "        return review_comment.html_url",
          "",
          "    def push_new_commit_to_pr(",
          "        self,",
          "        pr,",
          "        commit_message: str,",
          "        file_patches: list[FilePatch] | None = None,",
          "        file_changes: list[FileChange] | None = None,",
          "    ):",
          "        if not file_patches and not file_changes:",
          "            raise ValueError(\"Must provide file_patches or file_changes\")",
          "        branch_name = pr.head.ref",
          "        tree_elements = []",
          "        if file_patches:",
          "            for patch in file_patches:",
          "                element = self.process_one_file_for_git_commit(branch_ref=branch_name, patch=patch)",
          "                if element:",
          "                    tree_elements.append(element)",
          "        elif file_changes:",
          "            for change in file_changes:",
          "                element = self.process_one_file_for_git_commit(",
          "                    branch_ref=branch_name, change=change",
          "                )",
          "                if element:",
          "                    tree_elements.append(element)",
          "        if not tree_elements:",
          "            logger.warning(\"No valid changes to commit\")",
          "            return None",
          "        latest_sha = self.get_branch_head_sha(branch_name)",
          "        latest_commit = self.repo.get_git_commit(latest_sha)",
          "        base_tree = latest_commit.tree",
          "        new_tree = self.repo.create_git_tree(tree_elements, base_tree)",
          "        new_commit = self.repo.create_git_commit(",
          "            message=commit_message, tree=new_tree, parents=[latest_commit]",
          "        )",
          "        branch_ref = self.repo.get_git_ref(f\"heads/{branch_name}\")",
          "        branch_ref.edit(sha=new_commit.sha)",
          "        return new_commit",
          ""
        ]
      }
    },
    {
      "file": "github/Repository.py",
      "image": "github.Repository",
      "is_application": false,
      "line": 2003,
      "name": "Repository.get_branch",
      "path": "/usr/local/lib/python3.11/dist-packages/github/Repository.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 119,
      "name": "FilterWarningsComponent.invoke",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 119,
        "name": "FilterWarningsComponent.invoke",
        "code": "        return None\n\n    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")\n    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")\n    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:\n        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)\n        warning_and_pr_files: list[WarningAndPrFile] = []\n        for warning in request.warnings:\n            try:\n                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)\n            except Exception:\n                self.logger.exception(\n                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}\n                )\n            else:\n                if warning_and_pr_file is not None:\n                    warning_and_pr_files.append(warning_and_pr_file)\n        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)\n\n",
        "lineRange": {
          "start": 110,
          "end": 129
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 105,
      "name": "FilterWarningsComponent._find_matching_pr_file",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 105,
        "name": "FilterWarningsComponent._find_matching_pr_file",
        "code": "            filepath_to_pr_file[filepath]\n            for filepath in warning_filepath_variations & set(filepath_to_pr_file)\n        ]\n\n    def _find_matching_pr_file(\n        self,\n        warning: StaticAnalysisWarning,\n        filepath_to_pr_file: dict[str, PrFile],\n    ) -> WarningAndPrFile | None:\n        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)\n        for pr_file in matching_pr_files:\n            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)\n            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch\n                return warning_and_pr_file\n        return None\n\n    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")\n    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")\n    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:\n        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
        "lineRange": {
          "start": 96,
          "end": 115
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 93,
      "name": "FilterWarningsComponent._matching_pr_files",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 93,
        "name": "FilterWarningsComponent._matching_pr_files",
        "code": "        )\n        warning_path = Path(*warning_path.parts[first_idx_non_dots:])\n        if \"..\" in warning_path.parts:\n            raise ValueError(\n                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"\n            )\n\n        warning_filepath_variations = {\n            warning_path.as_posix(),\n            *left_truncated_paths(warning_path, max_num_paths=2),\n        }\n        return [\n            filepath_to_pr_file[filepath]\n            for filepath in warning_filepath_variations & set(filepath_to_pr_file)\n        ]\n\n    def _find_matching_pr_file(\n        self,\n        warning: StaticAnalysisWarning,\n        filepath_to_pr_file: dict[str, PrFile],",
        "lineRange": {
          "start": 84,
          "end": 103
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codebase/utils.py",
      "image": "seer.automation.codebase.utils",
      "is_application": true,
      "line": 263,
      "name": "left_truncated_paths",
      "path": "/app/src/seer/automation/codebase/utils.py",
      "codeContext": {
        "file": "seer/automation/codebase/utils.py",
        "line": 263,
        "name": "left_truncated_paths",
        "code": "        ]\n    \"\"\"\n    parts = list(path.parts)\n    num_dirs = len(parts) - 1  # -1 for the filename\n    num_paths = min(max_num_paths, num_dirs)\n\n    result = []\n    for _ in range(num_paths):\n        parts.pop(0)\n        result.append(Path(*parts).as_posix())\n    return result\n",
        "lineRange": {
          "start": 254,
          "end": 265
        },
        "lines": [
          "import functools",
          "import logging",
          "import os",
          "import shutil",
          "from pathlib import Path",
          "",
          "from seer.automation.codebase.models import Document",
          "from seer.automation.models import StacktraceFrame, right_justified",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "language_to_extensions = {",
          "    \"apl\": [\".apl\"],",
          "    \"apex\": [\".cls\", \".apex\"],",
          "    \"assembly\": [\".asm\", \".s\"],",
          "    \"astro\": [\".astro\"],",
          "    \"bash\": [\".sh\"],",
          "    \"bicep\": [\".bicep\"],",
          "    \"c\": [\".c\"],",
          "    \"c_sharp\": [\".cs\"],",
          "    \"clojure\": [\".clj\", \".cljs\"],",
          "    \"coffeescript\": [\".coffee\"],",
          "    \"commonlisp\": [\".lisp\"],",
          "    \"cpp\": [\".cpp\", \".cxx\", \".cc\", \".hpp\"],",
          "    \"crystal\": [\".cr\"],",
          "    \"css\": [\".css\", \".scss\", \".sass\"],",
          "    \"cuda\": [\".cu\", \".cuh\"],",
          "    \"dart\": [\".dart\"],",
          "    \"dockerfile\": [\"Dockerfile\"],",
          "    \"dot\": [\".dot\"],",
          "    \"elisp\": [\".el\"],",
          "    \"elixir\": [\".ex\", \".exs\"],",
          "    \"elm\": [\".elm\"],",
          "    \"embedded_template\": [\".tmpl\", \".tpl\"],",
          "    \"erlang\": [\".erl\", \".hrl\"],",
          "    \"fixed_form_fortran\": [\".f\", \".for\", \".f77\"],",
          "    \"fortran\": [\".f90\", \".f95\", \".f03\", \".f08\"],",
          "    \"fsharp\": [\".fs\", \".fsx\"],",
          "    \"gdscript\": [\".gd\", \".gdshader\"],",
          "    \"gleam\": [\".gleam\"],",
          "    \"glsl\": [\".glsl\", \".frag\", \".vert\"],",
          "    \"go\": [\".go\"],",
          "    \"gradle\": [\".gradle\"],",
          "    \"graphql\": [\".graphql\", \".gql\"],",
          "    \"groovy\": [\".groovy\", \".gvy\"],",
          "    \"hack\": [\".hack\"],",
          "    \"handlebars\": [\".hbs\", \".handlebars\"],",
          "    \"haskell\": [\".hs\", \".lhs\"],",
          "    \"hcl\": [\".hcl\"],",
          "    \"hlsl\": [\".hlsl\"],",
          "    \"html\": [\".html\", \".htm\"],",
          "    \"ignore\": [\".gitignore\", \".npmignore\"],",
          "    \"java\": [\".java\"],",
          "    \"javascript\": [\".js\", \".jsx\"],",
          "    \"jest\": [\".jest.js\", \".spec.js\", \".test.js\"],",
          "    \"jsdoc\": [\".jsdoc\"],",
          "    \"json\": [\".json\"],",
          "    \"julia\": [\".jl\"],",
          "    \"jupyter\": [\".ipynb\"],",
          "    \"kotlin\": [\".kt\", \".kts\"],",
          "    \"less\": [\".less\"],",
          "    \"lua\": [\".lua\"],",
          "    \"make\": [\"Makefile\"],",
          "    \"markdown\": [\".md\", \".markdown\"],",
          "    \"matlab\": [\".m\", \".mat\"],",
          "    \"mojo\": [\".🔥\", \".mojo\"],",
          "    \"nim\": [\".nim\"],",
          "    \"nix\": [\".nix\"],",
          "    \"objc\": [\".m\", \".h\", \".mm\"],",
          "    \"ocaml\": [\".ml\", \".mli\"],",
          "    \"pascal\": [\".pas\", \".pp\"],",
          "    \"perl\": [\".pl\", \".pm\"],",
          "    \"php\": [\".php\", \".phtml\", \".php3\", \".php4\", \".php5\", \".php7\", \".phps\", \".php-s\"],",
          "    \"powershell\": [\".ps1\", \".psm1\"],",
          "    \"proto\": [\".proto\"],",
          "    \"pug\": [\".pug\", \".jade\"],",
          "    \"puppet\": [\".pp\"],",
          "    \"purescript\": [\".purs\"],",
          "    \"python\": [\".py\"],",
          "    \"ql\": [\".ql\"],",
          "    \"r\": [\".r\", \".R\"],",
          "    \"racket\": [\".rkt\"],",
          "    \"raku\": [\".raku\", \".rakumod\"],",
          "    \"regex\": [\".re\"],",
          "    \"rescript\": [\".res\", \".resi\"],",
          "    \"rst\": [\".rst\"],",
          "    \"ruby\": [\".rb\"],",
          "    \"rust\": [\".rs\"],",
          "    \"scala\": [\".scala\", \".sc\"],",
          "    \"shell\": [\".sh\", \".bash\", \".zsh\"],",
          "    \"smali\": [\".smali\"],",
          "    \"solidity\": [\".sol\"],",
          "    \"sql\": [\".sql\"],",
          "    \"sqlite\": [\".sqlite\"],",
          "    \"stylus\": [\".styl\"],",
          "    \"svelte\": [\".svelte\"],",
          "    \"swift\": [\".swift\"],",
          "    \"terraform\": [\".tf\", \".tfvars\"],",
          "    \"toml\": [\".toml\"],",
          "    \"tsq\": [\".tsq\"],",
          "    \"tsx\": [\".tsx\"],",
          "    \"twig\": [\".twig\"],",
          "    \"typescript\": [\".ts\"],",
          "    \"typescriptdef\": [\".d.ts\"],",
          "    \"v\": [\".v\"],",
          "    \"verilog\": [\".v\", \".vh\"],",
          "    \"vhdl\": [\".vhd\", \".vhdl\"],",
          "    \"vue\": [\".vue\"],",
          "    \"wasm\": [\".wat\", \".wasm\"],",
          "    \"wgsl\": [\".wgsl\"],",
          "    \"xml\": [\".xml\"],",
          "    \"yaml\": [\".yaml\", \".yml\"],",
          "    \"zig\": [\".zig\"],",
          "}",
          "",
          "",
          "@functools.cache",
          "def get_all_supported_extensions() -> set[str]:",
          "    \"\"\"",
          "    Returns a set of all supported file extensions across all languages.",
          "",
          "    :return: A set of file extensions including the dot prefix (e.g. {'.py', '.js'})",
          "    \"\"\"",
          "    extensions = set()",
          "    for extensions_list in language_to_extensions.values():",
          "        extensions.update(extensions_list)",
          "    return extensions",
          "",
          "",
          "@functools.cache",
          "def get_extension_to_language_map():",
          "    extension_to_language: dict[str, str] = {}",
          "    for language, extensions in language_to_extensions.items():",
          "        for extension in extensions:",
          "            extension_to_language[extension] = language",
          "    return extension_to_language",
          "",
          "",
          "def get_language_from_path(path: str) -> str | None:",
          "    extension = os.path.splitext(path)[1]",
          "    return get_extension_to_language_map().get(extension, None)",
          "",
          "",
          "def read_specific_files(repo_path: str, files: list[str] | set[str]) -> list[Document]:",
          "    \"\"\"",
          "    Reads the contents of specific files and returns a list of Document objects.",
          "",
          "    :param files: A list of file paths to read.",
          "    :return: A list of Document objects representing the file contents.",
          "    \"\"\"",
          "    documents = []",
          "    for file in files:",
          "        file_path = os.path.join(repo_path, file)",
          "",
          "        language = get_language_from_path(file_path)",
          "",
          "        # TODO: Support languages that are out of this list in the near future by simply using dumb chunking.",
          "        if not language:",
          "            continue",
          "",
          "        try:",
          "            with open(file_path, \"r\", encoding=\"utf-8\") as f:",
          "                text = f.read()",
          "        except UnicodeDecodeError:",
          "            logger.warning(f\"Unicode decode error: {file_path}\")",
          "            continue",
          "        except FileNotFoundError:",
          "            logger.warning(f\"File not found: {file_path}\")",
          "            documents.append(Document(path=file, text=\"\", language=language))",
          "            continue",
          "",
          "        documents.append(Document(path=file, text=text, language=language))",
          "    return documents",
          "",
          "",
          "def cleanup_dir(directory: str):",
          "    if os.path.exists(directory):",
          "        shutil.rmtree(directory)",
          "        logger.info(f\"Cleaned up directory: {directory}\")",
          "    else:",
          "        logger.info(f\"Directory {directory} already cleaned!\")",
          "",
          "",
          "def potential_frame_match(src_file: str, frame: StacktraceFrame) -> bool:",
          "    \"\"\"Determine if the frame filename represents a source code file.\"\"\"",
          "    match = False",
          "",
          "    src_split = src_file.split(\"/\")[::-1]",
          "",
          "    filename = frame.filename or frame.package",
          "    if filename:",
          "        # Remove leading './' or '.' from filename",
          "        filename = filename.lstrip(\"./\")",
          "        frame_split = filename.split(\"/\")[::-1]",
          "",
          "        if len(src_split) > 0 and len(frame_split) > 0 and len(src_split) >= len(frame_split):",
          "            for i in range(len(frame_split)):",
          "                if src_split[i] == frame_split[i]:",
          "                    match = True",
          "                else:",
          "                    match = False",
          "                    break",
          "",
          "    return match",
          "",
          "",
          "def group_documents_by_language(documents: list[Document]) -> dict[str, list[Document]]:",
          "    file_type_count: dict[str, list[Document]] = {}",
          "    for doc in documents:",
          "        file_type = doc.language",
          "        if file_type not in file_type_count:",
          "            file_type_count[file_type] = []",
          "        file_type_count[file_type].append(doc)",
          "",
          "    return file_type_count",
          "",
          "",
          "def code_snippet(",
          "    lines: list[str],",
          "    start_line: int,",
          "    end_line: int,",
          "    padding_size: int = 0,",
          "    start_line_override: int | None = None,",
          ") -> list[str]:",
          "    \"\"\"",
          "    `start_line` and `end_line` are assumed to be 1-indexed. `end_line` is inclusive.",
          "",
          "    Pass `start_line_override` to override the line numbers as shown in the snippet.",
          "    \"\"\"",
          "    if (start_line <= 0) or (end_line <= 0):",
          "        raise ValueError(\"start_line and end_line must be greater than 0. They're 1-indexed.\")",
          "",
          "    start_idx = start_line - 1",
          "    end_idx = end_line - 1",
          "    start_snippet = max(0, start_idx - padding_size)",
          "    end_snippet = min(end_idx + padding_size + 1, len(lines))",
          "    lines_snippet = lines[start_snippet:end_snippet]",
          "",
          "    start_line = start_line_override or start_snippet + 1",
          "    end_line = start_line + len(lines_snippet) - 1",
          "    line_numbers = right_justified(start_line, end_line)",
          "    return [f\"{line_number}| {line}\" for line_number, line in zip(line_numbers, lines_snippet)]",
          "",
          "",
          "def left_truncated_paths(path: Path, max_num_paths: int = 2) -> list[str]:",
          "    \"\"\"",
          "    Example::",
          "",
          "        path = Path(\"src/seer/automation/agent/client.py\")",
          "        paths = left_truncated_paths(path, 2)",
          "        assert paths == [",
          "            \"seer/automation/agent/client.py\",",
          "            \"automation/agent/client.py\",",
          "        ]",
          "    \"\"\"",
          "    parts = list(path.parts)",
          "    num_dirs = len(parts) - 1  # -1 for the filename",
          "    num_paths = min(max_num_paths, num_dirs)",
          "",
          "    result = []",
          "    for _ in range(num_paths):",
          "        parts.pop(0)",
          "        result.append(Path(*parts).as_posix())",
          "    return result",
          ""
        ]
      }
    },
    {
      "file": "pathlib.py",
      "image": "pathlib",
      "is_application": false,
      "line": 872,
      "name": "Path.__new__",
      "path": "/usr/lib/python3.11/pathlib.py"
    },
    {
      "file": "pathlib.py",
      "image": "pathlib",
      "is_application": false,
      "line": 510,
      "name": "PurePath._from_parts",
      "path": "/usr/lib/python3.11/pathlib.py"
    },
    {
      "file": "pathlib.py",
      "image": "pathlib",
      "is_application": false,
      "line": 503,
      "name": "PurePath._parse_args",
      "path": "/usr/lib/python3.11/pathlib.py"
    },
    {
      "file": "pathlib.py",
      "image": "pathlib",
      "is_application": false,
      "line": 73,
      "name": "_Flavour.parse_parts",
      "path": "/usr/lib/python3.11/pathlib.py"
    },
    {
      "file": "pathlib.py",
      "image": "pathlib",
      "is_application": false,
      "line": 554,
      "name": "PurePath.as_posix",
      "path": "/usr/lib/python3.11/pathlib.py"
    },
    {
      "file": "pathlib.py",
      "image": "pathlib",
      "is_application": false,
      "line": 543,
      "name": "PurePath.__str__",
      "path": "/usr/lib/python3.11/pathlib.py"
    },
    {
      "file": "pathlib.py",
      "image": "pathlib",
      "is_application": false,
      "line": 529,
      "name": "PurePath._format_parsed_parts",
      "path": "/usr/lib/python3.11/pathlib.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 216,
      "name": "FetchIssuesComponent.invoke",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 216,
        "name": "FetchIssuesComponent.invoke",
        "code": "\n    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")\n    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")\n    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:\n        if self.context.repo.provider_raw is None:\n            raise TypeError(\n                f\"provider_raw is not set for repo: {self.context.repo}. \"\n                \"Something went wrong during initialization of the RepoDefinition.\"\n            )\n        filename_to_issues = self._fetch_issues(\n            organization_id=request.organization_id,\n            provider=self.context.repo.provider_raw,\n            external_id=self.context.repo.external_id,\n            pr_files=request.pr_files,\n        )\n        for filename, issues in filename_to_issues.items():\n            self.logger.info(\n                f\"Found {len(issues)} issues for file {filename}\",\n                extra={\"issue_ids\": [issue.id for issue in issues]},\n            )",
        "lineRange": {
          "start": 207,
          "end": 226
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 196,
      "name": "FetchIssuesComponent._fetch_issues",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 196,
        "name": "FetchIssuesComponent._fetch_issues",
        "code": "        pr_files_eligible = [\n            pr_file\n            for pr_file in pr_files\n            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed\n        ]\n        if not pr_files_eligible:\n            self.logger.info(\"No eligible files in PR.\")\n            return {}\n\n        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")\n        filename_to_issues = {\n            pr_file.filename: _fetch_issues_for_pr_file(\n                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger\n            )\n            for pr_file in pr_files_eligible[:max_files_analyzed]\n        }\n        return {\n            filename: [IssueDetails.model_validate(issue) for issue in issues]\n            for filename, issues in filename_to_issues.items()\n        }",
        "lineRange": {
          "start": 187,
          "end": 206
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/component.py",
      "image": "seer.automation.component",
      "is_application": true,
      "line": 43,
      "name": "BaseComponent.logger",
      "path": "/app/src/seer/automation/component.py",
      "codeContext": {
        "file": "seer/automation/component.py",
        "line": 43,
        "name": "BaseComponent.logger",
        "code": "    def __init__(self, context: PipelineContext):\n        self.context = context\n\n    @abc.abstractmethod\n    def invoke(self, request: BCR) -> BCO | None:\n        pass\n\n    @cached_property\n    def logger(self):\n        run_id = self.context.run_id\n        name = f\"{type(self).__module__}.{type(self).__qualname__}\"\n        prefix = f\"[{run_id=}] [{name}] \"\n        return prefix_logger(prefix, logger)\n",
        "lineRange": {
          "start": 34,
          "end": 47
        },
        "lines": [
          "import abc",
          "import logging",
          "from functools import cached_property",
          "from typing import Generic, TypeVar",
          "",
          "from pydantic import BaseModel",
          "",
          "from seer.automation.models import PromptXmlModel",
          "from seer.automation.pipeline import PipelineContext",
          "from seer.utils import prefix_logger",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "class BaseComponentRequest(BaseModel):",
          "    pass",
          "",
          "",
          "class BaseComponentXmlOutput(PromptXmlModel):",
          "    pass",
          "",
          "",
          "class BaseComponentOutput(BaseModel):",
          "    pass",
          "",
          "",
          "BCR = TypeVar(\"BCR\", bound=BaseComponentRequest)",
          "BCO = TypeVar(\"BCO\", bound=BaseComponentOutput | BaseComponentXmlOutput)",
          "",
          "",
          "class BaseComponent(abc.ABC, Generic[BCR, BCO]):",
          "    context: PipelineContext",
          "",
          "    def __init__(self, context: PipelineContext):",
          "        self.context = context",
          "",
          "    @abc.abstractmethod",
          "    def invoke(self, request: BCR) -> BCO | None:",
          "        pass",
          "",
          "    @cached_property",
          "    def logger(self):",
          "        run_id = self.context.run_id",
          "        name = f\"{type(self).__module__}.{type(self).__qualname__}\"",
          "        prefix = f\"[{run_id=}] [{name}] \"",
          "        return prefix_logger(prefix, logger)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 198,
      "name": "FetchIssuesComponent._fetch_issues.<locals>.<dictcomp>",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 198,
        "name": "FetchIssuesComponent._fetch_issues.<locals>.<dictcomp>",
        "code": "            for pr_file in pr_files\n            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed\n        ]\n        if not pr_files_eligible:\n            self.logger.info(\"No eligible files in PR.\")\n            return {}\n\n        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")\n        filename_to_issues = {\n            pr_file.filename: _fetch_issues_for_pr_file(\n                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger\n            )\n            for pr_file in pr_files_eligible[:max_files_analyzed]\n        }\n        return {\n            filename: [IssueDetails.model_validate(issue) for issue in issues]\n            for filename, issues in filename_to_issues.items()\n        }\n\n    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
        "lineRange": {
          "start": 189,
          "end": 208
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 140,
      "name": "_fetch_issues_for_pr_file",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 140,
        "name": "_fetch_issues_for_pr_file",
        "code": "def _fetch_issues_for_pr_file(\n    organization_id: int,\n    provider: str,\n    external_id: str,\n    pr_file: PrFile,\n    run_id: int,\n    logger: logging.Logger,\n    client: RpcClient = injected,\n) -> list[dict[str, Any]]:\n    pr_filename_to_issues = client.call(\n        \"get_issues_related_to_file_patches\",\n        organization_id=organization_id,\n        provider=provider,\n        external_id=external_id,\n        pr_files=[pr_file.model_dump()],\n        run_id=run_id,\n    )\n    if pr_filename_to_issues is None:\n        logger.exception(\n            \"Something went wrong with the issue-fetching RPC call\",",
        "lineRange": {
          "start": 131,
          "end": 150
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/rpc.py",
      "image": "seer.rpc",
      "is_application": true,
      "line": 121,
      "name": "SentryRpcClient.call",
      "path": "/app/src/seer/rpc.py",
      "codeContext": {
        "file": "seer/rpc.py",
        "line": 121,
        "name": "SentryRpcClient.call",
        "code": "    def _generate_request_signature(self, url_path: str, body: bytes) -> str:\n        signature_input = b\"%s:%s\" % (url_path.encode(\"utf8\"), body)\n        signature = hmac.new(\n            self.shared_secret.encode(\"utf-8\"), signature_input, hashlib.sha256\n        ).hexdigest()\n        return f\"rpc0:{signature}\"\n\n    def call(self, method: str, **kwargs) -> dict[str, Any] | None:\n        body_bytes, endpoint, headers = self._prepare_request(method, kwargs)\n        response = requests.post(endpoint, headers=headers, data=body_bytes)\n        response.raise_for_status()\n        if response.headers.get(\"Content-Type\", \"\") != \"application/json\":\n            logger.warning(\"No application/json content type\")\n            return None\n        return response.json()\n\n    def _prepare_request(self, method, kwargs):\n        url_path = f\"/api/0/internal/seer-rpc/{method}/\"\n        endpoint = f\"{self.base_url}{url_path}\"\n        body_dict = {\"args\": kwargs}",
        "lineRange": {
          "start": 112,
          "end": 131
        },
        "lines": [
          "import dataclasses",
          "import hashlib",
          "import hmac",
          "import logging",
          "import os",
          "from abc import ABC, abstractmethod",
          "from functools import cached_property",
          "from typing import Any",
          "",
          "import requests",
          "from requests import HTTPError",
          "",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import Module, injected",
          "from seer.utils import json_dumps",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "module = Module()",
          "module.enable()",
          "",
          "rpc_stub_module = Module()",
          "",
          "",
          "class RpcClient(ABC):",
          "    @abstractmethod",
          "    def call(self, method: str, **kwargs) -> dict[str, Any] | None:",
          "        pass",
          "",
          "",
          "@dataclasses.dataclass",
          "class FakeHttpResponse:",
          "    status_code: int",
          "    content: bytes",
          "",
          "    @property",
          "    def text(self) -> str:",
          "        return self.content.decode(\"utf-8\")",
          "",
          "",
          "@dataclasses.dataclass",
          "class DummyRpcClient(RpcClient):",
          "    \"\"\"",
          "    A mock RPCClient that forces (method, **kwargs) to search for method named \"method\" and",
          "    invoke that for a response, or uses a default implementation that logs the interaction",
          "    without providing a definitive response.",
          "",
          "    Subclass this client to add unique behavior or add to it to control default dummy behavior.",
          "    \"\"\"",
          "",
          "    should_log: bool = False",
          "    dry_run: bool = False",
          "    invocations: list[tuple[str, dict[str, Any]]] = dataclasses.field(default_factory=list)",
          "",
          "    # use to force a specific consent value for get_organization_autofix_consent",
          "    should_consent: bool = False",
          "",
          "    # Use to force all requests to fail with a given http status code",
          "    force_failure_status: int | None = None",
          "",
          "    def call(self, method: str, **kwargs) -> dict[str, Any] | None:",
          "        logger.warn(f\"Call to Sentry autofix API {method} handled by DummyRpcClient\")",
          "        self.invocations.append((method, kwargs))",
          "",
          "        if self.force_failure_status:",
          "            raise HTTPError(",
          "                response=FakeHttpResponse(status_code=self.force_failure_status, content=b\"\")",
          "            )",
          "",
          "        result = getattr(self, method, self._default_call)(method, kwargs)",
          "        if result is None:",
          "            return None",
          "        if isinstance(result, dict):",
          "            return result",
          "        status, msg = result",
          "        raise HTTPError(response=FakeHttpResponse(status_code=status, content=msg.encode(\"utf-8\")))",
          "",
          "    def get_organization_autofix_consent(",
          "        self, method: str, kwargs: dict[str, Any]",
          "    ) -> dict[str, Any]:",
          "        return {\"consent\": self.should_consent}",
          "",
          "    def _default_call(",
          "        self, method: str, kwargs: dict[str, Any]",
          "    ) -> dict[str, Any] | tuple[int, str] | None:",
          "        if self.dry_run:",
          "            return None",
          "",
          "        body_dict = {\"args\": kwargs}",
          "        json_dump = json_dumps(body_dict, separators=(\",\", \":\"))",
          "",
          "        if self.should_log:",
          "            print(f\"Calling {method} with {json_dump}\")",
          "        return 404, \"Not Found\"",
          "",
          "",
          "class SentryRpcClient(RpcClient):",
          "    @cached_property",
          "    def shared_secret(self) -> str:",
          "        shared_secret = os.environ.get(\"RPC_SHARED_SECRET\")",
          "        if not shared_secret:",
          "            raise RuntimeError(\"RPC_SHARED_SECRET must be set\")",
          "        return shared_secret",
          "",
          "    @cached_property",
          "    def base_url(self) -> str:",
          "        base_url = os.environ.get(\"SENTRY_BASE_URL\")",
          "        if not base_url:",
          "            raise RuntimeError(\"SENTRY_BASE_URL must be set\")",
          "        return base_url",
          "",
          "    def _generate_request_signature(self, url_path: str, body: bytes) -> str:",
          "        signature_input = b\"%s:%s\" % (url_path.encode(\"utf8\"), body)",
          "        signature = hmac.new(",
          "            self.shared_secret.encode(\"utf-8\"), signature_input, hashlib.sha256",
          "        ).hexdigest()",
          "        return f\"rpc0:{signature}\"",
          "",
          "    def call(self, method: str, **kwargs) -> dict[str, Any] | None:",
          "        body_bytes, endpoint, headers = self._prepare_request(method, kwargs)",
          "        response = requests.post(endpoint, headers=headers, data=body_bytes)",
          "        response.raise_for_status()",
          "        if response.headers.get(\"Content-Type\", \"\") != \"application/json\":",
          "            logger.warning(\"No application/json content type\")",
          "            return None",
          "        return response.json()",
          "",
          "    def _prepare_request(self, method, kwargs):",
          "        url_path = f\"/api/0/internal/seer-rpc/{method}/\"",
          "        endpoint = f\"{self.base_url}{url_path}\"",
          "        body_dict = {\"args\": kwargs}",
          "        body = json_dumps(body_dict, separators=(\",\", \":\"))",
          "        body_bytes = body.encode(\"utf-8\")",
          "        signature = self._generate_request_signature(url_path, body_bytes)",
          "        headers = {",
          "            \"Content-Type\": \"application/json\",",
          "            \"Authorization\": f\"Rpcsignature {signature}\",",
          "        }",
          "        return body_bytes, endpoint, headers",
          "",
          "",
          "@module.provider",
          "def get_sentry_client(config: AppConfig = injected) -> RpcClient:",
          "    if config.NO_SENTRY_INTEGRATION:",
          "        rpc_client: DummyRpcClient = DummyRpcClient()",
          "        rpc_client.dry_run = True",
          "        return rpc_client",
          "    else:",
          "        return SentryRpcClient()",
          "",
          "",
          "# By using two providers by both these type names, you can access the same",
          "@rpc_stub_module.provider",
          "def get_sentry_dummy_client() -> DummyRpcClient:",
          "    return DummyRpcClient()",
          "",
          "",
          "@rpc_stub_module.provider",
          "def get_sentry_stub_client(dummy_client: DummyRpcClient = injected) -> RpcClient:",
          "    return dummy_client",
          ""
        ]
      }
    },
    {
      "file": "urllib3/connection.py",
      "image": "urllib3.connection",
      "is_application": false,
      "line": 205,
      "name": "HTTPConnection.connect",
      "path": "/usr/local/lib/python3.11/dist-packages/urllib3/connection.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 284,
      "name": "AssociateWarningsWithIssuesComponent.invoke",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 284,
        "name": "AssociateWarningsWithIssuesComponent.invoke",
        "code": "        # De-duplicate in case the same issue is present across multiple files. That's possible when\n        # the issue's stacktrace matches multiple files modified in the PR.\n        # This should be ok b/c the issue should contain enough information that the downstream LLM\n        # calls can match any relevant warnings to it. The filename is not the strongest signal.\n\n        if not request.warning_and_pr_files:\n            self.logger.info(\"No warnings to associate with issues.\")\n            return AssociateWarningsWithIssuesOutput(candidate_associations=[])\n        if not issue_id_to_issue_with_pr_filename:\n            self.logger.info(\"No issues to associate with warnings.\")\n            return AssociateWarningsWithIssuesOutput(candidate_associations=[])\n\n        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())\n        issues_formatted = [\n            self._format_issue_with_related_filename(issue, pr_filename)\n            for issue, pr_filename in issues_with_pr_filename\n        ]\n\n        model = GoogleProviderEmbeddings.model(\n            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
        "lineRange": {
          "start": 275,
          "end": 294
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "sqlalchemy/engine/base.py",
      "image": "sqlalchemy.engine.base",
      "is_application": false,
      "line": 1249,
      "name": "Connection.close",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 1516,
      "name": "_ConnectionFairy.close",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 1391,
      "name": "_ConnectionFairy._checkin",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 988,
      "name": "_finalize_fairy",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "sqlalchemy/pool/base.py",
      "image": "sqlalchemy.pool.base",
      "is_application": false,
      "line": 1438,
      "name": "_ConnectionFairy._reset",
      "path": "/usr/local/lib/python3.11/dist-packages/sqlalchemy/pool/base.py"
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 577,
      "name": "StaticAnalysisSuggestionsComponent.invoke",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 577,
        "name": "StaticAnalysisSuggestionsComponent.invoke",
        "code": "    @inject\n    def invoke(\n        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected\n    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:\n        # Current open questions on trading-off context for suggestions:\n        # Limit diff size?\n        # Limit number of warnings?\n        # Limit number of fixable issues? or issue size?\n        # Better, more concise way to encode the information for the LLM in the prompt?\n        diff_with_warnings = format_diff(\n            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True\n        )\n        formatted_issues = (\n            \"<sentry_issues>\\n\"\n            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])\n            + \"</sentry_issues>\"\n        )\n        completion = llm_client.generate_structured(\n            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),\n            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
        "lineRange": {
          "start": 568,
          "end": 587
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 424,
      "name": "format_diff",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 424,
        "name": "format_diff",
        "code": "    pr_files: list[PrFile],\n    patch_delim: str = \"\\n\\n#################\\n\\n\",\n    include_warnings_after_patch: bool = True,\n) -> str:\n    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)\n    for warning_and_pr_file in warning_and_pr_files:\n        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(\n            warning_and_pr_file.warning\n        )\n    body = patch_delim.join(\n        _format_patch_with_warnings(\n            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch\n        )\n        for pr_file in pr_files\n    )\n    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"\n\n\n@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))\ndef _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
        "lineRange": {
          "start": 415,
          "end": 434
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 425,
      "name": "format_diff.<locals>.<genexpr>",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 425,
        "name": "format_diff.<locals>.<genexpr>",
        "code": "    patch_delim: str = \"\\n\\n#################\\n\\n\",\n    include_warnings_after_patch: bool = True,\n) -> str:\n    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)\n    for warning_and_pr_file in warning_and_pr_files:\n        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(\n            warning_and_pr_file.warning\n        )\n    body = patch_delim.join(\n        _format_patch_with_warnings(\n            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch\n        )\n        for pr_file in pr_files\n    )\n    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"\n\n\n@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))\ndef _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:\n    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
        "lineRange": {
          "start": 416,
          "end": 435
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/codegen/relevant_warnings_component.py",
      "image": "seer.automation.codegen.relevant_warnings_component",
      "is_application": true,
      "line": 386,
      "name": "_format_patch_with_warnings",
      "path": "/app/src/seer/automation/codegen/relevant_warnings_component.py",
      "codeContext": {
        "file": "seer/automation/codegen/relevant_warnings_component.py",
        "line": 386,
        "name": "_format_patch_with_warnings",
        "code": "\n    target_line_to_warning_annotation = {\n        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"\n        + \" || \".join(\n            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings\n        )\n        for target_line, warnings in target_line_to_warnings.items()\n    }\n\n    hunks = FilePatch.to_hunks(\n        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation\n    )\n    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))\n\n    if include_warnings_after_patch:\n        if not warnings:\n            formatted_warnings = \"No warnings were found in this file.\"\n        else:\n            formatted_warnings = \"\\n\\n\".join(\n                warning.format_warning(filename=pr_file.filename) for warning in warnings",
        "lineRange": {
          "start": 377,
          "end": 396
        },
        "lines": [
          "import logging",
          "import textwrap",
          "from collections import defaultdict",
          "from pathlib import Path",
          "from typing import Any",
          "",
          "import numpy as np",
          "from cachetools import LRUCache, cached  # type: ignore[import-untyped]",
          "from cachetools.keys import hashkey  # type: ignore[import-untyped]",
          "from langfuse.decorators import observe",
          "from sentry_sdk.ai.monitoring import ai_track",
          "",
          "from seer.automation.agent.client import GeminiProvider, LlmClient",
          "from seer.automation.agent.embeddings import GoogleProviderEmbeddings",
          "from seer.automation.codebase.models import PrFile, StaticAnalysisWarning",
          "from seer.automation.codebase.repo_client import RepoClient",
          "from seer.automation.codebase.utils import code_snippet, left_truncated_paths",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import (",
          "    AssociateWarningsWithIssuesOutput,",
          "    AssociateWarningsWithIssuesRequest,",
          "    CodeAreIssuesFixableOutput,",
          "    CodeAreIssuesFixableRequest,",
          "    CodeFetchIssuesOutput,",
          "    CodeFetchIssuesRequest,",
          "    CodePredictRelevantWarningsOutput,",
          "    CodePredictRelevantWarningsRequest,",
          "    CodePredictStaticAnalysisSuggestionsOutput,",
          "    CodePredictStaticAnalysisSuggestionsRequest,",
          "    FilterWarningsOutput,",
          "    FilterWarningsRequest,",
          "    RelevantWarningResult,",
          "    WarningAndPrFile,",
          ")",
          "from seer.automation.codegen.prompts import (",
          "    IsFixableIssuePrompts,",
          "    ReleventWarningsPrompts,",
          "    StaticAnalysisSuggestionsPrompts,",
          ")",
          "from seer.automation.component import BaseComponent",
          "from seer.automation.models import EventDetails, FilePatch, IssueDetails, annotate_hunks",
          "from seer.dependency_injection import inject, injected",
          "from seer.rpc import RpcClient",
          "",
          "MAX_FILES_ANALYZED = 7",
          "MAX_LINES_ANALYZED = 500",
          "",
          "",
          "class FilterWarningsComponent(BaseComponent[FilterWarningsRequest, FilterWarningsOutput]):",
          "    \"\"\"",
          "    Filter out warnings that aren't on the PR diff lines.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _build_filepath_mapping(self, pr_files: list[PrFile]) -> dict[str, PrFile]:",
          "        \"\"\"",
          "        Build mapping of possible filepaths to PR files, including truncated variations.",
          "        \"\"\"",
          "        filepath_to_pr_file: dict[str, PrFile] = {}",
          "        for pr_file in pr_files:",
          "            path = Path(pr_file.filename)",
          "            filepath_to_pr_file[path.as_posix()] = pr_file",
          "            for truncated in left_truncated_paths(path, max_num_paths=1):",
          "                filepath_to_pr_file[truncated] = pr_file",
          "        return filepath_to_pr_file",
          "",
          "    def _matching_pr_files(",
          "        self, warning: StaticAnalysisWarning, filepath_to_pr_file: dict[str, PrFile]",
          "    ) -> list[PrFile]:",
          "        \"\"\"",
          "        Find PR files that may match a warning's location.",
          "        This handles cases where the warning location and PR file paths may be specified differently:",
          "        - With different numbers of parent directories",
          "        - With or without a repo prefix",
          "        - With relative vs absolute paths",
          "        \"\"\"",
          "        warning_filename = warning.encoded_location.split(\":\")[0]",
          "        warning_path = Path(warning_filename)",
          "",
          "        # If the path is relative, it shouldn't contain intermediate `..`s.",
          "        first_idx_non_dots = next(",
          "            (idx for idx, part in enumerate(warning_path.parts) if part != \"..\")",
          "        )",
          "        warning_path = Path(*warning_path.parts[first_idx_non_dots:])",
          "        if \"..\" in warning_path.parts:",
          "            raise ValueError(",
          "                f\"Found `..` in the middle of the warning's path. Encoded location: {warning.encoded_location}\"",
          "            )",
          "",
          "        warning_filepath_variations = {",
          "            warning_path.as_posix(),",
          "            *left_truncated_paths(warning_path, max_num_paths=2),",
          "        }",
          "        return [",
          "            filepath_to_pr_file[filepath]",
          "            for filepath in warning_filepath_variations & set(filepath_to_pr_file)",
          "        ]",
          "",
          "    def _find_matching_pr_file(",
          "        self,",
          "        warning: StaticAnalysisWarning,",
          "        filepath_to_pr_file: dict[str, PrFile],",
          "    ) -> WarningAndPrFile | None:",
          "        matching_pr_files = self._matching_pr_files(warning, filepath_to_pr_file)",
          "        for pr_file in matching_pr_files:",
          "            warning_and_pr_file = WarningAndPrFile(warning=warning, pr_file=pr_file)",
          "            if warning_and_pr_file.overlapping_hunk_idxs:  # the warning is roughly in the patch",
          "                return warning_and_pr_file",
          "        return None",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Filter Warnings Component\")",
          "    def invoke(self, request: FilterWarningsRequest) -> FilterWarningsOutput:",
          "        filepath_to_pr_file = self._build_filepath_mapping(request.pr_files)",
          "        warning_and_pr_files: list[WarningAndPrFile] = []",
          "        for warning in request.warnings:",
          "            try:",
          "                warning_and_pr_file = self._find_matching_pr_file(warning, filepath_to_pr_file)",
          "            except Exception:",
          "                self.logger.exception(",
          "                    \"Failed to match warning. Skipping.\", extra={\"warning_id\": warning.id}",
          "                )",
          "            else:",
          "                if warning_and_pr_file is not None:",
          "                    warning_and_pr_files.append(warning_and_pr_file)",
          "        return FilterWarningsOutput(warning_and_pr_files=warning_and_pr_files)",
          "",
          "",
          "@inject",
          "def _fetch_issues_for_pr_file(",
          "    organization_id: int,",
          "    provider: str,",
          "    external_id: str,",
          "    pr_file: PrFile,",
          "    run_id: int,",
          "    logger: logging.Logger,",
          "    client: RpcClient = injected,",
          ") -> list[dict[str, Any]]:",
          "    pr_filename_to_issues = client.call(",
          "        \"get_issues_related_to_file_patches\",",
          "        organization_id=organization_id,",
          "        provider=provider,",
          "        external_id=external_id,",
          "        pr_files=[pr_file.model_dump()],",
          "        run_id=run_id,",
          "    )",
          "    if pr_filename_to_issues is None:",
          "        logger.exception(",
          "            \"Something went wrong with the issue-fetching RPC call\",",
          "            extra={\"file\": pr_file.filename},",
          "        )",
          "        return []",
          "    if not pr_filename_to_issues:",
          "        return []",
          "    assert list(pr_filename_to_issues.keys()) == [",
          "        pr_file.filename",
          "    ], f\"expected {pr_file.filename} but got {list(pr_filename_to_issues.keys())}\"",
          "    return list(pr_filename_to_issues.values())[0]",
          "",
          "",
          "class FetchIssuesComponent(BaseComponent[CodeFetchIssuesRequest, CodeFetchIssuesOutput]):",
          "    \"\"\"",
          "    Fetch issues related to the files in a PR by analyzing stacktrace frames in the issue.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _fetch_issues(",
          "        self,",
          "        organization_id: int,",
          "        provider: str,",
          "        external_id: str,",
          "        pr_files: list[PrFile],",
          "        max_files_analyzed: int = MAX_FILES_ANALYZED,",
          "        max_lines_analyzed: int = MAX_LINES_ANALYZED,",
          "    ) -> dict[str, list[IssueDetails]]:",
          "        \"\"\"",
          "        Returns a dict mapping a subset of file names in the PR to issues related to the file.",
          "        They're related if the functions and filenames in the issue's stacktrace overlap with those",
          "        modified in the PR.",
          "",
          "        The `max_files_analyzed` and `max_lines_analyzed` checks ensure that the payload we send to",
          "        seer_rpc doesn't get too large.",
          "        They're roughly like the qualification checks in [Open PR Comments](https://sentry.engineering/blog/how-open-pr-comments-work#qualification-checks).",
          "        \"\"\"",
          "        pr_files_eligible = [",
          "            pr_file",
          "            for pr_file in pr_files",
          "            if pr_file.status == \"modified\" and pr_file.changes <= max_lines_analyzed",
          "        ]",
          "        if not pr_files_eligible:",
          "            self.logger.info(\"No eligible files in PR.\")",
          "            return {}",
          "",
          "        self.logger.info(f\"Repo query: {organization_id=}, {provider=}, {external_id=}\")",
          "        filename_to_issues = {",
          "            pr_file.filename: _fetch_issues_for_pr_file(",
          "                organization_id, provider, external_id, pr_file, self.context.run_id, self.logger",
          "            )",
          "            for pr_file in pr_files_eligible[:max_files_analyzed]",
          "        }",
          "        return {",
          "            filename: [IssueDetails.model_validate(issue) for issue in issues]",
          "            for filename, issues in filename_to_issues.items()",
          "        }",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Fetch Issues Component\")",
          "    def invoke(self, request: CodeFetchIssuesRequest) -> CodeFetchIssuesOutput:",
          "        if self.context.repo.provider_raw is None:",
          "            raise TypeError(",
          "                f\"provider_raw is not set for repo: {self.context.repo}. \"",
          "                \"Something went wrong during initialization of the RepoDefinition.\"",
          "            )",
          "        filename_to_issues = self._fetch_issues(",
          "            organization_id=request.organization_id,",
          "            provider=self.context.repo.provider_raw,",
          "            external_id=self.context.repo.external_id,",
          "            pr_files=request.pr_files,",
          "        )",
          "        for filename, issues in filename_to_issues.items():",
          "            self.logger.info(",
          "                f\"Found {len(issues)} issues for file {filename}\",",
          "                extra={\"issue_ids\": [issue.id for issue in issues]},",
          "            )",
          "        return CodeFetchIssuesOutput(filename_to_issues=filename_to_issues)",
          "",
          "",
          "class AssociateWarningsWithIssuesComponent(",
          "    BaseComponent[AssociateWarningsWithIssuesRequest, AssociateWarningsWithIssuesOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warnings and a list of issues, return warning-issue pairs which should be",
          "    analyzed by an LLM.",
          "",
          "    The purpose of this step is to reduce LLM calls. If we have n warnings and m issues,",
          "    we can reduce the number of pairs to consider from n * m to the top k, which is configurable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _format_issue_with_related_filename(issue: IssueDetails, related_filename: str) -> str:",
          "        event_details = EventDetails.from_event(issue.events[0])",
          "        return textwrap.dedent(",
          "            f\"\"\"\\",
          "            {event_details.format_event_without_breadcrumbs(include_context=False, include_var_values=False)}",
          "            ----------",
          "            This file, in particular, contained function(s) that overlapped with the exceptions: {related_filename}",
          "            \"\"\"",
          "        )",
          "",
          "    @staticmethod",
          "    def _top_k_indices(distances: np.ndarray, k: int) -> list[tuple[int, ...]]:",
          "        flat_indices_sorted_by_distance = distances.argsort(axis=None)",
          "        top_k_indices = np.unravel_index(flat_indices_sorted_by_distance[:k], distances.shape)",
          "        return list(zip(*top_k_indices))",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Associate Warnings With Issues Component\")",
          "    def invoke(",
          "        self, request: AssociateWarningsWithIssuesRequest",
          "    ) -> AssociateWarningsWithIssuesOutput:",
          "",
          "        warnings_formatted = [",
          "            warning_and_pr_file.warning.format_warning()",
          "            for warning_and_pr_file in request.warning_and_pr_files",
          "        ]",
          "        issue_id_to_issue_with_pr_filename = {",
          "            issue.id: (issue, filename)",
          "            for filename, issues in request.filename_to_issues.items()",
          "            for issue in issues",
          "        }",
          "        # De-duplicate in case the same issue is present across multiple files. That's possible when",
          "        # the issue's stacktrace matches multiple files modified in the PR.",
          "        # This should be ok b/c the issue should contain enough information that the downstream LLM",
          "        # calls can match any relevant warnings to it. The filename is not the strongest signal.",
          "",
          "        if not request.warning_and_pr_files:",
          "            self.logger.info(\"No warnings to associate with issues.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "        if not issue_id_to_issue_with_pr_filename:",
          "            self.logger.info(\"No issues to associate with warnings.\")",
          "            return AssociateWarningsWithIssuesOutput(candidate_associations=[])",
          "",
          "        issues_with_pr_filename = list(issue_id_to_issue_with_pr_filename.values())",
          "        issues_formatted = [",
          "            self._format_issue_with_related_filename(issue, pr_filename)",
          "            for issue, pr_filename in issues_with_pr_filename",
          "        ]",
          "",
          "        model = GoogleProviderEmbeddings.model(",
          "            \"text-embedding-005\", task_type=\"CODE_RETRIEVAL_QUERY\"",
          "        )",
          "        embeddings_warnings = model.encode(warnings_formatted)",
          "        embeddings_issues = model.encode(issues_formatted)",
          "        warning_issue_cosine_similarities = embeddings_warnings @ embeddings_issues.T",
          "        warning_issue_cosine_distances = 1 - warning_issue_cosine_similarities",
          "        warning_issue_indices = self._top_k_indices(",
          "            warning_issue_cosine_distances, request.max_num_associations",
          "        )",
          "        candidate_associations = [",
          "            (request.warning_and_pr_files[warning_idx], issues_with_pr_filename[issue_idx][0])",
          "            for warning_idx, issue_idx in warning_issue_indices",
          "        ]",
          "        return AssociateWarningsWithIssuesOutput(candidate_associations=candidate_associations)",
          "",
          "",
          "def _is_issue_fixable_cache_key(issue: IssueDetails) -> tuple[str]:",
          "    return hashkey(issue.id)",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=4096), key=_is_issue_fixable_cache_key)",
          "@inject",
          "def _is_issue_fixable(issue: IssueDetails, llm_client: LlmClient = injected) -> bool:",
          "    # LRU-cached by the issue id. The same issue could be analyzed many times if, e.g.,",
          "    # a repo has a set of files which are frequently used to handle and raise exceptions.",
          "    completion = llm_client.generate_structured(",
          "        model=GeminiProvider.model(\"gemini-2.0-flash-lite\"),",
          "        system_prompt=IsFixableIssuePrompts.format_system_msg(),",
          "        prompt=IsFixableIssuePrompts.format_prompt(",
          "            formatted_error=EventDetails.from_event(",
          "                issue.events[0]",
          "            ).format_event_without_breadcrumbs(),",
          "        ),",
          "        response_format=IsFixableIssuePrompts.IsIssueFixable,",
          "        temperature=0.0,",
          "        max_tokens=64,",
          "    )",
          "    if completion.parsed is None:",
          "        raise ValueError(\"No structured output from LLM.\")",
          "    return completion.parsed.is_fixable",
          "",
          "",
          "class AreIssuesFixableComponent(",
          "    BaseComponent[CodeAreIssuesFixableRequest, CodeAreIssuesFixableOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of issues, predict whether each is fixable.",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Are Issues Fixable Component\")",
          "    def invoke(self, request: CodeAreIssuesFixableRequest) -> CodeAreIssuesFixableOutput:",
          "        \"\"\"",
          "        It's fine if there are duplicate issues in the request. That can happen if issues were",
          "        passed in from a list of warning-issue associations.",
          "        \"\"\"",
          "        issue_id_to_issue = {issue.id: issue for issue in request.candidate_issues}",
          "        issue_ids = list(issue_id_to_issue.keys())[: request.max_num_issues_analyzed]",
          "        issue_id_to_is_fixable = {}",
          "        for issue_id in issue_ids:",
          "            try:",
          "                is_fixable = _is_issue_fixable(issue_id_to_issue[issue_id])",
          "            except Exception:",
          "                # It's not critical that this component makes an actual prediction.",
          "                # Assume it's fixable b/c the next (predict relevancy) step handles it.",
          "                self.logger.exception(\"Error predicting fixability of issue\")",
          "                is_fixable = True",
          "            issue_id_to_is_fixable[issue_id] = is_fixable",
          "        return CodeAreIssuesFixableOutput(",
          "            are_fixable=[issue_id_to_is_fixable.get(issue.id) for issue in request.candidate_issues]",
          "        )",
          "",
          "",
          "def _format_patch_with_warnings(",
          "    pr_file: PrFile,",
          "    warnings: list[StaticAnalysisWarning],",
          "    include_warnings_after_patch: bool = False,",
          ") -> str:",
          "    target_line_to_warnings: dict[int, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning in warnings:",
          "        target_line_to_warnings[warning.start_line].append(warning)",
          "",
          "    target_line_to_warning_annotation = {",
          "        target_line: \"  <-- STATIC ANALYSIS WARNINGS: \"",
          "        + \" || \".join(",
          "            warning.format_warning_id_and_message().replace(\"\\n\", \"\\\\n\") for warning in warnings",
          "        )",
          "        for target_line, warnings in target_line_to_warnings.items()",
          "    }",
          "",
          "    hunks = FilePatch.to_hunks(",
          "        pr_file.patch, target_line_to_extra=target_line_to_warning_annotation",
          "    )",
          "    formatted_hunks = \"\\n\\n\".join(annotate_hunks(hunks))",
          "",
          "    if include_warnings_after_patch:",
          "        if not warnings:",
          "            formatted_warnings = \"No warnings were found in this file.\"",
          "        else:",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                warning.format_warning(filename=pr_file.filename) for warning in warnings",
          "            )  # override the filename to reduce the chance of a hallucinated path",
          "            formatted_warnings = \"\\n\\n\".join(",
          "                (",
          "                    f\"Here's more information about the static analysis warnings in {pr_file.filename}:\",",
          "                    f\"<warnings>\\n\\n{formatted_warnings}\\n\\n</warnings>\",",
          "                )",
          "            )",
          "    else:",
          "        formatted_warnings = \"\"",
          "",
          "    tag_start = f\"<file><filename>{pr_file.filename}</filename>\"",
          "    tag_end = \"</file>\"",
          "    title = f\"Here are the changes made to file {pr_file.filename}:\"",
          "    return \"\\n\\n\".join((tag_start, title, formatted_hunks, formatted_warnings, tag_end))",
          "",
          "",
          "def format_diff(",
          "    warning_and_pr_files: list[WarningAndPrFile],",
          "    pr_files: list[PrFile],",
          "    patch_delim: str = \"\\n\\n#################\\n\\n\",",
          "    include_warnings_after_patch: bool = True,",
          ") -> str:",
          "    filename_to_warnings: dict[str, list[StaticAnalysisWarning]] = defaultdict(list)",
          "    for warning_and_pr_file in warning_and_pr_files:",
          "        filename_to_warnings[warning_and_pr_file.pr_file.filename].append(",
          "            warning_and_pr_file.warning",
          "        )",
          "    body = patch_delim.join(",
          "        _format_patch_with_warnings(",
          "            pr_file, filename_to_warnings[pr_file.filename], include_warnings_after_patch",
          "        )",
          "        for pr_file in pr_files",
          "    )",
          "    return f\"<diff>\\n\\n{body}\\n\\n</diff>\"",
          "",
          "",
          "@cached(cache=LRUCache(maxsize=MAX_FILES_ANALYZED))",
          "def _cached_file_contents(repo_client: RepoClient, path: str, commit_sha: str) -> str:",
          "    file_contents, _ = repo_client.get_file_content(path, sha=commit_sha)",
          "    if file_contents is None:",
          "        raise ValueError(\"Failed to get file contents\")  # raise => don't cache",
          "    return file_contents",
          "",
          "",
          "class PredictRelevantWarningsComponent(",
          "    BaseComponent[CodePredictRelevantWarningsRequest, CodePredictRelevantWarningsOutput]",
          "):",
          "    \"\"\"",
          "    Given a list of warning-issue associations, predict whether each is relevant.",
          "    A warning is relevant to an issue if fixing the warning would fix the issue (according to an",
          "    LLM).",
          "    \"\"\"",
          "",
          "    context: CodegenContext",
          "",
          "    def _code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> list[str] | None:",
          "        try:",
          "            file_contents = _cached_file_contents(",
          "                self.context.get_repo_client(), warning_and_pr_file.pr_file.filename, commit_sha",
          "            )",
          "        except Exception:",
          "            self.logger.exception(\"Error getting file contents\")",
          "            return None",
          "",
          "        lines = file_contents.split(\"\\n\")",
          "        if warning_and_pr_file.warning.end_line > len(lines):",
          "            self.logger.error(",
          "                \"The warning's end line is greater than the number of lines in the file. \"",
          "                \"Warning-file matching in FilterWarningsComponent was wrong or out of date.\",",
          "            )",
          "            return None",
          "",
          "        return code_snippet(",
          "            lines,",
          "            warning_and_pr_file.warning.start_line,",
          "            warning_and_pr_file.warning.end_line,",
          "            padding_size=padding_size,",
          "        )",
          "",
          "    def _format_code_snippet_around_warning(",
          "        self, warning_and_pr_file: WarningAndPrFile, commit_sha: str, padding_size: int = 10",
          "    ) -> str:",
          "        code_snippet = self._code_snippet_around_warning(",
          "            warning_and_pr_file, commit_sha, padding_size",
          "        )",
          "        if code_snippet is None:",
          "            return \"< Could not extract the code snippet containing the warning >\"",
          "        return \"\\n\".join(code_snippet)",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @ai_track(description=\"Codegen - Relevant Warnings - Predict Relevant Warnings Component\")",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictRelevantWarningsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictRelevantWarningsOutput:",
          "        # TODO(kddubey): instead of looking at every association, probably faster and cheaper to input one",
          "        # warning and prompt for which of its associated issues are relevant. May not work as well.",
          "        relevant_warning_results: list[RelevantWarningResult] = []",
          "        for warning_and_pr_file, issue in request.candidate_associations:",
          "            self.logger.info(",
          "                f\"Predicting relevance of warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "            )",
          "            completion = llm_client.generate_structured(",
          "                model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "                system_prompt=ReleventWarningsPrompts.format_system_msg(),",
          "                prompt=ReleventWarningsPrompts.format_prompt(",
          "                    formatted_warning=warning_and_pr_file.warning.format_warning(),",
          "                    formatted_error=EventDetails.from_event(",
          "                        issue.events[0]",
          "                    ).format_event_without_breadcrumbs(),",
          "                ),",
          "                response_format=ReleventWarningsPrompts.DoesFixingWarningFixIssue,",
          "                temperature=0.0,",
          "                max_tokens=2048,",
          "                timeout=15.0,",
          "            )",
          "            if completion.parsed is None:  # Gemini quirk",
          "                self.logger.warning(",
          "                    f\"No response from LLM for warning {warning_and_pr_file.warning.id} and issue {issue.id}\"",
          "                )",
          "                continue",
          "            relevant_warning_results.append(",
          "                RelevantWarningResult(",
          "                    warning_id=warning_and_pr_file.warning.id,",
          "                    issue_id=issue.id,",
          "                    does_fixing_warning_fix_issue=completion.parsed.does_fixing_warning_fix_issue,",
          "                    relevance_probability=completion.parsed.relevance_probability,",
          "                    reasoning=completion.parsed.analysis,",
          "                    short_description=completion.parsed.short_description or \"\",",
          "                    short_justification=completion.parsed.short_justification or \"\",",
          "                    encoded_location=warning_and_pr_file.warning.encoded_location,",
          "                )",
          "            )",
          "        num_relevant_warnings = sum(",
          "            result.does_fixing_warning_fix_issue for result in relevant_warning_results",
          "        )",
          "        self.logger.info(",
          "            f\"Found {num_relevant_warnings} relevant warnings out of \"",
          "            f\"{len(relevant_warning_results)} pairs.\"",
          "        )",
          "        return CodePredictRelevantWarningsOutput(relevant_warning_results=relevant_warning_results)",
          "",
          "",
          "class StaticAnalysisSuggestionsComponent(",
          "    BaseComponent[",
          "        CodePredictStaticAnalysisSuggestionsRequest, CodePredictStaticAnalysisSuggestionsOutput",
          "    ]",
          "):",
          "    \"\"\"",
          "    Given a diff, a list of warnings around the diff, and a list of fixable issues,",
          "    surface potential issues in the diff (according to an LLM)",
          "    \"\"\"",
          "",
          "    def _format_issue(self, issue: IssueDetails) -> str:",
          "        # EventDetails are not formatted with the ID, so we add it manually.",
          "        # Also the formatting is a weird half-XML, so we complete the XML tags.",
          "        event_details = EventDetails.from_event(issue.events[0]).format_event_without_breadcrumbs()",
          "        title, other_lines = event_details.split(\"\\n\", 1)",
          "        return (",
          "            f\"<sentry_issue><issue_id>{issue.id}</issue_id>\\n\"",
          "            + f\"<title>{title}</title>\\n\"",
          "            + other_lines",
          "            + \"</sentry_issue>\"",
          "        )",
          "",
          "    @observe(name=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\")",
          "    @ai_track(",
          "        description=\"Codegen - Relevant Warnings - Static-Analysis-Suggestions-Based Component\"",
          "    )",
          "    @inject",
          "    def invoke(",
          "        self, request: CodePredictStaticAnalysisSuggestionsRequest, llm_client: LlmClient = injected",
          "    ) -> CodePredictStaticAnalysisSuggestionsOutput | None:",
          "        # Current open questions on trading-off context for suggestions:",
          "        # Limit diff size?",
          "        # Limit number of warnings?",
          "        # Limit number of fixable issues? or issue size?",
          "        # Better, more concise way to encode the information for the LLM in the prompt?",
          "        diff_with_warnings = format_diff(",
          "            request.warning_and_pr_files, request.pr_files, include_warnings_after_patch=True",
          "        )",
          "        formatted_issues = (",
          "            \"<sentry_issues>\\n\"",
          "            + \"\\n\".join([self._format_issue(issue) for issue in request.fixable_issues])",
          "            + \"</sentry_issues>\"",
          "        )",
          "        completion = llm_client.generate_structured(",
          "            model=GeminiProvider.model(\"gemini-2.0-flash-001\"),",
          "            system_prompt=StaticAnalysisSuggestionsPrompts.format_system_msg(),",
          "            prompt=StaticAnalysisSuggestionsPrompts.format_prompt(",
          "                diff_with_warnings=diff_with_warnings,",
          "                formatted_issues=formatted_issues,",
          "            ),",
          "            response_format=StaticAnalysisSuggestionsPrompts.AnalysisAndSuggestions,",
          "            temperature=0.0,",
          "            max_tokens=8192,",
          "        )",
          "        if completion.parsed is None:",
          "            return None",
          "        return CodePredictStaticAnalysisSuggestionsOutput(suggestions=completion.parsed.suggestions)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/models.py",
      "image": "seer.automation.models",
      "is_application": true,
      "line": 1257,
      "name": "FilePatch.to_hunks",
      "path": "/app/src/seer/automation/models.py",
      "codeContext": {
        "file": "seer/automation/models.py",
        "line": 1257,
        "name": "FilePatch.to_hunks",
        "code": "                    target_length=target_length,\n                    section_header=line,\n                    lines=[],\n                )\n                current_lines = [line]  # starts with section header\n            elif current_lines:\n                current_lines.append(line)\n\n        if current_lines:\n            current_hunk.lines = raw_lines_to_lines(\n                current_lines,\n                current_hunk.source_start,\n                current_hunk.target_start,\n                target_line_to_extra=target_line_to_extra,\n            )\n            hunks.append(current_hunk)\n\n        return hunks\n\n",
        "lineRange": {
          "start": 1248,
          "end": 1267
        },
        "lines": [
          "import json",
          "import re",
          "import textwrap",
          "from typing import Annotated, Any, Literal, NotRequired, Optional",
          "from xml.etree import ElementTree as ET",
          "",
          "import sentry_sdk",
          "from johen.examples import Examples",
          "from johen.generators import specialized",
          "from pydantic import (",
          "    AliasChoices,",
          "    AliasGenerator,",
          "    BaseModel,",
          "    ConfigDict,",
          "    Field,",
          "    ValidationError,",
          "    ValidationInfo,",
          "    field_validator,",
          "    model_validator,",
          ")",
          "from pydantic.alias_generators import to_camel, to_snake",
          "from pydantic_xml import BaseXmlModel",
          "from typing_extensions import TypedDict",
          "",
          "from seer.automation.utils import (",
          "    format_dict,",
          "    format_list,",
          "    process_repo_provider,",
          "    unescape_xml_chars,",
          ")",
          "from seer.db import DbSeerProjectPreference",
          "",
          "",
          "class StacktraceFrame(BaseModel):",
          "    model_config = ConfigDict(",
          "        alias_generator=AliasGenerator(",
          "            validation_alias=lambda k: AliasChoices(to_camel(k), to_snake(k)),",
          "            serialization_alias=to_camel,",
          "        )",
          "    )",
          "",
          "    function: Optional[Annotated[str, Examples(specialized.ascii_words)]] = None",
          "    filename: Optional[Annotated[str, Examples(specialized.file_names)]]",
          "    abs_path: Optional[Annotated[str, Examples(specialized.file_paths)]]",
          "    line_no: Optional[int]",
          "    col_no: Optional[int]",
          "    context: list[tuple[int, Optional[str]]] = []",
          "    repo_name: Optional[str] = None",
          "    in_app: bool | None = False",
          "    vars: Optional[dict[str, Any]] = None",
          "    package: Optional[str] = None",
          "",
          "    @field_validator(\"vars\", mode=\"before\")",
          "    @classmethod",
          "    def validate_vars(cls, vars: Optional[dict[str, Any]], info: ValidationInfo):",
          "        if not vars or \"context\" not in info.data or not info.data[\"context\"]:",
          "            return vars",
          "        code_str = \"\"",
          "        for _, line in info.data[\"context\"]:",
          "            code_str += line + \"\\n\"",
          "        return cls._trim_vars(vars, code_str)",
          "",
          "    @staticmethod",
          "    def _trim_vars(vars: dict[str, Any], code_context: str):",
          "        # only keep variables mentioned in the context of the stacktrace frame",
          "        # and filter out any values containing \"[Filtered]\"",
          "        trimmed_vars = {}",
          "        for key, val in vars.items():",
          "            if key in code_context:",
          "                if isinstance(val, (dict, list)):",
          "                    filtered_val = StacktraceFrame._filter_nested_value(val)",
          "                    if filtered_val is not None:",
          "                        trimmed_vars[key] = filtered_val",
          "                elif not StacktraceFrame._contains_filtered(val):",
          "                    trimmed_vars[key] = val",
          "        return trimmed_vars",
          "",
          "    @staticmethod",
          "    def _filter_nested_value(value: Any) -> Any:",
          "        if isinstance(value, dict):",
          "            filtered_dict = {}",
          "            for k, v in value.items():",
          "                if isinstance(v, (dict, list)):",
          "                    filtered_v = StacktraceFrame._filter_nested_value(v)",
          "                    if filtered_v is not None:",
          "                        filtered_dict[k] = filtered_v",
          "                elif not StacktraceFrame._contains_filtered(v):",
          "                    filtered_dict[k] = v",
          "            return filtered_dict if filtered_dict else None",
          "        elif isinstance(value, list):",
          "            filtered_list = []",
          "            for item in value:",
          "                if isinstance(item, (dict, list)):",
          "                    filtered_item = StacktraceFrame._filter_nested_value(item)",
          "                    if filtered_item is not None:",
          "                        filtered_list.append(filtered_item)",
          "                elif not StacktraceFrame._contains_filtered(item):",
          "                    filtered_list.append(item)",
          "            return filtered_list if filtered_list else None",
          "        return None if StacktraceFrame._contains_filtered(value) else value",
          "",
          "    @staticmethod",
          "    def _contains_filtered(value: Any) -> bool:",
          "        return isinstance(value, str) and \"[Filtered]\" in value",
          "",
          "",
          "class SentryFrame(TypedDict):",
          "    absPath: Optional[str]",
          "    colNo: Optional[int]",
          "    context: list[tuple[int, str]]",
          "    filename: NotRequired[Optional[str]]",
          "    function: NotRequired[Optional[str]]",
          "    inApp: NotRequired[bool]",
          "    instructionAddr: NotRequired[Optional[str]]",
          "    lineNo: NotRequired[Optional[int]]",
          "    module: NotRequired[Optional[str]]",
          "    package: NotRequired[Optional[str]]",
          "    platform: NotRequired[Optional[str]]",
          "    rawFunction: NotRequired[Optional[str]]",
          "    symbol: NotRequired[Optional[str]]",
          "    symbolAddr: NotRequired[Optional[str]]",
          "    trust: NotRequired[Optional[Any]]",
          "    vars: NotRequired[Optional[dict[str, Any]]]",
          "    addrMode: NotRequired[Optional[str]]",
          "    isPrefix: NotRequired[bool]",
          "    isSentinel: NotRequired[bool]",
          "    lock: NotRequired[Optional[Any]]",
          "    map: NotRequired[Optional[str]]",
          "    mapUrl: NotRequired[Optional[str]]",
          "    minGroupingLevel: NotRequired[int]",
          "    origAbsPath: NotRequired[Optional[str]]",
          "    sourceLink: NotRequired[Optional[str]]",
          "    symbolicatorStatus: NotRequired[Optional[Any]]",
          "",
          "",
          "class Stacktrace(BaseModel):",
          "    frames: list[StacktraceFrame]",
          "",
          "    @field_validator(\"frames\", mode=\"before\")",
          "    @classmethod",
          "    def validate_frames(cls, frames: list[StacktraceFrame | SentryFrame]):",
          "        stacktrace_frames = []",
          "        for frame in frames:",
          "            if isinstance(frame, dict):",
          "                if \"function\" not in frame:",
          "                    frame[\"function\"] = None",
          "                try:",
          "                    stacktrace_frames.append(StacktraceFrame.model_validate(frame))",
          "                except ValidationError:",
          "                    sentry_sdk.capture_exception()",
          "                    continue",
          "            else:",
          "                stacktrace_frames.append(frame)",
          "",
          "        return cls._trim_frames(stacktrace_frames)",
          "",
          "    def to_str(",
          "        self,",
          "        max_frames: int = 16,",
          "        in_app_only: bool = False,",
          "        include_context: bool = True,",
          "        include_var_values: bool = True,",
          "    ):",
          "        stack_str = \"\"",
          "",
          "        frames = self.frames",
          "        if in_app_only:",
          "            frames = [frame for frame in frames if frame.in_app]",
          "",
          "        for frame in reversed(frames[-max_frames:]):",
          "            col_no_str = f\", column {frame.col_no}\" if frame.col_no is not None else \"\"",
          "            repo_str = f\" in repo {frame.repo_name}\" if frame.repo_name else \"\"",
          "            line_no_str = (",
          "                f\"[Line {frame.line_no}{col_no_str}]\"",
          "                if frame.line_no is not None",
          "                else \"[Line: Unknown]\"",
          "            )",
          "",
          "            function = frame.function if frame.function else \"Unknown function\"",
          "            if frame.filename:",
          "                stack_str += f\" {function} in file {frame.filename}{repo_str} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            elif frame.package:",
          "                stack_str += f\" {function} in package {frame.package} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            else:",
          "                stack_str += f\" {function} in unknown file {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "",
          "            if include_context:",
          "                for ctx in frame.context:",
          "                    is_suspect_line = ctx[0] == frame.line_no",
          "                    stack_str += f\"{ctx[1]}{'  <-- SUSPECT LINE' if is_suspect_line else ''}\\n\"",
          "",
          "            if frame.vars:",
          "                if include_var_values:",
          "                    vars_title = \"Variable values at the time of the exception:\"",
          "                    vars_str = format_dict(frame.vars)",
          "                else:",
          "                    vars_title = \"Variables at the time of the exception:\"",
          "                    vars_str = \", \".join(frame.vars.keys())",
          "",
          "                stack_str += textwrap.dedent(",
          "                    \"\"\"\\",
          "                    ---",
          "                    {vars_title}:",
          "                    {vars_str}",
          "                    \"\"\"",
          "                ).format(vars_title=vars_title, vars_str=vars_str)",
          "            stack_str += \"------\\n\"",
          "",
          "        return stack_str",
          "",
          "    @staticmethod",
          "    def _trim_frames(frames: list[StacktraceFrame], frame_allowance=16):",
          "        frames_len = len(frames)",
          "        if frames_len <= frame_allowance:",
          "            return frames",
          "",
          "        app_frames = [frame for frame in frames if frame.in_app]",
          "        system_frames = [frame for frame in frames if not frame.in_app]",
          "",
          "        app_count = len(app_frames)",
          "        system_allowance = max(frame_allowance - app_count, 0)",
          "        app_allowance = frame_allowance - system_allowance",
          "",
          "        if system_allowance > 0:",
          "            # prioritize trimming system frames",
          "            half_system = system_allowance // 2",
          "            kept_system_frames = system_frames[:half_system] + system_frames[-half_system:]",
          "        else:",
          "            kept_system_frames = []",
          "",
          "        if app_allowance > 0:",
          "            half_app = app_allowance // 2",
          "            kept_app_frames = app_frames[:half_app] + app_frames[-half_app:]",
          "        else:",
          "            kept_app_frames = []",
          "",
          "        # combine and sort the kept frames based on their original order",
          "        kept_frames = kept_system_frames + kept_app_frames",
          "        kept_frames.sort(key=lambda frame: frames.index(frame))",
          "        return kept_frames",
          "",
          "",
          "class SentryStacktrace(TypedDict):",
          "    frames: list[SentryFrame]",
          "",
          "",
          "class SentryEventEntryDataValue(TypedDict):",
          "    type: str",
          "    value: str",
          "    stacktrace: SentryStacktrace",
          "",
          "",
          "class SentryExceptionEventData(TypedDict):",
          "    values: list[SentryEventEntryDataValue]",
          "",
          "",
          "class SentryExceptionEntry(BaseModel):",
          "    type: Literal[\"exception\"]",
          "    data: SentryExceptionEventData",
          "",
          "",
          "class SentryEventData(TypedDict):",
          "    title: str",
          "    entries: list[dict]",
          "    tags: NotRequired[list[dict[str, str | None]]]",
          "",
          "",
          "class ExceptionMechanism(TypedDict):",
          "    type: str",
          "    handled: NotRequired[bool]",
          "",
          "",
          "class ExceptionDetails(BaseModel):",
          "    type: Optional[str] = \"\"",
          "    value: Optional[str] = \"\"",
          "    stacktrace: Optional[Stacktrace] = None",
          "    mechanism: Optional[ExceptionMechanism] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class ThreadDetails(BaseModel):",
          "    id: Optional[int | str] = None",
          "    name: Optional[str] = None",
          "    crashed: Optional[bool] = False",
          "    current: Optional[bool] = False",
          "    state: Optional[str] = None",
          "    main: Optional[bool] = False",
          "",
          "    stacktrace: Optional[Stacktrace] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace | None):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class BreadcrumbsDetails(BaseModel):",
          "    type: Optional[str] = None",
          "    message: Optional[str] = None",
          "    category: Optional[str] = None",
          "    data: Optional[dict] = None",
          "    level: Optional[str] = None",
          "",
          "",
          "class RequestDetails(BaseModel):",
          "    url: str | None = None",
          "    method: str | None = None",
          "    data: dict[str, Any] | str | list[Any] | None = None",
          "    # not including cookies, headers, env, query, etc. for now",
          "",
          "",
          "class EventDetails(BaseModel):",
          "    title: str",
          "    message: str | None = None",
          "    transaction_name: str | None = None",
          "    exceptions: list[ExceptionDetails] = Field(default_factory=list, exclude=False)",
          "    threads: list[ThreadDetails] = Field(default_factory=list, exclude=False)",
          "    breadcrumbs: list[BreadcrumbsDetails] = Field(default_factory=list, exclude=False)",
          "    stacktraces: list[Stacktrace] = Field(default_factory=list, exclude=False)",
          "    request: RequestDetails | None = None",
          "",
          "    @classmethod",
          "    def from_event(cls, error_event: SentryEventData):",
          "        MAX_THREADS = 8  # TODO: Smarter logic for max threads",
          "",
          "        exceptions: list[ExceptionDetails] = []",
          "        threads: list[ThreadDetails] = []",
          "        breadcrumbs: list[BreadcrumbsDetails] = []",
          "        stacktraces: list[Stacktrace] = []",
          "        transaction_name: str | None = None",
          "        message: str | None = None",
          "        request: RequestDetails | None = None",
          "",
          "        for tag in error_event.get(\"tags\", []):",
          "            if tag.get(\"key\") == \"transaction\":",
          "                transaction_name = tag.get(\"value\")",
          "",
          "        for entry in error_event.get(\"entries\", []):",
          "            if entry.get(\"type\") == \"exception\":",
          "                for exception in entry.get(\"data\", {}).get(\"values\", []):",
          "                    exceptions.append(ExceptionDetails.model_validate(exception))",
          "            elif entry.get(\"type\") == \"threads\":",
          "                for thread in entry.get(\"data\", {}).get(\"values\", []):",
          "                    thread_details = ThreadDetails.model_validate(thread)",
          "                    if (",
          "                        thread_details.stacktrace",
          "                        and thread_details.stacktrace.frames",
          "                        and len(threads) < MAX_THREADS",
          "                    ):",
          "                        threads.append(thread_details)",
          "            elif entry.get(\"type\") == \"breadcrumbs\":",
          "                all_breadcrumbs = entry.get(\"data\", {}).get(\"values\", [])",
          "                for breadcrumb in all_breadcrumbs[-10:]:  # only look at the most recent breadcrumbs",
          "                    # Skip breadcrumbs with filtered content in message or data",
          "                    if StacktraceFrame._contains_filtered(",
          "                        breadcrumb.get(\"message\")",
          "                    ) or StacktraceFrame._contains_filtered(str(breadcrumb.get(\"data\"))):",
          "                        continue",
          "                    crumb_details = BreadcrumbsDetails.model_validate(breadcrumb)",
          "                    breadcrumbs.append(crumb_details)",
          "            elif entry.get(\"type\") == \"stacktrace\":",
          "                stacktraces.append(Stacktrace.model_validate(entry.get(\"data\", {})))",
          "            elif entry.get(\"type\") == \"message\":",
          "                message = entry.get(\"data\", {}).get(\"formatted\", None)",
          "            elif entry.get(\"type\") == \"request\":",
          "                request = RequestDetails.model_validate(entry.get(\"data\", {}))",
          "",
          "        return cls(",
          "            title=error_event.get(\"title\"),",
          "            transaction_name=transaction_name,",
          "            exceptions=exceptions,",
          "            threads=threads,",
          "            breadcrumbs=breadcrumbs,",
          "            message=message,",
          "            stacktraces=stacktraces,",
          "            request=request,",
          "        )",
          "",
          "    def format_event(self):",
          "        exceptions = self.format_exceptions()",
          "        breadcrumbs = self.format_breadcrumbs()",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "        stacktraces = self.format_stacktraces()",
          "        request = self.format_request()",
          "",
          "        return (",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "            {title} {transaction}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            {breadcrumbs}",
          "            {request}",
          "            \"\"\"",
          "            )",
          "            .format(",
          "                title=self.title,",
          "                transaction=(",
          "                    f\"(occurred in: {self.transaction_name})\" if self.transaction_name else \"\"",
          "                ),",
          "                message=f\"\\n<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "                exceptions=(",
          "                    f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\"",
          "                ),",
          "                stacktraces=(",
          "                    f\"\\n<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "                ),",
          "                breadcrumbs=(",
          "                    f\"\\n<breadcrumb_logs>\\n{breadcrumbs}\\n</breadcrumb_logs>\"",
          "                    if breadcrumbs.strip()",
          "                    else \"\"",
          "                ),",
          "                request=f\"\\n<http_request>\\n{request}\\n</http_request>\" if request.strip() else \"\",",
          "            )",
          "            .strip()",
          "        )",
          "",
          "    def format_event_without_breadcrumbs(",
          "        self, include_context: bool = True, include_var_values: bool = True",
          "    ):",
          "        exceptions = self.format_exceptions(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        stacktraces = self.format_stacktraces(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {title}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            \"\"\"",
          "        ).format(",
          "            title=self.title,",
          "            exceptions=f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\",",
          "            stacktraces=(",
          "                f\"<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "            ),",
          "            message=f\"<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "        )",
          "",
          "    def format_exceptions(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <exception_{i}{handled}{exception_type}{exception_message}>",
          "                    {stacktrace}",
          "                    </exception{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                exception_type=f' type=\"{exception.type}\"' if exception.type else \"\",",
          "                exception_message=f' message=\"{exception.value}\"' if exception.value else \"\",",
          "                stacktrace=(",
          "                    exception.stacktrace.to_str(",
          "                        include_context=include_context,",
          "                        include_var_values=include_var_values,",
          "                    )",
          "                    if exception.stacktrace",
          "                    else \"\"",
          "                ),",
          "                handled=(",
          "                    f' is_exception_handled=\"{\"yes\" if exception.mechanism.get(\"handled\") else \"no\"}\"'",
          "                    if exception.mechanism and exception.mechanism.get(\"handled\", None) is not None",
          "                    else \"\"",
          "                ),",
          "            )",
          "            for i, exception in enumerate(self.exceptions)",
          "        )",
          "",
          "    def format_threads(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <thread_{thread_id} name=\"{thread_name}\" is_current=\"{thread_current}\" state=\"{thread_state}\" is_main=\"{thread_main}\" crashed=\"{thread_crashed}\">",
          "                    <stacktrace>",
          "                    {stacktrace}",
          "                    </stacktrace>",
          "                    </thread_{thread_id}>\"\"\"",
          "            ).format(",
          "                thread_id=thread.id,",
          "                thread_name=thread.name,",
          "                thread_state=thread.state,",
          "                thread_current=thread.current,",
          "                thread_crashed=thread.crashed,",
          "                thread_main=thread.main,",
          "                stacktrace=thread.stacktrace.to_str() if thread.stacktrace else \"\",",
          "            )",
          "            for thread in self.threads",
          "        )",
          "",
          "    def format_breadcrumbs(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <breadcrumb_{i}{breadcrumb_type}{breadcrumb_category}{level}>",
          "                {content}",
          "                </breadcrumb_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                breadcrumb_type=f' type=\"{breadcrumb.type}\"' if breadcrumb.type else \"\",",
          "                breadcrumb_category=(",
          "                    f' category=\"{breadcrumb.category}\"' if breadcrumb.category else \"\"",
          "                ),",
          "                content=\"\\n\".join(",
          "                    filter(",
          "                        None,",
          "                        [",
          "                            f\"{breadcrumb.message}\\n\" if breadcrumb.message else \"\",",
          "                            (",
          "                                f\"{str({k: v for k, v in breadcrumb.data.items() if v})}\\n\"",
          "                                if breadcrumb.data",
          "                                else \"\"",
          "                            ),",
          "                        ],",
          "                    )",
          "                ),",
          "                level=f' level=\"{breadcrumb.level}\"' if breadcrumb.level else \"\",",
          "            )",
          "            for i, breadcrumb in enumerate(self.breadcrumbs)",
          "        )",
          "",
          "    def format_stacktraces(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <stacktrace_{i}>",
          "                {stacktrace}",
          "                </stacktrace_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                stacktrace=stacktrace.to_str(",
          "                    include_context=include_context, include_var_values=include_var_values",
          "                ),",
          "            )",
          "            for i, stacktrace in enumerate(self.stacktraces)",
          "        )",
          "",
          "    def format_request(self):",
          "        if not self.request:",
          "            return \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {method} {url}",
          "            {data}",
          "            \"\"\"",
          "        ).format(",
          "            method=self.request.method if self.request.method else \"\",",
          "            url=self.request.url if self.request.url else \"\",",
          "            data=(",
          "                f\"Body:\\n{format_dict(self.request.data)}\"",
          "                if self.request.data and isinstance(self.request.data, dict)",
          "                else (",
          "                    f\"Body:\\n{format_list(self.request.data)}\"",
          "                    if self.request.data and isinstance(self.request.data, list)",
          "                    else (f\"Body:\\n{self.request.data}\" if self.request.data else \"\")",
          "                )",
          "            ),",
          "        )",
          "",
          "",
          "class IssueDetails(BaseModel):",
          "    id: Annotated[int, Examples(specialized.unsigned_ints)]",
          "    title: Annotated[str, Examples(specialized.ascii_words)]",
          "    short_id: Optional[str] = None",
          "    events: list[SentryEventData]",
          "",
          "",
          "class ProfileFrame(TypedDict):",
          "    function: str",
          "    module: str",
          "    filename: str",
          "    lineno: int",
          "    in_app: bool",
          "    duration_ns: NotRequired[float]",
          "    children: NotRequired[list[\"ProfileFrame\"]]",
          "",
          "",
          "class Profile(BaseModel):",
          "    profile_matches_issue: bool = Field(default=False)",
          "    execution_tree: list[ProfileFrame] = Field(default_factory=list)",
          "    relevant_functions: set[str] = Field(default_factory=set)",
          "",
          "    def format_profile(",
          "        self,",
          "        context_before: int = 20,",
          "        context_after: int = 3,",
          "    ) -> str:",
          "        \"\"\"",
          "        Format the profile tree, focusing on relevant functions from the stacktrace.",
          "",
          "        Args:",
          "            context_before: Number of lines to include before first relevant function",
          "            context_after: Number of lines to include after last relevant function",
          "",
          "        Returns:",
          "            str: Formatted profile string, showing relevant sections of the execution tree",
          "        \"\"\"",
          "        full_profile = self._format_profile_helper(self.execution_tree)",
          "",
          "        if self.relevant_functions:",
          "            relevant_window = self._get_relevant_code_window(",
          "                full_profile, context_before=context_before, context_after=context_after",
          "            )",
          "            if relevant_window:",
          "                return relevant_window",
          "",
          "        return full_profile",
          "",
          "    def _get_relevant_code_window(",
          "        self, code: str, context_before: int = 20, context_after: int = 3",
          "    ) -> str | None:",
          "        \"\"\"",
          "        Find the relevant section of code containing functions from the stacktrace.",
          "        Expands the selection to include context lines before and after.",
          "",
          "        Args:",
          "            code: Multi-line string of formatted profile to analyze",
          "            context_before: Number of lines to include before first relevant line",
          "            context_after: Number of lines to include after last relevant line",
          "",
          "        Returns:",
          "            str | None: Selected profile window with context, or None if no relevant functions found",
          "        \"\"\"",
          "        if not self.relevant_functions or not code:",
          "            return None",
          "",
          "        lines = code.splitlines()",
          "        first_relevant_line = None",
          "        last_relevant_line = None",
          "",
          "        # Find first and last lines containing relevant functions",
          "        for i, line in enumerate(lines):",
          "            if any(func in line for func in self.relevant_functions):",
          "                if first_relevant_line is None:",
          "                    first_relevant_line = i",
          "                last_relevant_line = i",
          "",
          "        if first_relevant_line is None:",
          "            first_relevant_line = 0",
          "        if last_relevant_line is None:",
          "            last_relevant_line = len(lines) - 1",
          "",
          "        # Calculate window boundaries with context",
          "        start_line = max(0, first_relevant_line - context_before)",
          "        end_line = min(len(lines), last_relevant_line + context_after + 1)",
          "",
          "        result = []",
          "        if start_line > 0:",
          "            result.append(\"...\")",
          "        result.extend(lines[start_line:end_line])",
          "        if end_line < len(lines):",
          "            result.append(\"...\")",
          "",
          "        return \"\\n\".join(result)",
          "",
          "    def _format_profile_helper(self, tree: list[ProfileFrame], prefix: str = \"\") -> str:",
          "        \"\"\"",
          "        Returns a pretty-printed string representation of the execution tree using tree formatting.",
          "",
          "        Args:",
          "            tree: List of dictionaries representing the execution tree",
          "            prefix: Current line prefix for tree structure",
          "",
          "        Returns:",
          "            str: Formatted string representation of the tree",
          "        \"\"\"",
          "        if not tree:",
          "            return \"\"",
          "",
          "        result = []",
          "",
          "        for i, node in enumerate(tree):",
          "            is_last = i == len(tree) - 1",
          "",
          "            # Format the current node",
          "            function_name = node.get(\"function\", \"Unknown function\")",
          "            filename = node.get(\"filename\", \"Unknown file\")",
          "            location_info = f\" ({filename}:{node.get('lineno', '')})\"",
          "            duration_ms = node.get(\"duration_ns\", -1) / 1_000_000",
          "            duration_info = f\" - {duration_ms:.0f}ms\" if duration_ms >= 0 else \"\"",
          "",
          "            # Add tree structure characters",
          "            if is_last:",
          "                result.append(f\"{prefix}└─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                result.append(f\"{prefix}├─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Recursively format children",
          "            if node.get(\"children\"):",
          "                children_str = self._format_profile_helper(node.get(\"children\", []), child_prefix)",
          "                if children_str:",
          "                    result.append(children_str)",
          "",
          "        return \"\\n\".join(result)",
          "",
          "",
          "class Span(BaseModel):",
          "    span_id: str",
          "    title: str | None = None",
          "    data: dict[str, Any] | None = None",
          "    duration: str | None = None",
          "    children: list[\"Span\"] = Field(default_factory=list)",
          "",
          "",
          "class TraceEvent(BaseModel):",
          "    event_id: str | None = None",
          "    title: str | None = None",
          "    is_transaction: bool = False",
          "    is_error: bool = False",
          "    platform: str | None = None",
          "    is_current_project: bool = True",
          "    project_slug: str | None = None",
          "    project_id: int | None = None",
          "    duration: str | None = None",
          "    profile_id: str | None = None",
          "    children: list[\"TraceEvent\"] = Field(default_factory=list)",
          "    spans: list[Span] = Field(default_factory=list)",
          "",
          "    def format_spans_tree(self) -> str:",
          "        \"\"\"",
          "        Returns a formatted string representation of the span tree.",
          "        \"\"\"",
          "        if not self.spans:",
          "            return \"No spans available\"",
          "",
          "        lines = [f\"Spans for {self.title or 'Unnamed Event'}\"]",
          "        self._format_spans(self.spans, \"\", lines)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_spans(self, spans: list[Span], prefix: str, lines: list[str]) -> None:",
          "        \"\"\"",
          "        Helper method to recursively format spans.",
          "",
          "        Args:",
          "            spans: List of spans to format",
          "            prefix: Current prefix for tree structure",
          "            lines: List of lines being built",
          "        \"\"\"",
          "        if not spans:",
          "            return",
          "",
          "        # Group consecutive similar spans",
          "        grouped_spans: list[list[Span]] = []",
          "        current_group: list[Span] | None = None",
          "",
          "        for span in spans:",
          "            if current_group and self._are_spans_similar(current_group[0], span):",
          "                current_group.append(span)",
          "            else:",
          "                if current_group:",
          "                    grouped_spans.append(current_group)",
          "                current_group = [span]",
          "",
          "        if current_group:",
          "            grouped_spans.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_spans):",
          "            is_last = i == len(grouped_spans) - 1",
          "            span = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted span line",
          "            span_line = self._format_span_line(span) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {span_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "                data_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {span_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "                data_prefix = f\"{prefix}│  \"",
          "",
          "            # Add the span data as JSON if available",
          "            if span.data:",
          "                data_str = json.dumps(span.data, indent=2)",
          "                data_lines = data_str.split(\"\\n\")",
          "                for data_line in data_lines:",
          "                    lines.append(f\"{data_prefix} {data_line}\")",
          "",
          "            # Process children",
          "            if span.children:",
          "                self._format_spans(span.children, child_prefix, lines)",
          "",
          "    def _format_span_line(self, span: Span) -> str:",
          "        \"\"\"Format a single span line.\"\"\"",
          "        parts = []",
          "",
          "        title = span.title or \"Unnamed Span\"",
          "        parts.append(title)",
          "        if span.duration:",
          "            parts.append(f\"({span.duration})\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_spans_similar(self, span1: Span, span2: Span) -> bool:",
          "        \"\"\"Check if two spans are similar enough to be grouped together.\"\"\"",
          "        if span1.title != span2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(span1.children) != len(span2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children are similar",
          "        for i in range(len(span1.children)):",
          "            if not self._are_spans_similar(span1.children[i], span2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "",
          "class TraceTree(BaseModel):",
          "    trace_id: str | None = None",
          "    org_id: int | None = None",
          "    events: list[TraceEvent] = Field(default_factory=list)  # only expecting transactions and errors",
          "",
          "    def format_trace_tree(self):",
          "        if not self.events:",
          "            return \"Trace (empty)\"",
          "",
          "        lines = [\"Trace\"]",
          "        self._format_events(self.events, \"\", lines, is_last_child=True)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_events(",
          "        self, events: list[TraceEvent], prefix: str, lines: list[str], is_last_child: bool",
          "    ):",
          "        if not events:",
          "            return",
          "",
          "        # Group consecutive similar events",
          "        grouped_events: list[list[TraceEvent]] = []",
          "        current_group: list[TraceEvent] | None = None",
          "",
          "        for event in events:",
          "            if current_group and self._are_events_similar(current_group[0], event):",
          "                current_group.append(event)",
          "            else:",
          "                if current_group:",
          "                    grouped_events.append(current_group)",
          "                current_group = [event]",
          "",
          "        if current_group:",
          "            grouped_events.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_events):",
          "            is_last = i == len(grouped_events) - 1",
          "            event = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted event line",
          "            event_line = self._format_event_line(event) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {event_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {event_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Process children",
          "            if event.children:",
          "                self._format_events(event.children, child_prefix, lines, is_last)",
          "",
          "    def _format_event_line(self, event: TraceEvent) -> str:",
          "        parts = []",
          "",
          "        # Add ERROR prefix if not a transaction",
          "        prefix = \"ERROR: \" if event.is_error else \"\"",
          "",
          "        # Add title",
          "        title = event.title or \"Unnamed Event\"",
          "        parts.append(f\"{prefix}{title}\")",
          "",
          "        # Add duration if it exists",
          "        if event.duration:",
          "            parts.append(f\"({event.duration})\")",
          "",
          "        # Add event_id (first 7 digits)",
          "        if event.event_id:",
          "            parts.append(f\"(event ID: {event.event_id[:7]})\")",
          "",
          "        # Add project",
          "        if event.project_slug and event.project_id:",
          "            project_str = f\"(project: {event.project_slug})\"",
          "            parts.append(project_str)",
          "",
          "        # Add platform",
          "        if event.platform:",
          "            parts.append(f\"({event.platform})\")",
          "",
          "        # Add profile",
          "        if event.profile_id:",
          "            parts.append(\"(profile available)\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_events_similar(self, event1: TraceEvent, event2: TraceEvent) -> bool:",
          "        \"\"\"Check if two events are similar enough to be grouped together\"\"\"",
          "        if event1.title != event2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(event1.children) != len(event2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children match",
          "        # This is a simplified check - for a full check we'd need to recursively compare children",
          "        for i in range(len(event1.children)):",
          "            if not self._are_events_similar(event1.children[i], event2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "    def get_full_event_id(self, truncated_id: str) -> str | None:",
          "        \"\"\"Return the full event_id given the first 7 characters\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event.event_id",
          "        return None",
          "",
          "    def get_event_by_id(self, truncated_id: str) -> TraceEvent | None:",
          "        \"\"\"Return the full TraceEvent object given a truncated event ID\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event",
          "        return None",
          "",
          "    def _get_all_events(self) -> list[TraceEvent]:",
          "        \"\"\"Return a flattened list of all events in the tree\"\"\"",
          "        all_events = []",
          "",
          "        def collect_events(events):",
          "            for event in events:",
          "                all_events.append(event)",
          "                if event.children:",
          "                    collect_events(event.children)",
          "",
          "        collect_events(self.events)",
          "        return all_events",
          "",
          "    def get_all_project_ids(self) -> list[int]:",
          "        \"\"\"Return a list of all project IDs in the trace tree\"\"\"",
          "        return list(set(event.project_id for event in self._get_all_events() if event.project_id))",
          "",
          "",
          "class RepoDefinition(BaseModel):",
          "    provider: Annotated[str, Examples((\"github\", \"integrations:github\"))]",
          "    owner: str",
          "    name: str",
          "    external_id: Annotated[str, Examples(specialized.ascii_words)]",
          "    branch_name: str | None = Field(",
          "        default=None,",
          "        description=\"The branch that Autofix will work on, otherwise the default branch will be used.\",",
          "    )",
          "    instructions: str | None = Field(",
          "        default=None,",
          "        description=\"Custom instructions when working in this repo.\",",
          "    )",
          "    base_commit_sha: str | None = None",
          "    provider_raw: str | None = None",
          "",
          "    @property",
          "    def full_name(self):",
          "        return f\"{self.owner}/{self.name}\"",
          "",
          "    @model_validator(mode=\"before\")",
          "    @classmethod",
          "    def store_provider_raw(cls, data):",
          "        if isinstance(data, dict) and \"provider\" in data and \"provider_raw\" not in data:",
          "            data[\"provider_raw\"] = data[\"provider\"]",
          "        return data",
          "",
          "    @field_validator(\"provider\", mode=\"after\")",
          "    @classmethod",
          "    def validate_provider(cls, provider: str):",
          "        return process_repo_provider(provider)",
          "",
          "    def __hash__(self):",
          "        return hash((self.provider, self.owner, self.name, self.external_id))",
          "",
          "",
          "class InitializationError(Exception):",
          "    pass",
          "",
          "",
          "class PromptXmlModel(BaseXmlModel):",
          "    def _pad_with_newlines(self, tree: ET.Element) -> None:",
          "        for elem in tree.iter():",
          "            if elem.text:",
          "                stripped = elem.text.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.text = \"\\n\" + stripped + \"\\n\"",
          "            if elem.tail:",
          "                stripped = elem.tail.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.tail = \"\\n\" + stripped + \"\\n\"",
          "",
          "    def to_prompt_str(self) -> str:",
          "        tree: ET.Element = self.to_xml_tree()",
          "",
          "        ET.indent(tree, space=\"\", level=0)",
          "",
          "        self._pad_with_newlines(tree)",
          "",
          "        return unescape_xml_chars(ET.tostring(tree, encoding=\"unicode\"))",
          "",
          "",
          "class Line(BaseModel):",
          "    source_line_no: Optional[int] = None",
          "    target_line_no: Optional[int] = None",
          "    diff_line_no: Optional[int] = None",
          "    value: str",
          "    line_type: Literal[\" \", \"+\", \"-\"]",
          "",
          "",
          "def raw_lines_to_lines(",
          "    lines: list[str],",
          "    source_start: int,",
          "    target_start: int,",
          "    target_line_to_extra: dict[int, str] | None = None,",
          ") -> list[Line]:",
          "    lines_after_header = lines[1:]",
          "    result = []",
          "    current_source_line = source_start",
          "    current_target_line = target_start",
          "    target_line_to_extra = target_line_to_extra or {}",
          "",
          "    for line in lines_after_header:",
          "        line_type = line[0]",
          "",
          "        if current_target_line in target_line_to_extra:",
          "            extra = target_line_to_extra[current_target_line]",
          "            line = line + extra",
          "",
          "        if line_type == \" \":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "            current_target_line += 1",
          "        elif line_type == \"+\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=None,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_target_line += 1",
          "        elif line_type == \"-\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=None,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "        elif line_type == \"\\\\\":",
          "            # Skip the \"\\ No newline at end of file\" marker",
          "            continue",
          "        else:",
          "            raise ValueError(f\"Invalid line type: {line_type}\")",
          "",
          "    return result",
          "",
          "",
          "def right_justified(min_num: int, max_num: int) -> list[str]:",
          "    max_digits = len(str(max_num))",
          "    return [f\"{number:>{max_digits}}\" for number in range(min_num, max_num + 1)]",
          "",
          "",
          "class Hunk(BaseModel):",
          "    source_start: int",
          "    source_length: int",
          "    target_start: int",
          "    target_length: int",
          "    section_header: str",
          "    lines: list[Line]",
          "",
          "    def raw(self) -> str:",
          "        \"\"\"",
          "        The raw hunk, like you see in `git diff` or the original patch string.",
          "        \"\"\"",
          "        return \"\\n\".join((self.section_header, *(line.value for line in self.lines)))",
          "",
          "    def annotated(",
          "        self,",
          "        max_digits_source: int | None = None,",
          "        max_digits_target: int | None = None,",
          "        source_target_delim: str = \"    \",",
          "    ) -> str:",
          "        \"\"\"",
          "        Hunk string with line numbers for the source and target, like you see in GitHub.",
          "        \"\"\"",
          "        if max_digits_source is None:",
          "            max_digits_source = len(str(self.lines[-1].source_line_no))",
          "        if max_digits_target is None:",
          "            max_digits_target = len(str(self.lines[-1].target_line_no))",
          "        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)",
          "        # +2 for the whitespace and then the line type character",
          "        return \"\\n\".join(",
          "            (",
          "                f\"{header_start}{self.section_header}\",",
          "                *(",
          "                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"",
          "                    for line in self.lines",
          "                ),",
          "            )",
          "        )",
          "",
          "",
          "def annotate_hunks(hunks: list[Hunk]) -> list[str]:",
          "    \"\"\"",
          "    Hunks annotated with line numbers for the source and target, like you see in GitHub.",
          "    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.",
          "    \"\"\"",
          "    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)",
          "    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)",
          "    return [",
          "        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)",
          "        for hunk in hunks",
          "    ]",
          "",
          "",
          "class FilePatch(BaseModel):",
          "    type: Literal[\"A\", \"M\", \"D\"]",
          "    path: str",
          "    added: int",
          "    removed: int",
          "    source_file: str",
          "    target_file: str",
          "    hunks: list[Hunk]",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.type == \"A\":",
          "            if file_contents is not None and file_contents.strip():",
          "                raise FileChangeError(\"Cannot add a file that already exists.\")",
          "            return self._apply_hunks([])",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for modify or delete operations.\")",
          "",
          "        if self.type == \"D\":",
          "            return None",
          "",
          "        # For M type",
          "        try:",
          "            new_contents = self._apply_hunks(file_contents.splitlines(keepends=True))",
          "        except Exception as e:",
          "            raise FileChangeError(f\"Error applying hunks: {e}\")",
          "",
          "        # Preserve any trailing characters from original",
          "        if file_contents:",
          "            trailing = file_contents[len(file_contents.rstrip()) :]",
          "            return new_contents + trailing",
          "",
          "        return new_contents",
          "",
          "    def _apply_hunks(self, lines: list[str]) -> str:",
          "        result = []",
          "        current_line = 0",
          "",
          "        for hunk in self.hunks:",
          "            # Add unchanged lines before the hunk",
          "            result.extend(lines[current_line : hunk.source_start - 1])",
          "            current_line = hunk.source_start - 1",
          "",
          "            for line in hunk.lines:",
          "                if line.line_type == \"+\":",
          "                    result.append(line.value + (\"\\n\" if not line.value.endswith(\"\\n\") else \"\"))",
          "                elif line.line_type == \" \":",
          "                    result.append(lines[current_line])",
          "                    current_line += 1",
          "                elif line.line_type == \"-\":",
          "                    current_line += 1",
          "",
          "        # Add any remaining unchanged lines after the last hunk",
          "        result.extend(lines[current_line:])",
          "",
          "        return \"\".join(result).rstrip(\"\\n\")",
          "",
          "    @staticmethod",
          "    def to_hunks(patch: str, target_line_to_extra: dict[int, str] | None = None) -> list[Hunk]:",
          "        hunk_header_pattern = r\"@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@\"",
          "",
          "        hunks: list[Hunk] = []",
          "        lines = patch.splitlines()",
          "        current_hunk = Hunk(",
          "            source_start=0,",
          "            source_length=0,",
          "            target_start=0,",
          "            target_length=0,",
          "            section_header=\"\",",
          "            lines=[],",
          "        )",
          "        current_lines: list[str] = []",
          "",
          "        for line in lines:",
          "            match = re.match(hunk_header_pattern, line)",
          "            if match:",
          "                if current_lines:",
          "                    current_hunk.lines = raw_lines_to_lines(",
          "                        current_lines,",
          "                        current_hunk.source_start,",
          "                        current_hunk.target_start,",
          "                        target_line_to_extra=target_line_to_extra,",
          "                    )",
          "                    hunks.append(current_hunk)",
          "                    current_lines = []",
          "                source_start, source_length, target_start, target_length = map(int, match.groups())",
          "                current_hunk = Hunk(",
          "                    source_start=source_start,",
          "                    source_length=source_length,",
          "                    target_start=target_start,",
          "                    target_length=target_length,",
          "                    section_header=line,",
          "                    lines=[],",
          "                )",
          "                current_lines = [line]  # starts with section header",
          "            elif current_lines:",
          "                current_lines.append(line)",
          "",
          "        if current_lines:",
          "            current_hunk.lines = raw_lines_to_lines(",
          "                current_lines,",
          "                current_hunk.source_start,",
          "                current_hunk.target_start,",
          "                target_line_to_extra=target_line_to_extra,",
          "            )",
          "            hunks.append(current_hunk)",
          "",
          "        return hunks",
          "",
          "",
          "class FileChangeError(Exception):",
          "    pass",
          "",
          "",
          "class FileChange(BaseModel):",
          "    change_type: Literal[\"create\", \"edit\", \"delete\"]",
          "    path: str",
          "    reference_snippet: Optional[str] = None",
          "    new_snippet: Optional[str] = None",
          "    description: Optional[str] = None",
          "    commit_message: Optional[str] = None",
          "    tool_call_id: Optional[str] = None",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.change_type == \"create\":",
          "            if file_contents is not None and file_contents != \"\":",
          "                raise FileChangeError(\"Cannot create a file that already exists.\")",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for creating a file.\")",
          "            return self.new_snippet",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for non-create operations.\")",
          "",
          "        if self.change_type == \"edit\":",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for editing a file.\")",
          "            if self.reference_snippet is None:",
          "                raise FileChangeError(\"Reference snippet must be provided for editing a file.\")",
          "            return file_contents.replace(self.reference_snippet, self.new_snippet)",
          "",
          "        # Delete",
          "        if self.reference_snippet is None:",
          "            return None",
          "",
          "        return file_contents.replace(self.reference_snippet, \"\")",
          "",
          "",
          "class SeerProjectPreference(BaseModel):",
          "    organization_id: int",
          "    project_id: int",
          "    repositories: list[RepoDefinition]",
          "",
          "    def to_db_model(self) -> DbSeerProjectPreference:",
          "        return DbSeerProjectPreference(",
          "            organization_id=self.organization_id,",
          "            project_id=self.project_id,",
          "            repositories=[repo.model_dump() for repo in self.repositories],",
          "        )",
          "",
          "    @classmethod",
          "    def from_db_model(cls, db_model: DbSeerProjectPreference) -> \"SeerProjectPreference\":",
          "        return cls(",
          "            organization_id=db_model.organization_id,",
          "            project_id=db_model.project_id,",
          "            repositories=db_model.repositories,",
          "        )",
          "",
          "",
          "class EAPTrace(BaseModel):",
          "    trace_id: str = Field(..., description=\"ID of the trace\")",
          "    trace: list[dict] = Field(..., description=\"List of spans in the trace\")",
          "",
          "    def _get_transaction_spans(self, trace: list[dict] | dict) -> list[dict]:",
          "        \"\"\"",
          "        Filters the trace to only include the transaction spans.",
          "        \"\"\"",
          "        if not trace:",
          "            return []",
          "",
          "        if isinstance(trace, list):",
          "            transaction_spans = []",
          "            for span in trace:",
          "                transaction_spans.extend(self._get_transaction_spans(span))",
          "            return transaction_spans",
          "",
          "        transaction_spans = []",
          "",
          "        if trace.get(\"is_transaction\"):",
          "            transaction_span = trace.copy()",
          "            transaction_span[\"children\"] = self._get_transaction_spans(trace.get(\"children\", []))",
          "            transaction_spans.append(transaction_span)",
          "        else:",
          "            transaction_spans.extend(self._get_transaction_spans(trace.get(\"children\", [])))",
          "",
          "        return transaction_spans",
          "",
          "    def get_and_format_trace(self, only_transactions=False) -> str:",
          "        \"\"\"",
          "        Formats the trace as a string of tags.",
          "        \"\"\"",
          "        trace = self.trace",
          "        if only_transactions:",
          "            trace = self._get_transaction_spans(self.trace)",
          "",
          "        def format_span_as_tag(span, depth=0):",
          "            indent = \"    \" * depth",
          "",
          "            attrs = []",
          "            for key, value in span.items():",
          "                # Ignore event_type since all events are marked as \"span\"",
          "                if key not in [\"children\", \"event_type\"]:",
          "                    attrs.append(f'{key}=\"{value}\"')",
          "            attrs_str = \" \".join(attrs)",
          "",
          "            tag_name = \"txn\" if span.get(\"is_transaction\") else \"span\"",
          "",
          "            if not span.get(\"children\"):",
          "                return f\"{indent}<{tag_name} {attrs_str} />\"",
          "            else:",
          "                tag_start = f\"{indent}<{tag_name} {attrs_str}>\"",
          "",
          "                # Format children recursively with increased depth",
          "                children = []",
          "                for child in span[\"children\"]:",
          "                    children.append(format_span_as_tag(child, depth + 1))",
          "",
          "                child_content = \"\\n\" + \"\\n\".join(children)",
          "                tag_end = f\"\\n{indent}</{tag_name}>\"",
          "",
          "                return f\"{tag_start}{child_content}{tag_end}\"",
          "",
          "        formatted = []",
          "        for span in trace:",
          "            formatted.append(format_span_as_tag(span, 0))",
          "",
          "        return \"\\n\".join(formatted)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/models.py",
      "image": "seer.automation.models",
      "is_application": true,
      "line": 1073,
      "name": "raw_lines_to_lines",
      "path": "/app/src/seer/automation/models.py",
      "codeContext": {
        "file": "seer/automation/models.py",
        "line": 1073,
        "name": "raw_lines_to_lines",
        "code": "                    line_type=line_type,\n                    source_line_no=current_source_line,\n                    target_line_no=current_target_line,\n                )\n            )\n            current_source_line += 1\n            current_target_line += 1\n        elif line_type == \"+\":\n            result.append(\n                Line(\n                    value=line,\n                    line_type=line_type,\n                    source_line_no=None,\n                    target_line_no=current_target_line,\n                )\n            )\n            current_target_line += 1\n        elif line_type == \"-\":\n            result.append(\n                Line(",
        "lineRange": {
          "start": 1064,
          "end": 1083
        },
        "lines": [
          "import json",
          "import re",
          "import textwrap",
          "from typing import Annotated, Any, Literal, NotRequired, Optional",
          "from xml.etree import ElementTree as ET",
          "",
          "import sentry_sdk",
          "from johen.examples import Examples",
          "from johen.generators import specialized",
          "from pydantic import (",
          "    AliasChoices,",
          "    AliasGenerator,",
          "    BaseModel,",
          "    ConfigDict,",
          "    Field,",
          "    ValidationError,",
          "    ValidationInfo,",
          "    field_validator,",
          "    model_validator,",
          ")",
          "from pydantic.alias_generators import to_camel, to_snake",
          "from pydantic_xml import BaseXmlModel",
          "from typing_extensions import TypedDict",
          "",
          "from seer.automation.utils import (",
          "    format_dict,",
          "    format_list,",
          "    process_repo_provider,",
          "    unescape_xml_chars,",
          ")",
          "from seer.db import DbSeerProjectPreference",
          "",
          "",
          "class StacktraceFrame(BaseModel):",
          "    model_config = ConfigDict(",
          "        alias_generator=AliasGenerator(",
          "            validation_alias=lambda k: AliasChoices(to_camel(k), to_snake(k)),",
          "            serialization_alias=to_camel,",
          "        )",
          "    )",
          "",
          "    function: Optional[Annotated[str, Examples(specialized.ascii_words)]] = None",
          "    filename: Optional[Annotated[str, Examples(specialized.file_names)]]",
          "    abs_path: Optional[Annotated[str, Examples(specialized.file_paths)]]",
          "    line_no: Optional[int]",
          "    col_no: Optional[int]",
          "    context: list[tuple[int, Optional[str]]] = []",
          "    repo_name: Optional[str] = None",
          "    in_app: bool | None = False",
          "    vars: Optional[dict[str, Any]] = None",
          "    package: Optional[str] = None",
          "",
          "    @field_validator(\"vars\", mode=\"before\")",
          "    @classmethod",
          "    def validate_vars(cls, vars: Optional[dict[str, Any]], info: ValidationInfo):",
          "        if not vars or \"context\" not in info.data or not info.data[\"context\"]:",
          "            return vars",
          "        code_str = \"\"",
          "        for _, line in info.data[\"context\"]:",
          "            code_str += line + \"\\n\"",
          "        return cls._trim_vars(vars, code_str)",
          "",
          "    @staticmethod",
          "    def _trim_vars(vars: dict[str, Any], code_context: str):",
          "        # only keep variables mentioned in the context of the stacktrace frame",
          "        # and filter out any values containing \"[Filtered]\"",
          "        trimmed_vars = {}",
          "        for key, val in vars.items():",
          "            if key in code_context:",
          "                if isinstance(val, (dict, list)):",
          "                    filtered_val = StacktraceFrame._filter_nested_value(val)",
          "                    if filtered_val is not None:",
          "                        trimmed_vars[key] = filtered_val",
          "                elif not StacktraceFrame._contains_filtered(val):",
          "                    trimmed_vars[key] = val",
          "        return trimmed_vars",
          "",
          "    @staticmethod",
          "    def _filter_nested_value(value: Any) -> Any:",
          "        if isinstance(value, dict):",
          "            filtered_dict = {}",
          "            for k, v in value.items():",
          "                if isinstance(v, (dict, list)):",
          "                    filtered_v = StacktraceFrame._filter_nested_value(v)",
          "                    if filtered_v is not None:",
          "                        filtered_dict[k] = filtered_v",
          "                elif not StacktraceFrame._contains_filtered(v):",
          "                    filtered_dict[k] = v",
          "            return filtered_dict if filtered_dict else None",
          "        elif isinstance(value, list):",
          "            filtered_list = []",
          "            for item in value:",
          "                if isinstance(item, (dict, list)):",
          "                    filtered_item = StacktraceFrame._filter_nested_value(item)",
          "                    if filtered_item is not None:",
          "                        filtered_list.append(filtered_item)",
          "                elif not StacktraceFrame._contains_filtered(item):",
          "                    filtered_list.append(item)",
          "            return filtered_list if filtered_list else None",
          "        return None if StacktraceFrame._contains_filtered(value) else value",
          "",
          "    @staticmethod",
          "    def _contains_filtered(value: Any) -> bool:",
          "        return isinstance(value, str) and \"[Filtered]\" in value",
          "",
          "",
          "class SentryFrame(TypedDict):",
          "    absPath: Optional[str]",
          "    colNo: Optional[int]",
          "    context: list[tuple[int, str]]",
          "    filename: NotRequired[Optional[str]]",
          "    function: NotRequired[Optional[str]]",
          "    inApp: NotRequired[bool]",
          "    instructionAddr: NotRequired[Optional[str]]",
          "    lineNo: NotRequired[Optional[int]]",
          "    module: NotRequired[Optional[str]]",
          "    package: NotRequired[Optional[str]]",
          "    platform: NotRequired[Optional[str]]",
          "    rawFunction: NotRequired[Optional[str]]",
          "    symbol: NotRequired[Optional[str]]",
          "    symbolAddr: NotRequired[Optional[str]]",
          "    trust: NotRequired[Optional[Any]]",
          "    vars: NotRequired[Optional[dict[str, Any]]]",
          "    addrMode: NotRequired[Optional[str]]",
          "    isPrefix: NotRequired[bool]",
          "    isSentinel: NotRequired[bool]",
          "    lock: NotRequired[Optional[Any]]",
          "    map: NotRequired[Optional[str]]",
          "    mapUrl: NotRequired[Optional[str]]",
          "    minGroupingLevel: NotRequired[int]",
          "    origAbsPath: NotRequired[Optional[str]]",
          "    sourceLink: NotRequired[Optional[str]]",
          "    symbolicatorStatus: NotRequired[Optional[Any]]",
          "",
          "",
          "class Stacktrace(BaseModel):",
          "    frames: list[StacktraceFrame]",
          "",
          "    @field_validator(\"frames\", mode=\"before\")",
          "    @classmethod",
          "    def validate_frames(cls, frames: list[StacktraceFrame | SentryFrame]):",
          "        stacktrace_frames = []",
          "        for frame in frames:",
          "            if isinstance(frame, dict):",
          "                if \"function\" not in frame:",
          "                    frame[\"function\"] = None",
          "                try:",
          "                    stacktrace_frames.append(StacktraceFrame.model_validate(frame))",
          "                except ValidationError:",
          "                    sentry_sdk.capture_exception()",
          "                    continue",
          "            else:",
          "                stacktrace_frames.append(frame)",
          "",
          "        return cls._trim_frames(stacktrace_frames)",
          "",
          "    def to_str(",
          "        self,",
          "        max_frames: int = 16,",
          "        in_app_only: bool = False,",
          "        include_context: bool = True,",
          "        include_var_values: bool = True,",
          "    ):",
          "        stack_str = \"\"",
          "",
          "        frames = self.frames",
          "        if in_app_only:",
          "            frames = [frame for frame in frames if frame.in_app]",
          "",
          "        for frame in reversed(frames[-max_frames:]):",
          "            col_no_str = f\", column {frame.col_no}\" if frame.col_no is not None else \"\"",
          "            repo_str = f\" in repo {frame.repo_name}\" if frame.repo_name else \"\"",
          "            line_no_str = (",
          "                f\"[Line {frame.line_no}{col_no_str}]\"",
          "                if frame.line_no is not None",
          "                else \"[Line: Unknown]\"",
          "            )",
          "",
          "            function = frame.function if frame.function else \"Unknown function\"",
          "            if frame.filename:",
          "                stack_str += f\" {function} in file {frame.filename}{repo_str} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            elif frame.package:",
          "                stack_str += f\" {function} in package {frame.package} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            else:",
          "                stack_str += f\" {function} in unknown file {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "",
          "            if include_context:",
          "                for ctx in frame.context:",
          "                    is_suspect_line = ctx[0] == frame.line_no",
          "                    stack_str += f\"{ctx[1]}{'  <-- SUSPECT LINE' if is_suspect_line else ''}\\n\"",
          "",
          "            if frame.vars:",
          "                if include_var_values:",
          "                    vars_title = \"Variable values at the time of the exception:\"",
          "                    vars_str = format_dict(frame.vars)",
          "                else:",
          "                    vars_title = \"Variables at the time of the exception:\"",
          "                    vars_str = \", \".join(frame.vars.keys())",
          "",
          "                stack_str += textwrap.dedent(",
          "                    \"\"\"\\",
          "                    ---",
          "                    {vars_title}:",
          "                    {vars_str}",
          "                    \"\"\"",
          "                ).format(vars_title=vars_title, vars_str=vars_str)",
          "            stack_str += \"------\\n\"",
          "",
          "        return stack_str",
          "",
          "    @staticmethod",
          "    def _trim_frames(frames: list[StacktraceFrame], frame_allowance=16):",
          "        frames_len = len(frames)",
          "        if frames_len <= frame_allowance:",
          "            return frames",
          "",
          "        app_frames = [frame for frame in frames if frame.in_app]",
          "        system_frames = [frame for frame in frames if not frame.in_app]",
          "",
          "        app_count = len(app_frames)",
          "        system_allowance = max(frame_allowance - app_count, 0)",
          "        app_allowance = frame_allowance - system_allowance",
          "",
          "        if system_allowance > 0:",
          "            # prioritize trimming system frames",
          "            half_system = system_allowance // 2",
          "            kept_system_frames = system_frames[:half_system] + system_frames[-half_system:]",
          "        else:",
          "            kept_system_frames = []",
          "",
          "        if app_allowance > 0:",
          "            half_app = app_allowance // 2",
          "            kept_app_frames = app_frames[:half_app] + app_frames[-half_app:]",
          "        else:",
          "            kept_app_frames = []",
          "",
          "        # combine and sort the kept frames based on their original order",
          "        kept_frames = kept_system_frames + kept_app_frames",
          "        kept_frames.sort(key=lambda frame: frames.index(frame))",
          "        return kept_frames",
          "",
          "",
          "class SentryStacktrace(TypedDict):",
          "    frames: list[SentryFrame]",
          "",
          "",
          "class SentryEventEntryDataValue(TypedDict):",
          "    type: str",
          "    value: str",
          "    stacktrace: SentryStacktrace",
          "",
          "",
          "class SentryExceptionEventData(TypedDict):",
          "    values: list[SentryEventEntryDataValue]",
          "",
          "",
          "class SentryExceptionEntry(BaseModel):",
          "    type: Literal[\"exception\"]",
          "    data: SentryExceptionEventData",
          "",
          "",
          "class SentryEventData(TypedDict):",
          "    title: str",
          "    entries: list[dict]",
          "    tags: NotRequired[list[dict[str, str | None]]]",
          "",
          "",
          "class ExceptionMechanism(TypedDict):",
          "    type: str",
          "    handled: NotRequired[bool]",
          "",
          "",
          "class ExceptionDetails(BaseModel):",
          "    type: Optional[str] = \"\"",
          "    value: Optional[str] = \"\"",
          "    stacktrace: Optional[Stacktrace] = None",
          "    mechanism: Optional[ExceptionMechanism] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class ThreadDetails(BaseModel):",
          "    id: Optional[int | str] = None",
          "    name: Optional[str] = None",
          "    crashed: Optional[bool] = False",
          "    current: Optional[bool] = False",
          "    state: Optional[str] = None",
          "    main: Optional[bool] = False",
          "",
          "    stacktrace: Optional[Stacktrace] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace | None):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class BreadcrumbsDetails(BaseModel):",
          "    type: Optional[str] = None",
          "    message: Optional[str] = None",
          "    category: Optional[str] = None",
          "    data: Optional[dict] = None",
          "    level: Optional[str] = None",
          "",
          "",
          "class RequestDetails(BaseModel):",
          "    url: str | None = None",
          "    method: str | None = None",
          "    data: dict[str, Any] | str | list[Any] | None = None",
          "    # not including cookies, headers, env, query, etc. for now",
          "",
          "",
          "class EventDetails(BaseModel):",
          "    title: str",
          "    message: str | None = None",
          "    transaction_name: str | None = None",
          "    exceptions: list[ExceptionDetails] = Field(default_factory=list, exclude=False)",
          "    threads: list[ThreadDetails] = Field(default_factory=list, exclude=False)",
          "    breadcrumbs: list[BreadcrumbsDetails] = Field(default_factory=list, exclude=False)",
          "    stacktraces: list[Stacktrace] = Field(default_factory=list, exclude=False)",
          "    request: RequestDetails | None = None",
          "",
          "    @classmethod",
          "    def from_event(cls, error_event: SentryEventData):",
          "        MAX_THREADS = 8  # TODO: Smarter logic for max threads",
          "",
          "        exceptions: list[ExceptionDetails] = []",
          "        threads: list[ThreadDetails] = []",
          "        breadcrumbs: list[BreadcrumbsDetails] = []",
          "        stacktraces: list[Stacktrace] = []",
          "        transaction_name: str | None = None",
          "        message: str | None = None",
          "        request: RequestDetails | None = None",
          "",
          "        for tag in error_event.get(\"tags\", []):",
          "            if tag.get(\"key\") == \"transaction\":",
          "                transaction_name = tag.get(\"value\")",
          "",
          "        for entry in error_event.get(\"entries\", []):",
          "            if entry.get(\"type\") == \"exception\":",
          "                for exception in entry.get(\"data\", {}).get(\"values\", []):",
          "                    exceptions.append(ExceptionDetails.model_validate(exception))",
          "            elif entry.get(\"type\") == \"threads\":",
          "                for thread in entry.get(\"data\", {}).get(\"values\", []):",
          "                    thread_details = ThreadDetails.model_validate(thread)",
          "                    if (",
          "                        thread_details.stacktrace",
          "                        and thread_details.stacktrace.frames",
          "                        and len(threads) < MAX_THREADS",
          "                    ):",
          "                        threads.append(thread_details)",
          "            elif entry.get(\"type\") == \"breadcrumbs\":",
          "                all_breadcrumbs = entry.get(\"data\", {}).get(\"values\", [])",
          "                for breadcrumb in all_breadcrumbs[-10:]:  # only look at the most recent breadcrumbs",
          "                    # Skip breadcrumbs with filtered content in message or data",
          "                    if StacktraceFrame._contains_filtered(",
          "                        breadcrumb.get(\"message\")",
          "                    ) or StacktraceFrame._contains_filtered(str(breadcrumb.get(\"data\"))):",
          "                        continue",
          "                    crumb_details = BreadcrumbsDetails.model_validate(breadcrumb)",
          "                    breadcrumbs.append(crumb_details)",
          "            elif entry.get(\"type\") == \"stacktrace\":",
          "                stacktraces.append(Stacktrace.model_validate(entry.get(\"data\", {})))",
          "            elif entry.get(\"type\") == \"message\":",
          "                message = entry.get(\"data\", {}).get(\"formatted\", None)",
          "            elif entry.get(\"type\") == \"request\":",
          "                request = RequestDetails.model_validate(entry.get(\"data\", {}))",
          "",
          "        return cls(",
          "            title=error_event.get(\"title\"),",
          "            transaction_name=transaction_name,",
          "            exceptions=exceptions,",
          "            threads=threads,",
          "            breadcrumbs=breadcrumbs,",
          "            message=message,",
          "            stacktraces=stacktraces,",
          "            request=request,",
          "        )",
          "",
          "    def format_event(self):",
          "        exceptions = self.format_exceptions()",
          "        breadcrumbs = self.format_breadcrumbs()",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "        stacktraces = self.format_stacktraces()",
          "        request = self.format_request()",
          "",
          "        return (",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "            {title} {transaction}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            {breadcrumbs}",
          "            {request}",
          "            \"\"\"",
          "            )",
          "            .format(",
          "                title=self.title,",
          "                transaction=(",
          "                    f\"(occurred in: {self.transaction_name})\" if self.transaction_name else \"\"",
          "                ),",
          "                message=f\"\\n<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "                exceptions=(",
          "                    f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\"",
          "                ),",
          "                stacktraces=(",
          "                    f\"\\n<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "                ),",
          "                breadcrumbs=(",
          "                    f\"\\n<breadcrumb_logs>\\n{breadcrumbs}\\n</breadcrumb_logs>\"",
          "                    if breadcrumbs.strip()",
          "                    else \"\"",
          "                ),",
          "                request=f\"\\n<http_request>\\n{request}\\n</http_request>\" if request.strip() else \"\",",
          "            )",
          "            .strip()",
          "        )",
          "",
          "    def format_event_without_breadcrumbs(",
          "        self, include_context: bool = True, include_var_values: bool = True",
          "    ):",
          "        exceptions = self.format_exceptions(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        stacktraces = self.format_stacktraces(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {title}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            \"\"\"",
          "        ).format(",
          "            title=self.title,",
          "            exceptions=f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\",",
          "            stacktraces=(",
          "                f\"<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "            ),",
          "            message=f\"<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "        )",
          "",
          "    def format_exceptions(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <exception_{i}{handled}{exception_type}{exception_message}>",
          "                    {stacktrace}",
          "                    </exception{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                exception_type=f' type=\"{exception.type}\"' if exception.type else \"\",",
          "                exception_message=f' message=\"{exception.value}\"' if exception.value else \"\",",
          "                stacktrace=(",
          "                    exception.stacktrace.to_str(",
          "                        include_context=include_context,",
          "                        include_var_values=include_var_values,",
          "                    )",
          "                    if exception.stacktrace",
          "                    else \"\"",
          "                ),",
          "                handled=(",
          "                    f' is_exception_handled=\"{\"yes\" if exception.mechanism.get(\"handled\") else \"no\"}\"'",
          "                    if exception.mechanism and exception.mechanism.get(\"handled\", None) is not None",
          "                    else \"\"",
          "                ),",
          "            )",
          "            for i, exception in enumerate(self.exceptions)",
          "        )",
          "",
          "    def format_threads(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <thread_{thread_id} name=\"{thread_name}\" is_current=\"{thread_current}\" state=\"{thread_state}\" is_main=\"{thread_main}\" crashed=\"{thread_crashed}\">",
          "                    <stacktrace>",
          "                    {stacktrace}",
          "                    </stacktrace>",
          "                    </thread_{thread_id}>\"\"\"",
          "            ).format(",
          "                thread_id=thread.id,",
          "                thread_name=thread.name,",
          "                thread_state=thread.state,",
          "                thread_current=thread.current,",
          "                thread_crashed=thread.crashed,",
          "                thread_main=thread.main,",
          "                stacktrace=thread.stacktrace.to_str() if thread.stacktrace else \"\",",
          "            )",
          "            for thread in self.threads",
          "        )",
          "",
          "    def format_breadcrumbs(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <breadcrumb_{i}{breadcrumb_type}{breadcrumb_category}{level}>",
          "                {content}",
          "                </breadcrumb_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                breadcrumb_type=f' type=\"{breadcrumb.type}\"' if breadcrumb.type else \"\",",
          "                breadcrumb_category=(",
          "                    f' category=\"{breadcrumb.category}\"' if breadcrumb.category else \"\"",
          "                ),",
          "                content=\"\\n\".join(",
          "                    filter(",
          "                        None,",
          "                        [",
          "                            f\"{breadcrumb.message}\\n\" if breadcrumb.message else \"\",",
          "                            (",
          "                                f\"{str({k: v for k, v in breadcrumb.data.items() if v})}\\n\"",
          "                                if breadcrumb.data",
          "                                else \"\"",
          "                            ),",
          "                        ],",
          "                    )",
          "                ),",
          "                level=f' level=\"{breadcrumb.level}\"' if breadcrumb.level else \"\",",
          "            )",
          "            for i, breadcrumb in enumerate(self.breadcrumbs)",
          "        )",
          "",
          "    def format_stacktraces(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <stacktrace_{i}>",
          "                {stacktrace}",
          "                </stacktrace_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                stacktrace=stacktrace.to_str(",
          "                    include_context=include_context, include_var_values=include_var_values",
          "                ),",
          "            )",
          "            for i, stacktrace in enumerate(self.stacktraces)",
          "        )",
          "",
          "    def format_request(self):",
          "        if not self.request:",
          "            return \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {method} {url}",
          "            {data}",
          "            \"\"\"",
          "        ).format(",
          "            method=self.request.method if self.request.method else \"\",",
          "            url=self.request.url if self.request.url else \"\",",
          "            data=(",
          "                f\"Body:\\n{format_dict(self.request.data)}\"",
          "                if self.request.data and isinstance(self.request.data, dict)",
          "                else (",
          "                    f\"Body:\\n{format_list(self.request.data)}\"",
          "                    if self.request.data and isinstance(self.request.data, list)",
          "                    else (f\"Body:\\n{self.request.data}\" if self.request.data else \"\")",
          "                )",
          "            ),",
          "        )",
          "",
          "",
          "class IssueDetails(BaseModel):",
          "    id: Annotated[int, Examples(specialized.unsigned_ints)]",
          "    title: Annotated[str, Examples(specialized.ascii_words)]",
          "    short_id: Optional[str] = None",
          "    events: list[SentryEventData]",
          "",
          "",
          "class ProfileFrame(TypedDict):",
          "    function: str",
          "    module: str",
          "    filename: str",
          "    lineno: int",
          "    in_app: bool",
          "    duration_ns: NotRequired[float]",
          "    children: NotRequired[list[\"ProfileFrame\"]]",
          "",
          "",
          "class Profile(BaseModel):",
          "    profile_matches_issue: bool = Field(default=False)",
          "    execution_tree: list[ProfileFrame] = Field(default_factory=list)",
          "    relevant_functions: set[str] = Field(default_factory=set)",
          "",
          "    def format_profile(",
          "        self,",
          "        context_before: int = 20,",
          "        context_after: int = 3,",
          "    ) -> str:",
          "        \"\"\"",
          "        Format the profile tree, focusing on relevant functions from the stacktrace.",
          "",
          "        Args:",
          "            context_before: Number of lines to include before first relevant function",
          "            context_after: Number of lines to include after last relevant function",
          "",
          "        Returns:",
          "            str: Formatted profile string, showing relevant sections of the execution tree",
          "        \"\"\"",
          "        full_profile = self._format_profile_helper(self.execution_tree)",
          "",
          "        if self.relevant_functions:",
          "            relevant_window = self._get_relevant_code_window(",
          "                full_profile, context_before=context_before, context_after=context_after",
          "            )",
          "            if relevant_window:",
          "                return relevant_window",
          "",
          "        return full_profile",
          "",
          "    def _get_relevant_code_window(",
          "        self, code: str, context_before: int = 20, context_after: int = 3",
          "    ) -> str | None:",
          "        \"\"\"",
          "        Find the relevant section of code containing functions from the stacktrace.",
          "        Expands the selection to include context lines before and after.",
          "",
          "        Args:",
          "            code: Multi-line string of formatted profile to analyze",
          "            context_before: Number of lines to include before first relevant line",
          "            context_after: Number of lines to include after last relevant line",
          "",
          "        Returns:",
          "            str | None: Selected profile window with context, or None if no relevant functions found",
          "        \"\"\"",
          "        if not self.relevant_functions or not code:",
          "            return None",
          "",
          "        lines = code.splitlines()",
          "        first_relevant_line = None",
          "        last_relevant_line = None",
          "",
          "        # Find first and last lines containing relevant functions",
          "        for i, line in enumerate(lines):",
          "            if any(func in line for func in self.relevant_functions):",
          "                if first_relevant_line is None:",
          "                    first_relevant_line = i",
          "                last_relevant_line = i",
          "",
          "        if first_relevant_line is None:",
          "            first_relevant_line = 0",
          "        if last_relevant_line is None:",
          "            last_relevant_line = len(lines) - 1",
          "",
          "        # Calculate window boundaries with context",
          "        start_line = max(0, first_relevant_line - context_before)",
          "        end_line = min(len(lines), last_relevant_line + context_after + 1)",
          "",
          "        result = []",
          "        if start_line > 0:",
          "            result.append(\"...\")",
          "        result.extend(lines[start_line:end_line])",
          "        if end_line < len(lines):",
          "            result.append(\"...\")",
          "",
          "        return \"\\n\".join(result)",
          "",
          "    def _format_profile_helper(self, tree: list[ProfileFrame], prefix: str = \"\") -> str:",
          "        \"\"\"",
          "        Returns a pretty-printed string representation of the execution tree using tree formatting.",
          "",
          "        Args:",
          "            tree: List of dictionaries representing the execution tree",
          "            prefix: Current line prefix for tree structure",
          "",
          "        Returns:",
          "            str: Formatted string representation of the tree",
          "        \"\"\"",
          "        if not tree:",
          "            return \"\"",
          "",
          "        result = []",
          "",
          "        for i, node in enumerate(tree):",
          "            is_last = i == len(tree) - 1",
          "",
          "            # Format the current node",
          "            function_name = node.get(\"function\", \"Unknown function\")",
          "            filename = node.get(\"filename\", \"Unknown file\")",
          "            location_info = f\" ({filename}:{node.get('lineno', '')})\"",
          "            duration_ms = node.get(\"duration_ns\", -1) / 1_000_000",
          "            duration_info = f\" - {duration_ms:.0f}ms\" if duration_ms >= 0 else \"\"",
          "",
          "            # Add tree structure characters",
          "            if is_last:",
          "                result.append(f\"{prefix}└─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                result.append(f\"{prefix}├─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Recursively format children",
          "            if node.get(\"children\"):",
          "                children_str = self._format_profile_helper(node.get(\"children\", []), child_prefix)",
          "                if children_str:",
          "                    result.append(children_str)",
          "",
          "        return \"\\n\".join(result)",
          "",
          "",
          "class Span(BaseModel):",
          "    span_id: str",
          "    title: str | None = None",
          "    data: dict[str, Any] | None = None",
          "    duration: str | None = None",
          "    children: list[\"Span\"] = Field(default_factory=list)",
          "",
          "",
          "class TraceEvent(BaseModel):",
          "    event_id: str | None = None",
          "    title: str | None = None",
          "    is_transaction: bool = False",
          "    is_error: bool = False",
          "    platform: str | None = None",
          "    is_current_project: bool = True",
          "    project_slug: str | None = None",
          "    project_id: int | None = None",
          "    duration: str | None = None",
          "    profile_id: str | None = None",
          "    children: list[\"TraceEvent\"] = Field(default_factory=list)",
          "    spans: list[Span] = Field(default_factory=list)",
          "",
          "    def format_spans_tree(self) -> str:",
          "        \"\"\"",
          "        Returns a formatted string representation of the span tree.",
          "        \"\"\"",
          "        if not self.spans:",
          "            return \"No spans available\"",
          "",
          "        lines = [f\"Spans for {self.title or 'Unnamed Event'}\"]",
          "        self._format_spans(self.spans, \"\", lines)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_spans(self, spans: list[Span], prefix: str, lines: list[str]) -> None:",
          "        \"\"\"",
          "        Helper method to recursively format spans.",
          "",
          "        Args:",
          "            spans: List of spans to format",
          "            prefix: Current prefix for tree structure",
          "            lines: List of lines being built",
          "        \"\"\"",
          "        if not spans:",
          "            return",
          "",
          "        # Group consecutive similar spans",
          "        grouped_spans: list[list[Span]] = []",
          "        current_group: list[Span] | None = None",
          "",
          "        for span in spans:",
          "            if current_group and self._are_spans_similar(current_group[0], span):",
          "                current_group.append(span)",
          "            else:",
          "                if current_group:",
          "                    grouped_spans.append(current_group)",
          "                current_group = [span]",
          "",
          "        if current_group:",
          "            grouped_spans.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_spans):",
          "            is_last = i == len(grouped_spans) - 1",
          "            span = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted span line",
          "            span_line = self._format_span_line(span) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {span_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "                data_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {span_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "                data_prefix = f\"{prefix}│  \"",
          "",
          "            # Add the span data as JSON if available",
          "            if span.data:",
          "                data_str = json.dumps(span.data, indent=2)",
          "                data_lines = data_str.split(\"\\n\")",
          "                for data_line in data_lines:",
          "                    lines.append(f\"{data_prefix} {data_line}\")",
          "",
          "            # Process children",
          "            if span.children:",
          "                self._format_spans(span.children, child_prefix, lines)",
          "",
          "    def _format_span_line(self, span: Span) -> str:",
          "        \"\"\"Format a single span line.\"\"\"",
          "        parts = []",
          "",
          "        title = span.title or \"Unnamed Span\"",
          "        parts.append(title)",
          "        if span.duration:",
          "            parts.append(f\"({span.duration})\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_spans_similar(self, span1: Span, span2: Span) -> bool:",
          "        \"\"\"Check if two spans are similar enough to be grouped together.\"\"\"",
          "        if span1.title != span2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(span1.children) != len(span2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children are similar",
          "        for i in range(len(span1.children)):",
          "            if not self._are_spans_similar(span1.children[i], span2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "",
          "class TraceTree(BaseModel):",
          "    trace_id: str | None = None",
          "    org_id: int | None = None",
          "    events: list[TraceEvent] = Field(default_factory=list)  # only expecting transactions and errors",
          "",
          "    def format_trace_tree(self):",
          "        if not self.events:",
          "            return \"Trace (empty)\"",
          "",
          "        lines = [\"Trace\"]",
          "        self._format_events(self.events, \"\", lines, is_last_child=True)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_events(",
          "        self, events: list[TraceEvent], prefix: str, lines: list[str], is_last_child: bool",
          "    ):",
          "        if not events:",
          "            return",
          "",
          "        # Group consecutive similar events",
          "        grouped_events: list[list[TraceEvent]] = []",
          "        current_group: list[TraceEvent] | None = None",
          "",
          "        for event in events:",
          "            if current_group and self._are_events_similar(current_group[0], event):",
          "                current_group.append(event)",
          "            else:",
          "                if current_group:",
          "                    grouped_events.append(current_group)",
          "                current_group = [event]",
          "",
          "        if current_group:",
          "            grouped_events.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_events):",
          "            is_last = i == len(grouped_events) - 1",
          "            event = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted event line",
          "            event_line = self._format_event_line(event) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {event_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {event_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Process children",
          "            if event.children:",
          "                self._format_events(event.children, child_prefix, lines, is_last)",
          "",
          "    def _format_event_line(self, event: TraceEvent) -> str:",
          "        parts = []",
          "",
          "        # Add ERROR prefix if not a transaction",
          "        prefix = \"ERROR: \" if event.is_error else \"\"",
          "",
          "        # Add title",
          "        title = event.title or \"Unnamed Event\"",
          "        parts.append(f\"{prefix}{title}\")",
          "",
          "        # Add duration if it exists",
          "        if event.duration:",
          "            parts.append(f\"({event.duration})\")",
          "",
          "        # Add event_id (first 7 digits)",
          "        if event.event_id:",
          "            parts.append(f\"(event ID: {event.event_id[:7]})\")",
          "",
          "        # Add project",
          "        if event.project_slug and event.project_id:",
          "            project_str = f\"(project: {event.project_slug})\"",
          "            parts.append(project_str)",
          "",
          "        # Add platform",
          "        if event.platform:",
          "            parts.append(f\"({event.platform})\")",
          "",
          "        # Add profile",
          "        if event.profile_id:",
          "            parts.append(\"(profile available)\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_events_similar(self, event1: TraceEvent, event2: TraceEvent) -> bool:",
          "        \"\"\"Check if two events are similar enough to be grouped together\"\"\"",
          "        if event1.title != event2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(event1.children) != len(event2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children match",
          "        # This is a simplified check - for a full check we'd need to recursively compare children",
          "        for i in range(len(event1.children)):",
          "            if not self._are_events_similar(event1.children[i], event2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "    def get_full_event_id(self, truncated_id: str) -> str | None:",
          "        \"\"\"Return the full event_id given the first 7 characters\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event.event_id",
          "        return None",
          "",
          "    def get_event_by_id(self, truncated_id: str) -> TraceEvent | None:",
          "        \"\"\"Return the full TraceEvent object given a truncated event ID\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event",
          "        return None",
          "",
          "    def _get_all_events(self) -> list[TraceEvent]:",
          "        \"\"\"Return a flattened list of all events in the tree\"\"\"",
          "        all_events = []",
          "",
          "        def collect_events(events):",
          "            for event in events:",
          "                all_events.append(event)",
          "                if event.children:",
          "                    collect_events(event.children)",
          "",
          "        collect_events(self.events)",
          "        return all_events",
          "",
          "    def get_all_project_ids(self) -> list[int]:",
          "        \"\"\"Return a list of all project IDs in the trace tree\"\"\"",
          "        return list(set(event.project_id for event in self._get_all_events() if event.project_id))",
          "",
          "",
          "class RepoDefinition(BaseModel):",
          "    provider: Annotated[str, Examples((\"github\", \"integrations:github\"))]",
          "    owner: str",
          "    name: str",
          "    external_id: Annotated[str, Examples(specialized.ascii_words)]",
          "    branch_name: str | None = Field(",
          "        default=None,",
          "        description=\"The branch that Autofix will work on, otherwise the default branch will be used.\",",
          "    )",
          "    instructions: str | None = Field(",
          "        default=None,",
          "        description=\"Custom instructions when working in this repo.\",",
          "    )",
          "    base_commit_sha: str | None = None",
          "    provider_raw: str | None = None",
          "",
          "    @property",
          "    def full_name(self):",
          "        return f\"{self.owner}/{self.name}\"",
          "",
          "    @model_validator(mode=\"before\")",
          "    @classmethod",
          "    def store_provider_raw(cls, data):",
          "        if isinstance(data, dict) and \"provider\" in data and \"provider_raw\" not in data:",
          "            data[\"provider_raw\"] = data[\"provider\"]",
          "        return data",
          "",
          "    @field_validator(\"provider\", mode=\"after\")",
          "    @classmethod",
          "    def validate_provider(cls, provider: str):",
          "        return process_repo_provider(provider)",
          "",
          "    def __hash__(self):",
          "        return hash((self.provider, self.owner, self.name, self.external_id))",
          "",
          "",
          "class InitializationError(Exception):",
          "    pass",
          "",
          "",
          "class PromptXmlModel(BaseXmlModel):",
          "    def _pad_with_newlines(self, tree: ET.Element) -> None:",
          "        for elem in tree.iter():",
          "            if elem.text:",
          "                stripped = elem.text.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.text = \"\\n\" + stripped + \"\\n\"",
          "            if elem.tail:",
          "                stripped = elem.tail.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.tail = \"\\n\" + stripped + \"\\n\"",
          "",
          "    def to_prompt_str(self) -> str:",
          "        tree: ET.Element = self.to_xml_tree()",
          "",
          "        ET.indent(tree, space=\"\", level=0)",
          "",
          "        self._pad_with_newlines(tree)",
          "",
          "        return unescape_xml_chars(ET.tostring(tree, encoding=\"unicode\"))",
          "",
          "",
          "class Line(BaseModel):",
          "    source_line_no: Optional[int] = None",
          "    target_line_no: Optional[int] = None",
          "    diff_line_no: Optional[int] = None",
          "    value: str",
          "    line_type: Literal[\" \", \"+\", \"-\"]",
          "",
          "",
          "def raw_lines_to_lines(",
          "    lines: list[str],",
          "    source_start: int,",
          "    target_start: int,",
          "    target_line_to_extra: dict[int, str] | None = None,",
          ") -> list[Line]:",
          "    lines_after_header = lines[1:]",
          "    result = []",
          "    current_source_line = source_start",
          "    current_target_line = target_start",
          "    target_line_to_extra = target_line_to_extra or {}",
          "",
          "    for line in lines_after_header:",
          "        line_type = line[0]",
          "",
          "        if current_target_line in target_line_to_extra:",
          "            extra = target_line_to_extra[current_target_line]",
          "            line = line + extra",
          "",
          "        if line_type == \" \":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "            current_target_line += 1",
          "        elif line_type == \"+\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=None,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_target_line += 1",
          "        elif line_type == \"-\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=None,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "        elif line_type == \"\\\\\":",
          "            # Skip the \"\\ No newline at end of file\" marker",
          "            continue",
          "        else:",
          "            raise ValueError(f\"Invalid line type: {line_type}\")",
          "",
          "    return result",
          "",
          "",
          "def right_justified(min_num: int, max_num: int) -> list[str]:",
          "    max_digits = len(str(max_num))",
          "    return [f\"{number:>{max_digits}}\" for number in range(min_num, max_num + 1)]",
          "",
          "",
          "class Hunk(BaseModel):",
          "    source_start: int",
          "    source_length: int",
          "    target_start: int",
          "    target_length: int",
          "    section_header: str",
          "    lines: list[Line]",
          "",
          "    def raw(self) -> str:",
          "        \"\"\"",
          "        The raw hunk, like you see in `git diff` or the original patch string.",
          "        \"\"\"",
          "        return \"\\n\".join((self.section_header, *(line.value for line in self.lines)))",
          "",
          "    def annotated(",
          "        self,",
          "        max_digits_source: int | None = None,",
          "        max_digits_target: int | None = None,",
          "        source_target_delim: str = \"    \",",
          "    ) -> str:",
          "        \"\"\"",
          "        Hunk string with line numbers for the source and target, like you see in GitHub.",
          "        \"\"\"",
          "        if max_digits_source is None:",
          "            max_digits_source = len(str(self.lines[-1].source_line_no))",
          "        if max_digits_target is None:",
          "            max_digits_target = len(str(self.lines[-1].target_line_no))",
          "        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)",
          "        # +2 for the whitespace and then the line type character",
          "        return \"\\n\".join(",
          "            (",
          "                f\"{header_start}{self.section_header}\",",
          "                *(",
          "                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"",
          "                    for line in self.lines",
          "                ),",
          "            )",
          "        )",
          "",
          "",
          "def annotate_hunks(hunks: list[Hunk]) -> list[str]:",
          "    \"\"\"",
          "    Hunks annotated with line numbers for the source and target, like you see in GitHub.",
          "    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.",
          "    \"\"\"",
          "    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)",
          "    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)",
          "    return [",
          "        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)",
          "        for hunk in hunks",
          "    ]",
          "",
          "",
          "class FilePatch(BaseModel):",
          "    type: Literal[\"A\", \"M\", \"D\"]",
          "    path: str",
          "    added: int",
          "    removed: int",
          "    source_file: str",
          "    target_file: str",
          "    hunks: list[Hunk]",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.type == \"A\":",
          "            if file_contents is not None and file_contents.strip():",
          "                raise FileChangeError(\"Cannot add a file that already exists.\")",
          "            return self._apply_hunks([])",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for modify or delete operations.\")",
          "",
          "        if self.type == \"D\":",
          "            return None",
          "",
          "        # For M type",
          "        try:",
          "            new_contents = self._apply_hunks(file_contents.splitlines(keepends=True))",
          "        except Exception as e:",
          "            raise FileChangeError(f\"Error applying hunks: {e}\")",
          "",
          "        # Preserve any trailing characters from original",
          "        if file_contents:",
          "            trailing = file_contents[len(file_contents.rstrip()) :]",
          "            return new_contents + trailing",
          "",
          "        return new_contents",
          "",
          "    def _apply_hunks(self, lines: list[str]) -> str:",
          "        result = []",
          "        current_line = 0",
          "",
          "        for hunk in self.hunks:",
          "            # Add unchanged lines before the hunk",
          "            result.extend(lines[current_line : hunk.source_start - 1])",
          "            current_line = hunk.source_start - 1",
          "",
          "            for line in hunk.lines:",
          "                if line.line_type == \"+\":",
          "                    result.append(line.value + (\"\\n\" if not line.value.endswith(\"\\n\") else \"\"))",
          "                elif line.line_type == \" \":",
          "                    result.append(lines[current_line])",
          "                    current_line += 1",
          "                elif line.line_type == \"-\":",
          "                    current_line += 1",
          "",
          "        # Add any remaining unchanged lines after the last hunk",
          "        result.extend(lines[current_line:])",
          "",
          "        return \"\".join(result).rstrip(\"\\n\")",
          "",
          "    @staticmethod",
          "    def to_hunks(patch: str, target_line_to_extra: dict[int, str] | None = None) -> list[Hunk]:",
          "        hunk_header_pattern = r\"@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@\"",
          "",
          "        hunks: list[Hunk] = []",
          "        lines = patch.splitlines()",
          "        current_hunk = Hunk(",
          "            source_start=0,",
          "            source_length=0,",
          "            target_start=0,",
          "            target_length=0,",
          "            section_header=\"\",",
          "            lines=[],",
          "        )",
          "        current_lines: list[str] = []",
          "",
          "        for line in lines:",
          "            match = re.match(hunk_header_pattern, line)",
          "            if match:",
          "                if current_lines:",
          "                    current_hunk.lines = raw_lines_to_lines(",
          "                        current_lines,",
          "                        current_hunk.source_start,",
          "                        current_hunk.target_start,",
          "                        target_line_to_extra=target_line_to_extra,",
          "                    )",
          "                    hunks.append(current_hunk)",
          "                    current_lines = []",
          "                source_start, source_length, target_start, target_length = map(int, match.groups())",
          "                current_hunk = Hunk(",
          "                    source_start=source_start,",
          "                    source_length=source_length,",
          "                    target_start=target_start,",
          "                    target_length=target_length,",
          "                    section_header=line,",
          "                    lines=[],",
          "                )",
          "                current_lines = [line]  # starts with section header",
          "            elif current_lines:",
          "                current_lines.append(line)",
          "",
          "        if current_lines:",
          "            current_hunk.lines = raw_lines_to_lines(",
          "                current_lines,",
          "                current_hunk.source_start,",
          "                current_hunk.target_start,",
          "                target_line_to_extra=target_line_to_extra,",
          "            )",
          "            hunks.append(current_hunk)",
          "",
          "        return hunks",
          "",
          "",
          "class FileChangeError(Exception):",
          "    pass",
          "",
          "",
          "class FileChange(BaseModel):",
          "    change_type: Literal[\"create\", \"edit\", \"delete\"]",
          "    path: str",
          "    reference_snippet: Optional[str] = None",
          "    new_snippet: Optional[str] = None",
          "    description: Optional[str] = None",
          "    commit_message: Optional[str] = None",
          "    tool_call_id: Optional[str] = None",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.change_type == \"create\":",
          "            if file_contents is not None and file_contents != \"\":",
          "                raise FileChangeError(\"Cannot create a file that already exists.\")",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for creating a file.\")",
          "            return self.new_snippet",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for non-create operations.\")",
          "",
          "        if self.change_type == \"edit\":",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for editing a file.\")",
          "            if self.reference_snippet is None:",
          "                raise FileChangeError(\"Reference snippet must be provided for editing a file.\")",
          "            return file_contents.replace(self.reference_snippet, self.new_snippet)",
          "",
          "        # Delete",
          "        if self.reference_snippet is None:",
          "            return None",
          "",
          "        return file_contents.replace(self.reference_snippet, \"\")",
          "",
          "",
          "class SeerProjectPreference(BaseModel):",
          "    organization_id: int",
          "    project_id: int",
          "    repositories: list[RepoDefinition]",
          "",
          "    def to_db_model(self) -> DbSeerProjectPreference:",
          "        return DbSeerProjectPreference(",
          "            organization_id=self.organization_id,",
          "            project_id=self.project_id,",
          "            repositories=[repo.model_dump() for repo in self.repositories],",
          "        )",
          "",
          "    @classmethod",
          "    def from_db_model(cls, db_model: DbSeerProjectPreference) -> \"SeerProjectPreference\":",
          "        return cls(",
          "            organization_id=db_model.organization_id,",
          "            project_id=db_model.project_id,",
          "            repositories=db_model.repositories,",
          "        )",
          "",
          "",
          "class EAPTrace(BaseModel):",
          "    trace_id: str = Field(..., description=\"ID of the trace\")",
          "    trace: list[dict] = Field(..., description=\"List of spans in the trace\")",
          "",
          "    def _get_transaction_spans(self, trace: list[dict] | dict) -> list[dict]:",
          "        \"\"\"",
          "        Filters the trace to only include the transaction spans.",
          "        \"\"\"",
          "        if not trace:",
          "            return []",
          "",
          "        if isinstance(trace, list):",
          "            transaction_spans = []",
          "            for span in trace:",
          "                transaction_spans.extend(self._get_transaction_spans(span))",
          "            return transaction_spans",
          "",
          "        transaction_spans = []",
          "",
          "        if trace.get(\"is_transaction\"):",
          "            transaction_span = trace.copy()",
          "            transaction_span[\"children\"] = self._get_transaction_spans(trace.get(\"children\", []))",
          "            transaction_spans.append(transaction_span)",
          "        else:",
          "            transaction_spans.extend(self._get_transaction_spans(trace.get(\"children\", [])))",
          "",
          "        return transaction_spans",
          "",
          "    def get_and_format_trace(self, only_transactions=False) -> str:",
          "        \"\"\"",
          "        Formats the trace as a string of tags.",
          "        \"\"\"",
          "        trace = self.trace",
          "        if only_transactions:",
          "            trace = self._get_transaction_spans(self.trace)",
          "",
          "        def format_span_as_tag(span, depth=0):",
          "            indent = \"    \" * depth",
          "",
          "            attrs = []",
          "            for key, value in span.items():",
          "                # Ignore event_type since all events are marked as \"span\"",
          "                if key not in [\"children\", \"event_type\"]:",
          "                    attrs.append(f'{key}=\"{value}\"')",
          "            attrs_str = \" \".join(attrs)",
          "",
          "            tag_name = \"txn\" if span.get(\"is_transaction\") else \"span\"",
          "",
          "            if not span.get(\"children\"):",
          "                return f\"{indent}<{tag_name} {attrs_str} />\"",
          "            else:",
          "                tag_start = f\"{indent}<{tag_name} {attrs_str}>\"",
          "",
          "                # Format children recursively with increased depth",
          "                children = []",
          "                for child in span[\"children\"]:",
          "                    children.append(format_span_as_tag(child, depth + 1))",
          "",
          "                child_content = \"\\n\" + \"\\n\".join(children)",
          "                tag_end = f\"\\n{indent}</{tag_name}>\"",
          "",
          "                return f\"{tag_start}{child_content}{tag_end}\"",
          "",
          "        formatted = []",
          "        for span in trace:",
          "            formatted.append(format_span_as_tag(span, 0))",
          "",
          "        return \"\\n\".join(formatted)",
          ""
        ]
      }
    },
    {
      "file": "pydantic/main.py",
      "image": "pydantic.main",
      "is_application": false,
      "line": 161,
      "name": "BaseModel.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/pydantic/main.py"
    },
    {
      "file": "seer/automation/models.py",
      "image": "seer.automation.models",
      "is_application": true,
      "line": 1152,
      "name": "annotate_hunks",
      "path": "/app/src/seer/automation/models.py",
      "codeContext": {
        "file": "seer/automation/models.py",
        "line": 1152,
        "name": "annotate_hunks",
        "code": "\n\ndef annotate_hunks(hunks: list[Hunk]) -> list[str]:\n    \"\"\"\n    Hunks annotated with line numbers for the source and target, like you see in GitHub.\n    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.\n    \"\"\"\n    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)\n    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)\n    return [\n        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)\n        for hunk in hunks\n    ]\n\n\nclass FilePatch(BaseModel):\n    type: Literal[\"A\", \"M\", \"D\"]\n    path: str\n    added: int\n    removed: int",
        "lineRange": {
          "start": 1143,
          "end": 1162
        },
        "lines": [
          "import json",
          "import re",
          "import textwrap",
          "from typing import Annotated, Any, Literal, NotRequired, Optional",
          "from xml.etree import ElementTree as ET",
          "",
          "import sentry_sdk",
          "from johen.examples import Examples",
          "from johen.generators import specialized",
          "from pydantic import (",
          "    AliasChoices,",
          "    AliasGenerator,",
          "    BaseModel,",
          "    ConfigDict,",
          "    Field,",
          "    ValidationError,",
          "    ValidationInfo,",
          "    field_validator,",
          "    model_validator,",
          ")",
          "from pydantic.alias_generators import to_camel, to_snake",
          "from pydantic_xml import BaseXmlModel",
          "from typing_extensions import TypedDict",
          "",
          "from seer.automation.utils import (",
          "    format_dict,",
          "    format_list,",
          "    process_repo_provider,",
          "    unescape_xml_chars,",
          ")",
          "from seer.db import DbSeerProjectPreference",
          "",
          "",
          "class StacktraceFrame(BaseModel):",
          "    model_config = ConfigDict(",
          "        alias_generator=AliasGenerator(",
          "            validation_alias=lambda k: AliasChoices(to_camel(k), to_snake(k)),",
          "            serialization_alias=to_camel,",
          "        )",
          "    )",
          "",
          "    function: Optional[Annotated[str, Examples(specialized.ascii_words)]] = None",
          "    filename: Optional[Annotated[str, Examples(specialized.file_names)]]",
          "    abs_path: Optional[Annotated[str, Examples(specialized.file_paths)]]",
          "    line_no: Optional[int]",
          "    col_no: Optional[int]",
          "    context: list[tuple[int, Optional[str]]] = []",
          "    repo_name: Optional[str] = None",
          "    in_app: bool | None = False",
          "    vars: Optional[dict[str, Any]] = None",
          "    package: Optional[str] = None",
          "",
          "    @field_validator(\"vars\", mode=\"before\")",
          "    @classmethod",
          "    def validate_vars(cls, vars: Optional[dict[str, Any]], info: ValidationInfo):",
          "        if not vars or \"context\" not in info.data or not info.data[\"context\"]:",
          "            return vars",
          "        code_str = \"\"",
          "        for _, line in info.data[\"context\"]:",
          "            code_str += line + \"\\n\"",
          "        return cls._trim_vars(vars, code_str)",
          "",
          "    @staticmethod",
          "    def _trim_vars(vars: dict[str, Any], code_context: str):",
          "        # only keep variables mentioned in the context of the stacktrace frame",
          "        # and filter out any values containing \"[Filtered]\"",
          "        trimmed_vars = {}",
          "        for key, val in vars.items():",
          "            if key in code_context:",
          "                if isinstance(val, (dict, list)):",
          "                    filtered_val = StacktraceFrame._filter_nested_value(val)",
          "                    if filtered_val is not None:",
          "                        trimmed_vars[key] = filtered_val",
          "                elif not StacktraceFrame._contains_filtered(val):",
          "                    trimmed_vars[key] = val",
          "        return trimmed_vars",
          "",
          "    @staticmethod",
          "    def _filter_nested_value(value: Any) -> Any:",
          "        if isinstance(value, dict):",
          "            filtered_dict = {}",
          "            for k, v in value.items():",
          "                if isinstance(v, (dict, list)):",
          "                    filtered_v = StacktraceFrame._filter_nested_value(v)",
          "                    if filtered_v is not None:",
          "                        filtered_dict[k] = filtered_v",
          "                elif not StacktraceFrame._contains_filtered(v):",
          "                    filtered_dict[k] = v",
          "            return filtered_dict if filtered_dict else None",
          "        elif isinstance(value, list):",
          "            filtered_list = []",
          "            for item in value:",
          "                if isinstance(item, (dict, list)):",
          "                    filtered_item = StacktraceFrame._filter_nested_value(item)",
          "                    if filtered_item is not None:",
          "                        filtered_list.append(filtered_item)",
          "                elif not StacktraceFrame._contains_filtered(item):",
          "                    filtered_list.append(item)",
          "            return filtered_list if filtered_list else None",
          "        return None if StacktraceFrame._contains_filtered(value) else value",
          "",
          "    @staticmethod",
          "    def _contains_filtered(value: Any) -> bool:",
          "        return isinstance(value, str) and \"[Filtered]\" in value",
          "",
          "",
          "class SentryFrame(TypedDict):",
          "    absPath: Optional[str]",
          "    colNo: Optional[int]",
          "    context: list[tuple[int, str]]",
          "    filename: NotRequired[Optional[str]]",
          "    function: NotRequired[Optional[str]]",
          "    inApp: NotRequired[bool]",
          "    instructionAddr: NotRequired[Optional[str]]",
          "    lineNo: NotRequired[Optional[int]]",
          "    module: NotRequired[Optional[str]]",
          "    package: NotRequired[Optional[str]]",
          "    platform: NotRequired[Optional[str]]",
          "    rawFunction: NotRequired[Optional[str]]",
          "    symbol: NotRequired[Optional[str]]",
          "    symbolAddr: NotRequired[Optional[str]]",
          "    trust: NotRequired[Optional[Any]]",
          "    vars: NotRequired[Optional[dict[str, Any]]]",
          "    addrMode: NotRequired[Optional[str]]",
          "    isPrefix: NotRequired[bool]",
          "    isSentinel: NotRequired[bool]",
          "    lock: NotRequired[Optional[Any]]",
          "    map: NotRequired[Optional[str]]",
          "    mapUrl: NotRequired[Optional[str]]",
          "    minGroupingLevel: NotRequired[int]",
          "    origAbsPath: NotRequired[Optional[str]]",
          "    sourceLink: NotRequired[Optional[str]]",
          "    symbolicatorStatus: NotRequired[Optional[Any]]",
          "",
          "",
          "class Stacktrace(BaseModel):",
          "    frames: list[StacktraceFrame]",
          "",
          "    @field_validator(\"frames\", mode=\"before\")",
          "    @classmethod",
          "    def validate_frames(cls, frames: list[StacktraceFrame | SentryFrame]):",
          "        stacktrace_frames = []",
          "        for frame in frames:",
          "            if isinstance(frame, dict):",
          "                if \"function\" not in frame:",
          "                    frame[\"function\"] = None",
          "                try:",
          "                    stacktrace_frames.append(StacktraceFrame.model_validate(frame))",
          "                except ValidationError:",
          "                    sentry_sdk.capture_exception()",
          "                    continue",
          "            else:",
          "                stacktrace_frames.append(frame)",
          "",
          "        return cls._trim_frames(stacktrace_frames)",
          "",
          "    def to_str(",
          "        self,",
          "        max_frames: int = 16,",
          "        in_app_only: bool = False,",
          "        include_context: bool = True,",
          "        include_var_values: bool = True,",
          "    ):",
          "        stack_str = \"\"",
          "",
          "        frames = self.frames",
          "        if in_app_only:",
          "            frames = [frame for frame in frames if frame.in_app]",
          "",
          "        for frame in reversed(frames[-max_frames:]):",
          "            col_no_str = f\", column {frame.col_no}\" if frame.col_no is not None else \"\"",
          "            repo_str = f\" in repo {frame.repo_name}\" if frame.repo_name else \"\"",
          "            line_no_str = (",
          "                f\"[Line {frame.line_no}{col_no_str}]\"",
          "                if frame.line_no is not None",
          "                else \"[Line: Unknown]\"",
          "            )",
          "",
          "            function = frame.function if frame.function else \"Unknown function\"",
          "            if frame.filename:",
          "                stack_str += f\" {function} in file {frame.filename}{repo_str} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            elif frame.package:",
          "                stack_str += f\" {function} in package {frame.package} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            else:",
          "                stack_str += f\" {function} in unknown file {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "",
          "            if include_context:",
          "                for ctx in frame.context:",
          "                    is_suspect_line = ctx[0] == frame.line_no",
          "                    stack_str += f\"{ctx[1]}{'  <-- SUSPECT LINE' if is_suspect_line else ''}\\n\"",
          "",
          "            if frame.vars:",
          "                if include_var_values:",
          "                    vars_title = \"Variable values at the time of the exception:\"",
          "                    vars_str = format_dict(frame.vars)",
          "                else:",
          "                    vars_title = \"Variables at the time of the exception:\"",
          "                    vars_str = \", \".join(frame.vars.keys())",
          "",
          "                stack_str += textwrap.dedent(",
          "                    \"\"\"\\",
          "                    ---",
          "                    {vars_title}:",
          "                    {vars_str}",
          "                    \"\"\"",
          "                ).format(vars_title=vars_title, vars_str=vars_str)",
          "            stack_str += \"------\\n\"",
          "",
          "        return stack_str",
          "",
          "    @staticmethod",
          "    def _trim_frames(frames: list[StacktraceFrame], frame_allowance=16):",
          "        frames_len = len(frames)",
          "        if frames_len <= frame_allowance:",
          "            return frames",
          "",
          "        app_frames = [frame for frame in frames if frame.in_app]",
          "        system_frames = [frame for frame in frames if not frame.in_app]",
          "",
          "        app_count = len(app_frames)",
          "        system_allowance = max(frame_allowance - app_count, 0)",
          "        app_allowance = frame_allowance - system_allowance",
          "",
          "        if system_allowance > 0:",
          "            # prioritize trimming system frames",
          "            half_system = system_allowance // 2",
          "            kept_system_frames = system_frames[:half_system] + system_frames[-half_system:]",
          "        else:",
          "            kept_system_frames = []",
          "",
          "        if app_allowance > 0:",
          "            half_app = app_allowance // 2",
          "            kept_app_frames = app_frames[:half_app] + app_frames[-half_app:]",
          "        else:",
          "            kept_app_frames = []",
          "",
          "        # combine and sort the kept frames based on their original order",
          "        kept_frames = kept_system_frames + kept_app_frames",
          "        kept_frames.sort(key=lambda frame: frames.index(frame))",
          "        return kept_frames",
          "",
          "",
          "class SentryStacktrace(TypedDict):",
          "    frames: list[SentryFrame]",
          "",
          "",
          "class SentryEventEntryDataValue(TypedDict):",
          "    type: str",
          "    value: str",
          "    stacktrace: SentryStacktrace",
          "",
          "",
          "class SentryExceptionEventData(TypedDict):",
          "    values: list[SentryEventEntryDataValue]",
          "",
          "",
          "class SentryExceptionEntry(BaseModel):",
          "    type: Literal[\"exception\"]",
          "    data: SentryExceptionEventData",
          "",
          "",
          "class SentryEventData(TypedDict):",
          "    title: str",
          "    entries: list[dict]",
          "    tags: NotRequired[list[dict[str, str | None]]]",
          "",
          "",
          "class ExceptionMechanism(TypedDict):",
          "    type: str",
          "    handled: NotRequired[bool]",
          "",
          "",
          "class ExceptionDetails(BaseModel):",
          "    type: Optional[str] = \"\"",
          "    value: Optional[str] = \"\"",
          "    stacktrace: Optional[Stacktrace] = None",
          "    mechanism: Optional[ExceptionMechanism] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class ThreadDetails(BaseModel):",
          "    id: Optional[int | str] = None",
          "    name: Optional[str] = None",
          "    crashed: Optional[bool] = False",
          "    current: Optional[bool] = False",
          "    state: Optional[str] = None",
          "    main: Optional[bool] = False",
          "",
          "    stacktrace: Optional[Stacktrace] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace | None):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class BreadcrumbsDetails(BaseModel):",
          "    type: Optional[str] = None",
          "    message: Optional[str] = None",
          "    category: Optional[str] = None",
          "    data: Optional[dict] = None",
          "    level: Optional[str] = None",
          "",
          "",
          "class RequestDetails(BaseModel):",
          "    url: str | None = None",
          "    method: str | None = None",
          "    data: dict[str, Any] | str | list[Any] | None = None",
          "    # not including cookies, headers, env, query, etc. for now",
          "",
          "",
          "class EventDetails(BaseModel):",
          "    title: str",
          "    message: str | None = None",
          "    transaction_name: str | None = None",
          "    exceptions: list[ExceptionDetails] = Field(default_factory=list, exclude=False)",
          "    threads: list[ThreadDetails] = Field(default_factory=list, exclude=False)",
          "    breadcrumbs: list[BreadcrumbsDetails] = Field(default_factory=list, exclude=False)",
          "    stacktraces: list[Stacktrace] = Field(default_factory=list, exclude=False)",
          "    request: RequestDetails | None = None",
          "",
          "    @classmethod",
          "    def from_event(cls, error_event: SentryEventData):",
          "        MAX_THREADS = 8  # TODO: Smarter logic for max threads",
          "",
          "        exceptions: list[ExceptionDetails] = []",
          "        threads: list[ThreadDetails] = []",
          "        breadcrumbs: list[BreadcrumbsDetails] = []",
          "        stacktraces: list[Stacktrace] = []",
          "        transaction_name: str | None = None",
          "        message: str | None = None",
          "        request: RequestDetails | None = None",
          "",
          "        for tag in error_event.get(\"tags\", []):",
          "            if tag.get(\"key\") == \"transaction\":",
          "                transaction_name = tag.get(\"value\")",
          "",
          "        for entry in error_event.get(\"entries\", []):",
          "            if entry.get(\"type\") == \"exception\":",
          "                for exception in entry.get(\"data\", {}).get(\"values\", []):",
          "                    exceptions.append(ExceptionDetails.model_validate(exception))",
          "            elif entry.get(\"type\") == \"threads\":",
          "                for thread in entry.get(\"data\", {}).get(\"values\", []):",
          "                    thread_details = ThreadDetails.model_validate(thread)",
          "                    if (",
          "                        thread_details.stacktrace",
          "                        and thread_details.stacktrace.frames",
          "                        and len(threads) < MAX_THREADS",
          "                    ):",
          "                        threads.append(thread_details)",
          "            elif entry.get(\"type\") == \"breadcrumbs\":",
          "                all_breadcrumbs = entry.get(\"data\", {}).get(\"values\", [])",
          "                for breadcrumb in all_breadcrumbs[-10:]:  # only look at the most recent breadcrumbs",
          "                    # Skip breadcrumbs with filtered content in message or data",
          "                    if StacktraceFrame._contains_filtered(",
          "                        breadcrumb.get(\"message\")",
          "                    ) or StacktraceFrame._contains_filtered(str(breadcrumb.get(\"data\"))):",
          "                        continue",
          "                    crumb_details = BreadcrumbsDetails.model_validate(breadcrumb)",
          "                    breadcrumbs.append(crumb_details)",
          "            elif entry.get(\"type\") == \"stacktrace\":",
          "                stacktraces.append(Stacktrace.model_validate(entry.get(\"data\", {})))",
          "            elif entry.get(\"type\") == \"message\":",
          "                message = entry.get(\"data\", {}).get(\"formatted\", None)",
          "            elif entry.get(\"type\") == \"request\":",
          "                request = RequestDetails.model_validate(entry.get(\"data\", {}))",
          "",
          "        return cls(",
          "            title=error_event.get(\"title\"),",
          "            transaction_name=transaction_name,",
          "            exceptions=exceptions,",
          "            threads=threads,",
          "            breadcrumbs=breadcrumbs,",
          "            message=message,",
          "            stacktraces=stacktraces,",
          "            request=request,",
          "        )",
          "",
          "    def format_event(self):",
          "        exceptions = self.format_exceptions()",
          "        breadcrumbs = self.format_breadcrumbs()",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "        stacktraces = self.format_stacktraces()",
          "        request = self.format_request()",
          "",
          "        return (",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "            {title} {transaction}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            {breadcrumbs}",
          "            {request}",
          "            \"\"\"",
          "            )",
          "            .format(",
          "                title=self.title,",
          "                transaction=(",
          "                    f\"(occurred in: {self.transaction_name})\" if self.transaction_name else \"\"",
          "                ),",
          "                message=f\"\\n<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "                exceptions=(",
          "                    f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\"",
          "                ),",
          "                stacktraces=(",
          "                    f\"\\n<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "                ),",
          "                breadcrumbs=(",
          "                    f\"\\n<breadcrumb_logs>\\n{breadcrumbs}\\n</breadcrumb_logs>\"",
          "                    if breadcrumbs.strip()",
          "                    else \"\"",
          "                ),",
          "                request=f\"\\n<http_request>\\n{request}\\n</http_request>\" if request.strip() else \"\",",
          "            )",
          "            .strip()",
          "        )",
          "",
          "    def format_event_without_breadcrumbs(",
          "        self, include_context: bool = True, include_var_values: bool = True",
          "    ):",
          "        exceptions = self.format_exceptions(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        stacktraces = self.format_stacktraces(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {title}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            \"\"\"",
          "        ).format(",
          "            title=self.title,",
          "            exceptions=f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\",",
          "            stacktraces=(",
          "                f\"<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "            ),",
          "            message=f\"<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "        )",
          "",
          "    def format_exceptions(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <exception_{i}{handled}{exception_type}{exception_message}>",
          "                    {stacktrace}",
          "                    </exception{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                exception_type=f' type=\"{exception.type}\"' if exception.type else \"\",",
          "                exception_message=f' message=\"{exception.value}\"' if exception.value else \"\",",
          "                stacktrace=(",
          "                    exception.stacktrace.to_str(",
          "                        include_context=include_context,",
          "                        include_var_values=include_var_values,",
          "                    )",
          "                    if exception.stacktrace",
          "                    else \"\"",
          "                ),",
          "                handled=(",
          "                    f' is_exception_handled=\"{\"yes\" if exception.mechanism.get(\"handled\") else \"no\"}\"'",
          "                    if exception.mechanism and exception.mechanism.get(\"handled\", None) is not None",
          "                    else \"\"",
          "                ),",
          "            )",
          "            for i, exception in enumerate(self.exceptions)",
          "        )",
          "",
          "    def format_threads(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <thread_{thread_id} name=\"{thread_name}\" is_current=\"{thread_current}\" state=\"{thread_state}\" is_main=\"{thread_main}\" crashed=\"{thread_crashed}\">",
          "                    <stacktrace>",
          "                    {stacktrace}",
          "                    </stacktrace>",
          "                    </thread_{thread_id}>\"\"\"",
          "            ).format(",
          "                thread_id=thread.id,",
          "                thread_name=thread.name,",
          "                thread_state=thread.state,",
          "                thread_current=thread.current,",
          "                thread_crashed=thread.crashed,",
          "                thread_main=thread.main,",
          "                stacktrace=thread.stacktrace.to_str() if thread.stacktrace else \"\",",
          "            )",
          "            for thread in self.threads",
          "        )",
          "",
          "    def format_breadcrumbs(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <breadcrumb_{i}{breadcrumb_type}{breadcrumb_category}{level}>",
          "                {content}",
          "                </breadcrumb_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                breadcrumb_type=f' type=\"{breadcrumb.type}\"' if breadcrumb.type else \"\",",
          "                breadcrumb_category=(",
          "                    f' category=\"{breadcrumb.category}\"' if breadcrumb.category else \"\"",
          "                ),",
          "                content=\"\\n\".join(",
          "                    filter(",
          "                        None,",
          "                        [",
          "                            f\"{breadcrumb.message}\\n\" if breadcrumb.message else \"\",",
          "                            (",
          "                                f\"{str({k: v for k, v in breadcrumb.data.items() if v})}\\n\"",
          "                                if breadcrumb.data",
          "                                else \"\"",
          "                            ),",
          "                        ],",
          "                    )",
          "                ),",
          "                level=f' level=\"{breadcrumb.level}\"' if breadcrumb.level else \"\",",
          "            )",
          "            for i, breadcrumb in enumerate(self.breadcrumbs)",
          "        )",
          "",
          "    def format_stacktraces(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <stacktrace_{i}>",
          "                {stacktrace}",
          "                </stacktrace_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                stacktrace=stacktrace.to_str(",
          "                    include_context=include_context, include_var_values=include_var_values",
          "                ),",
          "            )",
          "            for i, stacktrace in enumerate(self.stacktraces)",
          "        )",
          "",
          "    def format_request(self):",
          "        if not self.request:",
          "            return \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {method} {url}",
          "            {data}",
          "            \"\"\"",
          "        ).format(",
          "            method=self.request.method if self.request.method else \"\",",
          "            url=self.request.url if self.request.url else \"\",",
          "            data=(",
          "                f\"Body:\\n{format_dict(self.request.data)}\"",
          "                if self.request.data and isinstance(self.request.data, dict)",
          "                else (",
          "                    f\"Body:\\n{format_list(self.request.data)}\"",
          "                    if self.request.data and isinstance(self.request.data, list)",
          "                    else (f\"Body:\\n{self.request.data}\" if self.request.data else \"\")",
          "                )",
          "            ),",
          "        )",
          "",
          "",
          "class IssueDetails(BaseModel):",
          "    id: Annotated[int, Examples(specialized.unsigned_ints)]",
          "    title: Annotated[str, Examples(specialized.ascii_words)]",
          "    short_id: Optional[str] = None",
          "    events: list[SentryEventData]",
          "",
          "",
          "class ProfileFrame(TypedDict):",
          "    function: str",
          "    module: str",
          "    filename: str",
          "    lineno: int",
          "    in_app: bool",
          "    duration_ns: NotRequired[float]",
          "    children: NotRequired[list[\"ProfileFrame\"]]",
          "",
          "",
          "class Profile(BaseModel):",
          "    profile_matches_issue: bool = Field(default=False)",
          "    execution_tree: list[ProfileFrame] = Field(default_factory=list)",
          "    relevant_functions: set[str] = Field(default_factory=set)",
          "",
          "    def format_profile(",
          "        self,",
          "        context_before: int = 20,",
          "        context_after: int = 3,",
          "    ) -> str:",
          "        \"\"\"",
          "        Format the profile tree, focusing on relevant functions from the stacktrace.",
          "",
          "        Args:",
          "            context_before: Number of lines to include before first relevant function",
          "            context_after: Number of lines to include after last relevant function",
          "",
          "        Returns:",
          "            str: Formatted profile string, showing relevant sections of the execution tree",
          "        \"\"\"",
          "        full_profile = self._format_profile_helper(self.execution_tree)",
          "",
          "        if self.relevant_functions:",
          "            relevant_window = self._get_relevant_code_window(",
          "                full_profile, context_before=context_before, context_after=context_after",
          "            )",
          "            if relevant_window:",
          "                return relevant_window",
          "",
          "        return full_profile",
          "",
          "    def _get_relevant_code_window(",
          "        self, code: str, context_before: int = 20, context_after: int = 3",
          "    ) -> str | None:",
          "        \"\"\"",
          "        Find the relevant section of code containing functions from the stacktrace.",
          "        Expands the selection to include context lines before and after.",
          "",
          "        Args:",
          "            code: Multi-line string of formatted profile to analyze",
          "            context_before: Number of lines to include before first relevant line",
          "            context_after: Number of lines to include after last relevant line",
          "",
          "        Returns:",
          "            str | None: Selected profile window with context, or None if no relevant functions found",
          "        \"\"\"",
          "        if not self.relevant_functions or not code:",
          "            return None",
          "",
          "        lines = code.splitlines()",
          "        first_relevant_line = None",
          "        last_relevant_line = None",
          "",
          "        # Find first and last lines containing relevant functions",
          "        for i, line in enumerate(lines):",
          "            if any(func in line for func in self.relevant_functions):",
          "                if first_relevant_line is None:",
          "                    first_relevant_line = i",
          "                last_relevant_line = i",
          "",
          "        if first_relevant_line is None:",
          "            first_relevant_line = 0",
          "        if last_relevant_line is None:",
          "            last_relevant_line = len(lines) - 1",
          "",
          "        # Calculate window boundaries with context",
          "        start_line = max(0, first_relevant_line - context_before)",
          "        end_line = min(len(lines), last_relevant_line + context_after + 1)",
          "",
          "        result = []",
          "        if start_line > 0:",
          "            result.append(\"...\")",
          "        result.extend(lines[start_line:end_line])",
          "        if end_line < len(lines):",
          "            result.append(\"...\")",
          "",
          "        return \"\\n\".join(result)",
          "",
          "    def _format_profile_helper(self, tree: list[ProfileFrame], prefix: str = \"\") -> str:",
          "        \"\"\"",
          "        Returns a pretty-printed string representation of the execution tree using tree formatting.",
          "",
          "        Args:",
          "            tree: List of dictionaries representing the execution tree",
          "            prefix: Current line prefix for tree structure",
          "",
          "        Returns:",
          "            str: Formatted string representation of the tree",
          "        \"\"\"",
          "        if not tree:",
          "            return \"\"",
          "",
          "        result = []",
          "",
          "        for i, node in enumerate(tree):",
          "            is_last = i == len(tree) - 1",
          "",
          "            # Format the current node",
          "            function_name = node.get(\"function\", \"Unknown function\")",
          "            filename = node.get(\"filename\", \"Unknown file\")",
          "            location_info = f\" ({filename}:{node.get('lineno', '')})\"",
          "            duration_ms = node.get(\"duration_ns\", -1) / 1_000_000",
          "            duration_info = f\" - {duration_ms:.0f}ms\" if duration_ms >= 0 else \"\"",
          "",
          "            # Add tree structure characters",
          "            if is_last:",
          "                result.append(f\"{prefix}└─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                result.append(f\"{prefix}├─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Recursively format children",
          "            if node.get(\"children\"):",
          "                children_str = self._format_profile_helper(node.get(\"children\", []), child_prefix)",
          "                if children_str:",
          "                    result.append(children_str)",
          "",
          "        return \"\\n\".join(result)",
          "",
          "",
          "class Span(BaseModel):",
          "    span_id: str",
          "    title: str | None = None",
          "    data: dict[str, Any] | None = None",
          "    duration: str | None = None",
          "    children: list[\"Span\"] = Field(default_factory=list)",
          "",
          "",
          "class TraceEvent(BaseModel):",
          "    event_id: str | None = None",
          "    title: str | None = None",
          "    is_transaction: bool = False",
          "    is_error: bool = False",
          "    platform: str | None = None",
          "    is_current_project: bool = True",
          "    project_slug: str | None = None",
          "    project_id: int | None = None",
          "    duration: str | None = None",
          "    profile_id: str | None = None",
          "    children: list[\"TraceEvent\"] = Field(default_factory=list)",
          "    spans: list[Span] = Field(default_factory=list)",
          "",
          "    def format_spans_tree(self) -> str:",
          "        \"\"\"",
          "        Returns a formatted string representation of the span tree.",
          "        \"\"\"",
          "        if not self.spans:",
          "            return \"No spans available\"",
          "",
          "        lines = [f\"Spans for {self.title or 'Unnamed Event'}\"]",
          "        self._format_spans(self.spans, \"\", lines)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_spans(self, spans: list[Span], prefix: str, lines: list[str]) -> None:",
          "        \"\"\"",
          "        Helper method to recursively format spans.",
          "",
          "        Args:",
          "            spans: List of spans to format",
          "            prefix: Current prefix for tree structure",
          "            lines: List of lines being built",
          "        \"\"\"",
          "        if not spans:",
          "            return",
          "",
          "        # Group consecutive similar spans",
          "        grouped_spans: list[list[Span]] = []",
          "        current_group: list[Span] | None = None",
          "",
          "        for span in spans:",
          "            if current_group and self._are_spans_similar(current_group[0], span):",
          "                current_group.append(span)",
          "            else:",
          "                if current_group:",
          "                    grouped_spans.append(current_group)",
          "                current_group = [span]",
          "",
          "        if current_group:",
          "            grouped_spans.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_spans):",
          "            is_last = i == len(grouped_spans) - 1",
          "            span = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted span line",
          "            span_line = self._format_span_line(span) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {span_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "                data_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {span_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "                data_prefix = f\"{prefix}│  \"",
          "",
          "            # Add the span data as JSON if available",
          "            if span.data:",
          "                data_str = json.dumps(span.data, indent=2)",
          "                data_lines = data_str.split(\"\\n\")",
          "                for data_line in data_lines:",
          "                    lines.append(f\"{data_prefix} {data_line}\")",
          "",
          "            # Process children",
          "            if span.children:",
          "                self._format_spans(span.children, child_prefix, lines)",
          "",
          "    def _format_span_line(self, span: Span) -> str:",
          "        \"\"\"Format a single span line.\"\"\"",
          "        parts = []",
          "",
          "        title = span.title or \"Unnamed Span\"",
          "        parts.append(title)",
          "        if span.duration:",
          "            parts.append(f\"({span.duration})\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_spans_similar(self, span1: Span, span2: Span) -> bool:",
          "        \"\"\"Check if two spans are similar enough to be grouped together.\"\"\"",
          "        if span1.title != span2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(span1.children) != len(span2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children are similar",
          "        for i in range(len(span1.children)):",
          "            if not self._are_spans_similar(span1.children[i], span2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "",
          "class TraceTree(BaseModel):",
          "    trace_id: str | None = None",
          "    org_id: int | None = None",
          "    events: list[TraceEvent] = Field(default_factory=list)  # only expecting transactions and errors",
          "",
          "    def format_trace_tree(self):",
          "        if not self.events:",
          "            return \"Trace (empty)\"",
          "",
          "        lines = [\"Trace\"]",
          "        self._format_events(self.events, \"\", lines, is_last_child=True)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_events(",
          "        self, events: list[TraceEvent], prefix: str, lines: list[str], is_last_child: bool",
          "    ):",
          "        if not events:",
          "            return",
          "",
          "        # Group consecutive similar events",
          "        grouped_events: list[list[TraceEvent]] = []",
          "        current_group: list[TraceEvent] | None = None",
          "",
          "        for event in events:",
          "            if current_group and self._are_events_similar(current_group[0], event):",
          "                current_group.append(event)",
          "            else:",
          "                if current_group:",
          "                    grouped_events.append(current_group)",
          "                current_group = [event]",
          "",
          "        if current_group:",
          "            grouped_events.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_events):",
          "            is_last = i == len(grouped_events) - 1",
          "            event = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted event line",
          "            event_line = self._format_event_line(event) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {event_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {event_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Process children",
          "            if event.children:",
          "                self._format_events(event.children, child_prefix, lines, is_last)",
          "",
          "    def _format_event_line(self, event: TraceEvent) -> str:",
          "        parts = []",
          "",
          "        # Add ERROR prefix if not a transaction",
          "        prefix = \"ERROR: \" if event.is_error else \"\"",
          "",
          "        # Add title",
          "        title = event.title or \"Unnamed Event\"",
          "        parts.append(f\"{prefix}{title}\")",
          "",
          "        # Add duration if it exists",
          "        if event.duration:",
          "            parts.append(f\"({event.duration})\")",
          "",
          "        # Add event_id (first 7 digits)",
          "        if event.event_id:",
          "            parts.append(f\"(event ID: {event.event_id[:7]})\")",
          "",
          "        # Add project",
          "        if event.project_slug and event.project_id:",
          "            project_str = f\"(project: {event.project_slug})\"",
          "            parts.append(project_str)",
          "",
          "        # Add platform",
          "        if event.platform:",
          "            parts.append(f\"({event.platform})\")",
          "",
          "        # Add profile",
          "        if event.profile_id:",
          "            parts.append(\"(profile available)\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_events_similar(self, event1: TraceEvent, event2: TraceEvent) -> bool:",
          "        \"\"\"Check if two events are similar enough to be grouped together\"\"\"",
          "        if event1.title != event2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(event1.children) != len(event2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children match",
          "        # This is a simplified check - for a full check we'd need to recursively compare children",
          "        for i in range(len(event1.children)):",
          "            if not self._are_events_similar(event1.children[i], event2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "    def get_full_event_id(self, truncated_id: str) -> str | None:",
          "        \"\"\"Return the full event_id given the first 7 characters\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event.event_id",
          "        return None",
          "",
          "    def get_event_by_id(self, truncated_id: str) -> TraceEvent | None:",
          "        \"\"\"Return the full TraceEvent object given a truncated event ID\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event",
          "        return None",
          "",
          "    def _get_all_events(self) -> list[TraceEvent]:",
          "        \"\"\"Return a flattened list of all events in the tree\"\"\"",
          "        all_events = []",
          "",
          "        def collect_events(events):",
          "            for event in events:",
          "                all_events.append(event)",
          "                if event.children:",
          "                    collect_events(event.children)",
          "",
          "        collect_events(self.events)",
          "        return all_events",
          "",
          "    def get_all_project_ids(self) -> list[int]:",
          "        \"\"\"Return a list of all project IDs in the trace tree\"\"\"",
          "        return list(set(event.project_id for event in self._get_all_events() if event.project_id))",
          "",
          "",
          "class RepoDefinition(BaseModel):",
          "    provider: Annotated[str, Examples((\"github\", \"integrations:github\"))]",
          "    owner: str",
          "    name: str",
          "    external_id: Annotated[str, Examples(specialized.ascii_words)]",
          "    branch_name: str | None = Field(",
          "        default=None,",
          "        description=\"The branch that Autofix will work on, otherwise the default branch will be used.\",",
          "    )",
          "    instructions: str | None = Field(",
          "        default=None,",
          "        description=\"Custom instructions when working in this repo.\",",
          "    )",
          "    base_commit_sha: str | None = None",
          "    provider_raw: str | None = None",
          "",
          "    @property",
          "    def full_name(self):",
          "        return f\"{self.owner}/{self.name}\"",
          "",
          "    @model_validator(mode=\"before\")",
          "    @classmethod",
          "    def store_provider_raw(cls, data):",
          "        if isinstance(data, dict) and \"provider\" in data and \"provider_raw\" not in data:",
          "            data[\"provider_raw\"] = data[\"provider\"]",
          "        return data",
          "",
          "    @field_validator(\"provider\", mode=\"after\")",
          "    @classmethod",
          "    def validate_provider(cls, provider: str):",
          "        return process_repo_provider(provider)",
          "",
          "    def __hash__(self):",
          "        return hash((self.provider, self.owner, self.name, self.external_id))",
          "",
          "",
          "class InitializationError(Exception):",
          "    pass",
          "",
          "",
          "class PromptXmlModel(BaseXmlModel):",
          "    def _pad_with_newlines(self, tree: ET.Element) -> None:",
          "        for elem in tree.iter():",
          "            if elem.text:",
          "                stripped = elem.text.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.text = \"\\n\" + stripped + \"\\n\"",
          "            if elem.tail:",
          "                stripped = elem.tail.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.tail = \"\\n\" + stripped + \"\\n\"",
          "",
          "    def to_prompt_str(self) -> str:",
          "        tree: ET.Element = self.to_xml_tree()",
          "",
          "        ET.indent(tree, space=\"\", level=0)",
          "",
          "        self._pad_with_newlines(tree)",
          "",
          "        return unescape_xml_chars(ET.tostring(tree, encoding=\"unicode\"))",
          "",
          "",
          "class Line(BaseModel):",
          "    source_line_no: Optional[int] = None",
          "    target_line_no: Optional[int] = None",
          "    diff_line_no: Optional[int] = None",
          "    value: str",
          "    line_type: Literal[\" \", \"+\", \"-\"]",
          "",
          "",
          "def raw_lines_to_lines(",
          "    lines: list[str],",
          "    source_start: int,",
          "    target_start: int,",
          "    target_line_to_extra: dict[int, str] | None = None,",
          ") -> list[Line]:",
          "    lines_after_header = lines[1:]",
          "    result = []",
          "    current_source_line = source_start",
          "    current_target_line = target_start",
          "    target_line_to_extra = target_line_to_extra or {}",
          "",
          "    for line in lines_after_header:",
          "        line_type = line[0]",
          "",
          "        if current_target_line in target_line_to_extra:",
          "            extra = target_line_to_extra[current_target_line]",
          "            line = line + extra",
          "",
          "        if line_type == \" \":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "            current_target_line += 1",
          "        elif line_type == \"+\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=None,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_target_line += 1",
          "        elif line_type == \"-\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=None,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "        elif line_type == \"\\\\\":",
          "            # Skip the \"\\ No newline at end of file\" marker",
          "            continue",
          "        else:",
          "            raise ValueError(f\"Invalid line type: {line_type}\")",
          "",
          "    return result",
          "",
          "",
          "def right_justified(min_num: int, max_num: int) -> list[str]:",
          "    max_digits = len(str(max_num))",
          "    return [f\"{number:>{max_digits}}\" for number in range(min_num, max_num + 1)]",
          "",
          "",
          "class Hunk(BaseModel):",
          "    source_start: int",
          "    source_length: int",
          "    target_start: int",
          "    target_length: int",
          "    section_header: str",
          "    lines: list[Line]",
          "",
          "    def raw(self) -> str:",
          "        \"\"\"",
          "        The raw hunk, like you see in `git diff` or the original patch string.",
          "        \"\"\"",
          "        return \"\\n\".join((self.section_header, *(line.value for line in self.lines)))",
          "",
          "    def annotated(",
          "        self,",
          "        max_digits_source: int | None = None,",
          "        max_digits_target: int | None = None,",
          "        source_target_delim: str = \"    \",",
          "    ) -> str:",
          "        \"\"\"",
          "        Hunk string with line numbers for the source and target, like you see in GitHub.",
          "        \"\"\"",
          "        if max_digits_source is None:",
          "            max_digits_source = len(str(self.lines[-1].source_line_no))",
          "        if max_digits_target is None:",
          "            max_digits_target = len(str(self.lines[-1].target_line_no))",
          "        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)",
          "        # +2 for the whitespace and then the line type character",
          "        return \"\\n\".join(",
          "            (",
          "                f\"{header_start}{self.section_header}\",",
          "                *(",
          "                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"",
          "                    for line in self.lines",
          "                ),",
          "            )",
          "        )",
          "",
          "",
          "def annotate_hunks(hunks: list[Hunk]) -> list[str]:",
          "    \"\"\"",
          "    Hunks annotated with line numbers for the source and target, like you see in GitHub.",
          "    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.",
          "    \"\"\"",
          "    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)",
          "    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)",
          "    return [",
          "        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)",
          "        for hunk in hunks",
          "    ]",
          "",
          "",
          "class FilePatch(BaseModel):",
          "    type: Literal[\"A\", \"M\", \"D\"]",
          "    path: str",
          "    added: int",
          "    removed: int",
          "    source_file: str",
          "    target_file: str",
          "    hunks: list[Hunk]",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.type == \"A\":",
          "            if file_contents is not None and file_contents.strip():",
          "                raise FileChangeError(\"Cannot add a file that already exists.\")",
          "            return self._apply_hunks([])",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for modify or delete operations.\")",
          "",
          "        if self.type == \"D\":",
          "            return None",
          "",
          "        # For M type",
          "        try:",
          "            new_contents = self._apply_hunks(file_contents.splitlines(keepends=True))",
          "        except Exception as e:",
          "            raise FileChangeError(f\"Error applying hunks: {e}\")",
          "",
          "        # Preserve any trailing characters from original",
          "        if file_contents:",
          "            trailing = file_contents[len(file_contents.rstrip()) :]",
          "            return new_contents + trailing",
          "",
          "        return new_contents",
          "",
          "    def _apply_hunks(self, lines: list[str]) -> str:",
          "        result = []",
          "        current_line = 0",
          "",
          "        for hunk in self.hunks:",
          "            # Add unchanged lines before the hunk",
          "            result.extend(lines[current_line : hunk.source_start - 1])",
          "            current_line = hunk.source_start - 1",
          "",
          "            for line in hunk.lines:",
          "                if line.line_type == \"+\":",
          "                    result.append(line.value + (\"\\n\" if not line.value.endswith(\"\\n\") else \"\"))",
          "                elif line.line_type == \" \":",
          "                    result.append(lines[current_line])",
          "                    current_line += 1",
          "                elif line.line_type == \"-\":",
          "                    current_line += 1",
          "",
          "        # Add any remaining unchanged lines after the last hunk",
          "        result.extend(lines[current_line:])",
          "",
          "        return \"\".join(result).rstrip(\"\\n\")",
          "",
          "    @staticmethod",
          "    def to_hunks(patch: str, target_line_to_extra: dict[int, str] | None = None) -> list[Hunk]:",
          "        hunk_header_pattern = r\"@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@\"",
          "",
          "        hunks: list[Hunk] = []",
          "        lines = patch.splitlines()",
          "        current_hunk = Hunk(",
          "            source_start=0,",
          "            source_length=0,",
          "            target_start=0,",
          "            target_length=0,",
          "            section_header=\"\",",
          "            lines=[],",
          "        )",
          "        current_lines: list[str] = []",
          "",
          "        for line in lines:",
          "            match = re.match(hunk_header_pattern, line)",
          "            if match:",
          "                if current_lines:",
          "                    current_hunk.lines = raw_lines_to_lines(",
          "                        current_lines,",
          "                        current_hunk.source_start,",
          "                        current_hunk.target_start,",
          "                        target_line_to_extra=target_line_to_extra,",
          "                    )",
          "                    hunks.append(current_hunk)",
          "                    current_lines = []",
          "                source_start, source_length, target_start, target_length = map(int, match.groups())",
          "                current_hunk = Hunk(",
          "                    source_start=source_start,",
          "                    source_length=source_length,",
          "                    target_start=target_start,",
          "                    target_length=target_length,",
          "                    section_header=line,",
          "                    lines=[],",
          "                )",
          "                current_lines = [line]  # starts with section header",
          "            elif current_lines:",
          "                current_lines.append(line)",
          "",
          "        if current_lines:",
          "            current_hunk.lines = raw_lines_to_lines(",
          "                current_lines,",
          "                current_hunk.source_start,",
          "                current_hunk.target_start,",
          "                target_line_to_extra=target_line_to_extra,",
          "            )",
          "            hunks.append(current_hunk)",
          "",
          "        return hunks",
          "",
          "",
          "class FileChangeError(Exception):",
          "    pass",
          "",
          "",
          "class FileChange(BaseModel):",
          "    change_type: Literal[\"create\", \"edit\", \"delete\"]",
          "    path: str",
          "    reference_snippet: Optional[str] = None",
          "    new_snippet: Optional[str] = None",
          "    description: Optional[str] = None",
          "    commit_message: Optional[str] = None",
          "    tool_call_id: Optional[str] = None",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.change_type == \"create\":",
          "            if file_contents is not None and file_contents != \"\":",
          "                raise FileChangeError(\"Cannot create a file that already exists.\")",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for creating a file.\")",
          "            return self.new_snippet",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for non-create operations.\")",
          "",
          "        if self.change_type == \"edit\":",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for editing a file.\")",
          "            if self.reference_snippet is None:",
          "                raise FileChangeError(\"Reference snippet must be provided for editing a file.\")",
          "            return file_contents.replace(self.reference_snippet, self.new_snippet)",
          "",
          "        # Delete",
          "        if self.reference_snippet is None:",
          "            return None",
          "",
          "        return file_contents.replace(self.reference_snippet, \"\")",
          "",
          "",
          "class SeerProjectPreference(BaseModel):",
          "    organization_id: int",
          "    project_id: int",
          "    repositories: list[RepoDefinition]",
          "",
          "    def to_db_model(self) -> DbSeerProjectPreference:",
          "        return DbSeerProjectPreference(",
          "            organization_id=self.organization_id,",
          "            project_id=self.project_id,",
          "            repositories=[repo.model_dump() for repo in self.repositories],",
          "        )",
          "",
          "    @classmethod",
          "    def from_db_model(cls, db_model: DbSeerProjectPreference) -> \"SeerProjectPreference\":",
          "        return cls(",
          "            organization_id=db_model.organization_id,",
          "            project_id=db_model.project_id,",
          "            repositories=db_model.repositories,",
          "        )",
          "",
          "",
          "class EAPTrace(BaseModel):",
          "    trace_id: str = Field(..., description=\"ID of the trace\")",
          "    trace: list[dict] = Field(..., description=\"List of spans in the trace\")",
          "",
          "    def _get_transaction_spans(self, trace: list[dict] | dict) -> list[dict]:",
          "        \"\"\"",
          "        Filters the trace to only include the transaction spans.",
          "        \"\"\"",
          "        if not trace:",
          "            return []",
          "",
          "        if isinstance(trace, list):",
          "            transaction_spans = []",
          "            for span in trace:",
          "                transaction_spans.extend(self._get_transaction_spans(span))",
          "            return transaction_spans",
          "",
          "        transaction_spans = []",
          "",
          "        if trace.get(\"is_transaction\"):",
          "            transaction_span = trace.copy()",
          "            transaction_span[\"children\"] = self._get_transaction_spans(trace.get(\"children\", []))",
          "            transaction_spans.append(transaction_span)",
          "        else:",
          "            transaction_spans.extend(self._get_transaction_spans(trace.get(\"children\", [])))",
          "",
          "        return transaction_spans",
          "",
          "    def get_and_format_trace(self, only_transactions=False) -> str:",
          "        \"\"\"",
          "        Formats the trace as a string of tags.",
          "        \"\"\"",
          "        trace = self.trace",
          "        if only_transactions:",
          "            trace = self._get_transaction_spans(self.trace)",
          "",
          "        def format_span_as_tag(span, depth=0):",
          "            indent = \"    \" * depth",
          "",
          "            attrs = []",
          "            for key, value in span.items():",
          "                # Ignore event_type since all events are marked as \"span\"",
          "                if key not in [\"children\", \"event_type\"]:",
          "                    attrs.append(f'{key}=\"{value}\"')",
          "            attrs_str = \" \".join(attrs)",
          "",
          "            tag_name = \"txn\" if span.get(\"is_transaction\") else \"span\"",
          "",
          "            if not span.get(\"children\"):",
          "                return f\"{indent}<{tag_name} {attrs_str} />\"",
          "            else:",
          "                tag_start = f\"{indent}<{tag_name} {attrs_str}>\"",
          "",
          "                # Format children recursively with increased depth",
          "                children = []",
          "                for child in span[\"children\"]:",
          "                    children.append(format_span_as_tag(child, depth + 1))",
          "",
          "                child_content = \"\\n\" + \"\\n\".join(children)",
          "                tag_end = f\"\\n{indent}</{tag_name}>\"",
          "",
          "                return f\"{tag_start}{child_content}{tag_end}\"",
          "",
          "        formatted = []",
          "        for span in trace:",
          "            formatted.append(format_span_as_tag(span, 0))",
          "",
          "        return \"\\n\".join(formatted)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/models.py",
      "image": "seer.automation.models",
      "is_application": true,
      "line": 1153,
      "name": "annotate_hunks.<locals>.<listcomp>",
      "path": "/app/src/seer/automation/models.py",
      "codeContext": {
        "file": "seer/automation/models.py",
        "line": 1153,
        "name": "annotate_hunks.<locals>.<listcomp>",
        "code": "\ndef annotate_hunks(hunks: list[Hunk]) -> list[str]:\n    \"\"\"\n    Hunks annotated with line numbers for the source and target, like you see in GitHub.\n    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.\n    \"\"\"\n    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)\n    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)\n    return [\n        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)\n        for hunk in hunks\n    ]\n\n\nclass FilePatch(BaseModel):\n    type: Literal[\"A\", \"M\", \"D\"]\n    path: str\n    added: int\n    removed: int\n    source_file: str",
        "lineRange": {
          "start": 1144,
          "end": 1163
        },
        "lines": [
          "import json",
          "import re",
          "import textwrap",
          "from typing import Annotated, Any, Literal, NotRequired, Optional",
          "from xml.etree import ElementTree as ET",
          "",
          "import sentry_sdk",
          "from johen.examples import Examples",
          "from johen.generators import specialized",
          "from pydantic import (",
          "    AliasChoices,",
          "    AliasGenerator,",
          "    BaseModel,",
          "    ConfigDict,",
          "    Field,",
          "    ValidationError,",
          "    ValidationInfo,",
          "    field_validator,",
          "    model_validator,",
          ")",
          "from pydantic.alias_generators import to_camel, to_snake",
          "from pydantic_xml import BaseXmlModel",
          "from typing_extensions import TypedDict",
          "",
          "from seer.automation.utils import (",
          "    format_dict,",
          "    format_list,",
          "    process_repo_provider,",
          "    unescape_xml_chars,",
          ")",
          "from seer.db import DbSeerProjectPreference",
          "",
          "",
          "class StacktraceFrame(BaseModel):",
          "    model_config = ConfigDict(",
          "        alias_generator=AliasGenerator(",
          "            validation_alias=lambda k: AliasChoices(to_camel(k), to_snake(k)),",
          "            serialization_alias=to_camel,",
          "        )",
          "    )",
          "",
          "    function: Optional[Annotated[str, Examples(specialized.ascii_words)]] = None",
          "    filename: Optional[Annotated[str, Examples(specialized.file_names)]]",
          "    abs_path: Optional[Annotated[str, Examples(specialized.file_paths)]]",
          "    line_no: Optional[int]",
          "    col_no: Optional[int]",
          "    context: list[tuple[int, Optional[str]]] = []",
          "    repo_name: Optional[str] = None",
          "    in_app: bool | None = False",
          "    vars: Optional[dict[str, Any]] = None",
          "    package: Optional[str] = None",
          "",
          "    @field_validator(\"vars\", mode=\"before\")",
          "    @classmethod",
          "    def validate_vars(cls, vars: Optional[dict[str, Any]], info: ValidationInfo):",
          "        if not vars or \"context\" not in info.data or not info.data[\"context\"]:",
          "            return vars",
          "        code_str = \"\"",
          "        for _, line in info.data[\"context\"]:",
          "            code_str += line + \"\\n\"",
          "        return cls._trim_vars(vars, code_str)",
          "",
          "    @staticmethod",
          "    def _trim_vars(vars: dict[str, Any], code_context: str):",
          "        # only keep variables mentioned in the context of the stacktrace frame",
          "        # and filter out any values containing \"[Filtered]\"",
          "        trimmed_vars = {}",
          "        for key, val in vars.items():",
          "            if key in code_context:",
          "                if isinstance(val, (dict, list)):",
          "                    filtered_val = StacktraceFrame._filter_nested_value(val)",
          "                    if filtered_val is not None:",
          "                        trimmed_vars[key] = filtered_val",
          "                elif not StacktraceFrame._contains_filtered(val):",
          "                    trimmed_vars[key] = val",
          "        return trimmed_vars",
          "",
          "    @staticmethod",
          "    def _filter_nested_value(value: Any) -> Any:",
          "        if isinstance(value, dict):",
          "            filtered_dict = {}",
          "            for k, v in value.items():",
          "                if isinstance(v, (dict, list)):",
          "                    filtered_v = StacktraceFrame._filter_nested_value(v)",
          "                    if filtered_v is not None:",
          "                        filtered_dict[k] = filtered_v",
          "                elif not StacktraceFrame._contains_filtered(v):",
          "                    filtered_dict[k] = v",
          "            return filtered_dict if filtered_dict else None",
          "        elif isinstance(value, list):",
          "            filtered_list = []",
          "            for item in value:",
          "                if isinstance(item, (dict, list)):",
          "                    filtered_item = StacktraceFrame._filter_nested_value(item)",
          "                    if filtered_item is not None:",
          "                        filtered_list.append(filtered_item)",
          "                elif not StacktraceFrame._contains_filtered(item):",
          "                    filtered_list.append(item)",
          "            return filtered_list if filtered_list else None",
          "        return None if StacktraceFrame._contains_filtered(value) else value",
          "",
          "    @staticmethod",
          "    def _contains_filtered(value: Any) -> bool:",
          "        return isinstance(value, str) and \"[Filtered]\" in value",
          "",
          "",
          "class SentryFrame(TypedDict):",
          "    absPath: Optional[str]",
          "    colNo: Optional[int]",
          "    context: list[tuple[int, str]]",
          "    filename: NotRequired[Optional[str]]",
          "    function: NotRequired[Optional[str]]",
          "    inApp: NotRequired[bool]",
          "    instructionAddr: NotRequired[Optional[str]]",
          "    lineNo: NotRequired[Optional[int]]",
          "    module: NotRequired[Optional[str]]",
          "    package: NotRequired[Optional[str]]",
          "    platform: NotRequired[Optional[str]]",
          "    rawFunction: NotRequired[Optional[str]]",
          "    symbol: NotRequired[Optional[str]]",
          "    symbolAddr: NotRequired[Optional[str]]",
          "    trust: NotRequired[Optional[Any]]",
          "    vars: NotRequired[Optional[dict[str, Any]]]",
          "    addrMode: NotRequired[Optional[str]]",
          "    isPrefix: NotRequired[bool]",
          "    isSentinel: NotRequired[bool]",
          "    lock: NotRequired[Optional[Any]]",
          "    map: NotRequired[Optional[str]]",
          "    mapUrl: NotRequired[Optional[str]]",
          "    minGroupingLevel: NotRequired[int]",
          "    origAbsPath: NotRequired[Optional[str]]",
          "    sourceLink: NotRequired[Optional[str]]",
          "    symbolicatorStatus: NotRequired[Optional[Any]]",
          "",
          "",
          "class Stacktrace(BaseModel):",
          "    frames: list[StacktraceFrame]",
          "",
          "    @field_validator(\"frames\", mode=\"before\")",
          "    @classmethod",
          "    def validate_frames(cls, frames: list[StacktraceFrame | SentryFrame]):",
          "        stacktrace_frames = []",
          "        for frame in frames:",
          "            if isinstance(frame, dict):",
          "                if \"function\" not in frame:",
          "                    frame[\"function\"] = None",
          "                try:",
          "                    stacktrace_frames.append(StacktraceFrame.model_validate(frame))",
          "                except ValidationError:",
          "                    sentry_sdk.capture_exception()",
          "                    continue",
          "            else:",
          "                stacktrace_frames.append(frame)",
          "",
          "        return cls._trim_frames(stacktrace_frames)",
          "",
          "    def to_str(",
          "        self,",
          "        max_frames: int = 16,",
          "        in_app_only: bool = False,",
          "        include_context: bool = True,",
          "        include_var_values: bool = True,",
          "    ):",
          "        stack_str = \"\"",
          "",
          "        frames = self.frames",
          "        if in_app_only:",
          "            frames = [frame for frame in frames if frame.in_app]",
          "",
          "        for frame in reversed(frames[-max_frames:]):",
          "            col_no_str = f\", column {frame.col_no}\" if frame.col_no is not None else \"\"",
          "            repo_str = f\" in repo {frame.repo_name}\" if frame.repo_name else \"\"",
          "            line_no_str = (",
          "                f\"[Line {frame.line_no}{col_no_str}]\"",
          "                if frame.line_no is not None",
          "                else \"[Line: Unknown]\"",
          "            )",
          "",
          "            function = frame.function if frame.function else \"Unknown function\"",
          "            if frame.filename:",
          "                stack_str += f\" {function} in file {frame.filename}{repo_str} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            elif frame.package:",
          "                stack_str += f\" {function} in package {frame.package} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            else:",
          "                stack_str += f\" {function} in unknown file {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "",
          "            if include_context:",
          "                for ctx in frame.context:",
          "                    is_suspect_line = ctx[0] == frame.line_no",
          "                    stack_str += f\"{ctx[1]}{'  <-- SUSPECT LINE' if is_suspect_line else ''}\\n\"",
          "",
          "            if frame.vars:",
          "                if include_var_values:",
          "                    vars_title = \"Variable values at the time of the exception:\"",
          "                    vars_str = format_dict(frame.vars)",
          "                else:",
          "                    vars_title = \"Variables at the time of the exception:\"",
          "                    vars_str = \", \".join(frame.vars.keys())",
          "",
          "                stack_str += textwrap.dedent(",
          "                    \"\"\"\\",
          "                    ---",
          "                    {vars_title}:",
          "                    {vars_str}",
          "                    \"\"\"",
          "                ).format(vars_title=vars_title, vars_str=vars_str)",
          "            stack_str += \"------\\n\"",
          "",
          "        return stack_str",
          "",
          "    @staticmethod",
          "    def _trim_frames(frames: list[StacktraceFrame], frame_allowance=16):",
          "        frames_len = len(frames)",
          "        if frames_len <= frame_allowance:",
          "            return frames",
          "",
          "        app_frames = [frame for frame in frames if frame.in_app]",
          "        system_frames = [frame for frame in frames if not frame.in_app]",
          "",
          "        app_count = len(app_frames)",
          "        system_allowance = max(frame_allowance - app_count, 0)",
          "        app_allowance = frame_allowance - system_allowance",
          "",
          "        if system_allowance > 0:",
          "            # prioritize trimming system frames",
          "            half_system = system_allowance // 2",
          "            kept_system_frames = system_frames[:half_system] + system_frames[-half_system:]",
          "        else:",
          "            kept_system_frames = []",
          "",
          "        if app_allowance > 0:",
          "            half_app = app_allowance // 2",
          "            kept_app_frames = app_frames[:half_app] + app_frames[-half_app:]",
          "        else:",
          "            kept_app_frames = []",
          "",
          "        # combine and sort the kept frames based on their original order",
          "        kept_frames = kept_system_frames + kept_app_frames",
          "        kept_frames.sort(key=lambda frame: frames.index(frame))",
          "        return kept_frames",
          "",
          "",
          "class SentryStacktrace(TypedDict):",
          "    frames: list[SentryFrame]",
          "",
          "",
          "class SentryEventEntryDataValue(TypedDict):",
          "    type: str",
          "    value: str",
          "    stacktrace: SentryStacktrace",
          "",
          "",
          "class SentryExceptionEventData(TypedDict):",
          "    values: list[SentryEventEntryDataValue]",
          "",
          "",
          "class SentryExceptionEntry(BaseModel):",
          "    type: Literal[\"exception\"]",
          "    data: SentryExceptionEventData",
          "",
          "",
          "class SentryEventData(TypedDict):",
          "    title: str",
          "    entries: list[dict]",
          "    tags: NotRequired[list[dict[str, str | None]]]",
          "",
          "",
          "class ExceptionMechanism(TypedDict):",
          "    type: str",
          "    handled: NotRequired[bool]",
          "",
          "",
          "class ExceptionDetails(BaseModel):",
          "    type: Optional[str] = \"\"",
          "    value: Optional[str] = \"\"",
          "    stacktrace: Optional[Stacktrace] = None",
          "    mechanism: Optional[ExceptionMechanism] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class ThreadDetails(BaseModel):",
          "    id: Optional[int | str] = None",
          "    name: Optional[str] = None",
          "    crashed: Optional[bool] = False",
          "    current: Optional[bool] = False",
          "    state: Optional[str] = None",
          "    main: Optional[bool] = False",
          "",
          "    stacktrace: Optional[Stacktrace] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace | None):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class BreadcrumbsDetails(BaseModel):",
          "    type: Optional[str] = None",
          "    message: Optional[str] = None",
          "    category: Optional[str] = None",
          "    data: Optional[dict] = None",
          "    level: Optional[str] = None",
          "",
          "",
          "class RequestDetails(BaseModel):",
          "    url: str | None = None",
          "    method: str | None = None",
          "    data: dict[str, Any] | str | list[Any] | None = None",
          "    # not including cookies, headers, env, query, etc. for now",
          "",
          "",
          "class EventDetails(BaseModel):",
          "    title: str",
          "    message: str | None = None",
          "    transaction_name: str | None = None",
          "    exceptions: list[ExceptionDetails] = Field(default_factory=list, exclude=False)",
          "    threads: list[ThreadDetails] = Field(default_factory=list, exclude=False)",
          "    breadcrumbs: list[BreadcrumbsDetails] = Field(default_factory=list, exclude=False)",
          "    stacktraces: list[Stacktrace] = Field(default_factory=list, exclude=False)",
          "    request: RequestDetails | None = None",
          "",
          "    @classmethod",
          "    def from_event(cls, error_event: SentryEventData):",
          "        MAX_THREADS = 8  # TODO: Smarter logic for max threads",
          "",
          "        exceptions: list[ExceptionDetails] = []",
          "        threads: list[ThreadDetails] = []",
          "        breadcrumbs: list[BreadcrumbsDetails] = []",
          "        stacktraces: list[Stacktrace] = []",
          "        transaction_name: str | None = None",
          "        message: str | None = None",
          "        request: RequestDetails | None = None",
          "",
          "        for tag in error_event.get(\"tags\", []):",
          "            if tag.get(\"key\") == \"transaction\":",
          "                transaction_name = tag.get(\"value\")",
          "",
          "        for entry in error_event.get(\"entries\", []):",
          "            if entry.get(\"type\") == \"exception\":",
          "                for exception in entry.get(\"data\", {}).get(\"values\", []):",
          "                    exceptions.append(ExceptionDetails.model_validate(exception))",
          "            elif entry.get(\"type\") == \"threads\":",
          "                for thread in entry.get(\"data\", {}).get(\"values\", []):",
          "                    thread_details = ThreadDetails.model_validate(thread)",
          "                    if (",
          "                        thread_details.stacktrace",
          "                        and thread_details.stacktrace.frames",
          "                        and len(threads) < MAX_THREADS",
          "                    ):",
          "                        threads.append(thread_details)",
          "            elif entry.get(\"type\") == \"breadcrumbs\":",
          "                all_breadcrumbs = entry.get(\"data\", {}).get(\"values\", [])",
          "                for breadcrumb in all_breadcrumbs[-10:]:  # only look at the most recent breadcrumbs",
          "                    # Skip breadcrumbs with filtered content in message or data",
          "                    if StacktraceFrame._contains_filtered(",
          "                        breadcrumb.get(\"message\")",
          "                    ) or StacktraceFrame._contains_filtered(str(breadcrumb.get(\"data\"))):",
          "                        continue",
          "                    crumb_details = BreadcrumbsDetails.model_validate(breadcrumb)",
          "                    breadcrumbs.append(crumb_details)",
          "            elif entry.get(\"type\") == \"stacktrace\":",
          "                stacktraces.append(Stacktrace.model_validate(entry.get(\"data\", {})))",
          "            elif entry.get(\"type\") == \"message\":",
          "                message = entry.get(\"data\", {}).get(\"formatted\", None)",
          "            elif entry.get(\"type\") == \"request\":",
          "                request = RequestDetails.model_validate(entry.get(\"data\", {}))",
          "",
          "        return cls(",
          "            title=error_event.get(\"title\"),",
          "            transaction_name=transaction_name,",
          "            exceptions=exceptions,",
          "            threads=threads,",
          "            breadcrumbs=breadcrumbs,",
          "            message=message,",
          "            stacktraces=stacktraces,",
          "            request=request,",
          "        )",
          "",
          "    def format_event(self):",
          "        exceptions = self.format_exceptions()",
          "        breadcrumbs = self.format_breadcrumbs()",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "        stacktraces = self.format_stacktraces()",
          "        request = self.format_request()",
          "",
          "        return (",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "            {title} {transaction}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            {breadcrumbs}",
          "            {request}",
          "            \"\"\"",
          "            )",
          "            .format(",
          "                title=self.title,",
          "                transaction=(",
          "                    f\"(occurred in: {self.transaction_name})\" if self.transaction_name else \"\"",
          "                ),",
          "                message=f\"\\n<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "                exceptions=(",
          "                    f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\"",
          "                ),",
          "                stacktraces=(",
          "                    f\"\\n<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "                ),",
          "                breadcrumbs=(",
          "                    f\"\\n<breadcrumb_logs>\\n{breadcrumbs}\\n</breadcrumb_logs>\"",
          "                    if breadcrumbs.strip()",
          "                    else \"\"",
          "                ),",
          "                request=f\"\\n<http_request>\\n{request}\\n</http_request>\" if request.strip() else \"\",",
          "            )",
          "            .strip()",
          "        )",
          "",
          "    def format_event_without_breadcrumbs(",
          "        self, include_context: bool = True, include_var_values: bool = True",
          "    ):",
          "        exceptions = self.format_exceptions(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        stacktraces = self.format_stacktraces(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {title}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            \"\"\"",
          "        ).format(",
          "            title=self.title,",
          "            exceptions=f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\",",
          "            stacktraces=(",
          "                f\"<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "            ),",
          "            message=f\"<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "        )",
          "",
          "    def format_exceptions(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <exception_{i}{handled}{exception_type}{exception_message}>",
          "                    {stacktrace}",
          "                    </exception{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                exception_type=f' type=\"{exception.type}\"' if exception.type else \"\",",
          "                exception_message=f' message=\"{exception.value}\"' if exception.value else \"\",",
          "                stacktrace=(",
          "                    exception.stacktrace.to_str(",
          "                        include_context=include_context,",
          "                        include_var_values=include_var_values,",
          "                    )",
          "                    if exception.stacktrace",
          "                    else \"\"",
          "                ),",
          "                handled=(",
          "                    f' is_exception_handled=\"{\"yes\" if exception.mechanism.get(\"handled\") else \"no\"}\"'",
          "                    if exception.mechanism and exception.mechanism.get(\"handled\", None) is not None",
          "                    else \"\"",
          "                ),",
          "            )",
          "            for i, exception in enumerate(self.exceptions)",
          "        )",
          "",
          "    def format_threads(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <thread_{thread_id} name=\"{thread_name}\" is_current=\"{thread_current}\" state=\"{thread_state}\" is_main=\"{thread_main}\" crashed=\"{thread_crashed}\">",
          "                    <stacktrace>",
          "                    {stacktrace}",
          "                    </stacktrace>",
          "                    </thread_{thread_id}>\"\"\"",
          "            ).format(",
          "                thread_id=thread.id,",
          "                thread_name=thread.name,",
          "                thread_state=thread.state,",
          "                thread_current=thread.current,",
          "                thread_crashed=thread.crashed,",
          "                thread_main=thread.main,",
          "                stacktrace=thread.stacktrace.to_str() if thread.stacktrace else \"\",",
          "            )",
          "            for thread in self.threads",
          "        )",
          "",
          "    def format_breadcrumbs(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <breadcrumb_{i}{breadcrumb_type}{breadcrumb_category}{level}>",
          "                {content}",
          "                </breadcrumb_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                breadcrumb_type=f' type=\"{breadcrumb.type}\"' if breadcrumb.type else \"\",",
          "                breadcrumb_category=(",
          "                    f' category=\"{breadcrumb.category}\"' if breadcrumb.category else \"\"",
          "                ),",
          "                content=\"\\n\".join(",
          "                    filter(",
          "                        None,",
          "                        [",
          "                            f\"{breadcrumb.message}\\n\" if breadcrumb.message else \"\",",
          "                            (",
          "                                f\"{str({k: v for k, v in breadcrumb.data.items() if v})}\\n\"",
          "                                if breadcrumb.data",
          "                                else \"\"",
          "                            ),",
          "                        ],",
          "                    )",
          "                ),",
          "                level=f' level=\"{breadcrumb.level}\"' if breadcrumb.level else \"\",",
          "            )",
          "            for i, breadcrumb in enumerate(self.breadcrumbs)",
          "        )",
          "",
          "    def format_stacktraces(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <stacktrace_{i}>",
          "                {stacktrace}",
          "                </stacktrace_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                stacktrace=stacktrace.to_str(",
          "                    include_context=include_context, include_var_values=include_var_values",
          "                ),",
          "            )",
          "            for i, stacktrace in enumerate(self.stacktraces)",
          "        )",
          "",
          "    def format_request(self):",
          "        if not self.request:",
          "            return \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {method} {url}",
          "            {data}",
          "            \"\"\"",
          "        ).format(",
          "            method=self.request.method if self.request.method else \"\",",
          "            url=self.request.url if self.request.url else \"\",",
          "            data=(",
          "                f\"Body:\\n{format_dict(self.request.data)}\"",
          "                if self.request.data and isinstance(self.request.data, dict)",
          "                else (",
          "                    f\"Body:\\n{format_list(self.request.data)}\"",
          "                    if self.request.data and isinstance(self.request.data, list)",
          "                    else (f\"Body:\\n{self.request.data}\" if self.request.data else \"\")",
          "                )",
          "            ),",
          "        )",
          "",
          "",
          "class IssueDetails(BaseModel):",
          "    id: Annotated[int, Examples(specialized.unsigned_ints)]",
          "    title: Annotated[str, Examples(specialized.ascii_words)]",
          "    short_id: Optional[str] = None",
          "    events: list[SentryEventData]",
          "",
          "",
          "class ProfileFrame(TypedDict):",
          "    function: str",
          "    module: str",
          "    filename: str",
          "    lineno: int",
          "    in_app: bool",
          "    duration_ns: NotRequired[float]",
          "    children: NotRequired[list[\"ProfileFrame\"]]",
          "",
          "",
          "class Profile(BaseModel):",
          "    profile_matches_issue: bool = Field(default=False)",
          "    execution_tree: list[ProfileFrame] = Field(default_factory=list)",
          "    relevant_functions: set[str] = Field(default_factory=set)",
          "",
          "    def format_profile(",
          "        self,",
          "        context_before: int = 20,",
          "        context_after: int = 3,",
          "    ) -> str:",
          "        \"\"\"",
          "        Format the profile tree, focusing on relevant functions from the stacktrace.",
          "",
          "        Args:",
          "            context_before: Number of lines to include before first relevant function",
          "            context_after: Number of lines to include after last relevant function",
          "",
          "        Returns:",
          "            str: Formatted profile string, showing relevant sections of the execution tree",
          "        \"\"\"",
          "        full_profile = self._format_profile_helper(self.execution_tree)",
          "",
          "        if self.relevant_functions:",
          "            relevant_window = self._get_relevant_code_window(",
          "                full_profile, context_before=context_before, context_after=context_after",
          "            )",
          "            if relevant_window:",
          "                return relevant_window",
          "",
          "        return full_profile",
          "",
          "    def _get_relevant_code_window(",
          "        self, code: str, context_before: int = 20, context_after: int = 3",
          "    ) -> str | None:",
          "        \"\"\"",
          "        Find the relevant section of code containing functions from the stacktrace.",
          "        Expands the selection to include context lines before and after.",
          "",
          "        Args:",
          "            code: Multi-line string of formatted profile to analyze",
          "            context_before: Number of lines to include before first relevant line",
          "            context_after: Number of lines to include after last relevant line",
          "",
          "        Returns:",
          "            str | None: Selected profile window with context, or None if no relevant functions found",
          "        \"\"\"",
          "        if not self.relevant_functions or not code:",
          "            return None",
          "",
          "        lines = code.splitlines()",
          "        first_relevant_line = None",
          "        last_relevant_line = None",
          "",
          "        # Find first and last lines containing relevant functions",
          "        for i, line in enumerate(lines):",
          "            if any(func in line for func in self.relevant_functions):",
          "                if first_relevant_line is None:",
          "                    first_relevant_line = i",
          "                last_relevant_line = i",
          "",
          "        if first_relevant_line is None:",
          "            first_relevant_line = 0",
          "        if last_relevant_line is None:",
          "            last_relevant_line = len(lines) - 1",
          "",
          "        # Calculate window boundaries with context",
          "        start_line = max(0, first_relevant_line - context_before)",
          "        end_line = min(len(lines), last_relevant_line + context_after + 1)",
          "",
          "        result = []",
          "        if start_line > 0:",
          "            result.append(\"...\")",
          "        result.extend(lines[start_line:end_line])",
          "        if end_line < len(lines):",
          "            result.append(\"...\")",
          "",
          "        return \"\\n\".join(result)",
          "",
          "    def _format_profile_helper(self, tree: list[ProfileFrame], prefix: str = \"\") -> str:",
          "        \"\"\"",
          "        Returns a pretty-printed string representation of the execution tree using tree formatting.",
          "",
          "        Args:",
          "            tree: List of dictionaries representing the execution tree",
          "            prefix: Current line prefix for tree structure",
          "",
          "        Returns:",
          "            str: Formatted string representation of the tree",
          "        \"\"\"",
          "        if not tree:",
          "            return \"\"",
          "",
          "        result = []",
          "",
          "        for i, node in enumerate(tree):",
          "            is_last = i == len(tree) - 1",
          "",
          "            # Format the current node",
          "            function_name = node.get(\"function\", \"Unknown function\")",
          "            filename = node.get(\"filename\", \"Unknown file\")",
          "            location_info = f\" ({filename}:{node.get('lineno', '')})\"",
          "            duration_ms = node.get(\"duration_ns\", -1) / 1_000_000",
          "            duration_info = f\" - {duration_ms:.0f}ms\" if duration_ms >= 0 else \"\"",
          "",
          "            # Add tree structure characters",
          "            if is_last:",
          "                result.append(f\"{prefix}└─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                result.append(f\"{prefix}├─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Recursively format children",
          "            if node.get(\"children\"):",
          "                children_str = self._format_profile_helper(node.get(\"children\", []), child_prefix)",
          "                if children_str:",
          "                    result.append(children_str)",
          "",
          "        return \"\\n\".join(result)",
          "",
          "",
          "class Span(BaseModel):",
          "    span_id: str",
          "    title: str | None = None",
          "    data: dict[str, Any] | None = None",
          "    duration: str | None = None",
          "    children: list[\"Span\"] = Field(default_factory=list)",
          "",
          "",
          "class TraceEvent(BaseModel):",
          "    event_id: str | None = None",
          "    title: str | None = None",
          "    is_transaction: bool = False",
          "    is_error: bool = False",
          "    platform: str | None = None",
          "    is_current_project: bool = True",
          "    project_slug: str | None = None",
          "    project_id: int | None = None",
          "    duration: str | None = None",
          "    profile_id: str | None = None",
          "    children: list[\"TraceEvent\"] = Field(default_factory=list)",
          "    spans: list[Span] = Field(default_factory=list)",
          "",
          "    def format_spans_tree(self) -> str:",
          "        \"\"\"",
          "        Returns a formatted string representation of the span tree.",
          "        \"\"\"",
          "        if not self.spans:",
          "            return \"No spans available\"",
          "",
          "        lines = [f\"Spans for {self.title or 'Unnamed Event'}\"]",
          "        self._format_spans(self.spans, \"\", lines)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_spans(self, spans: list[Span], prefix: str, lines: list[str]) -> None:",
          "        \"\"\"",
          "        Helper method to recursively format spans.",
          "",
          "        Args:",
          "            spans: List of spans to format",
          "            prefix: Current prefix for tree structure",
          "            lines: List of lines being built",
          "        \"\"\"",
          "        if not spans:",
          "            return",
          "",
          "        # Group consecutive similar spans",
          "        grouped_spans: list[list[Span]] = []",
          "        current_group: list[Span] | None = None",
          "",
          "        for span in spans:",
          "            if current_group and self._are_spans_similar(current_group[0], span):",
          "                current_group.append(span)",
          "            else:",
          "                if current_group:",
          "                    grouped_spans.append(current_group)",
          "                current_group = [span]",
          "",
          "        if current_group:",
          "            grouped_spans.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_spans):",
          "            is_last = i == len(grouped_spans) - 1",
          "            span = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted span line",
          "            span_line = self._format_span_line(span) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {span_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "                data_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {span_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "                data_prefix = f\"{prefix}│  \"",
          "",
          "            # Add the span data as JSON if available",
          "            if span.data:",
          "                data_str = json.dumps(span.data, indent=2)",
          "                data_lines = data_str.split(\"\\n\")",
          "                for data_line in data_lines:",
          "                    lines.append(f\"{data_prefix} {data_line}\")",
          "",
          "            # Process children",
          "            if span.children:",
          "                self._format_spans(span.children, child_prefix, lines)",
          "",
          "    def _format_span_line(self, span: Span) -> str:",
          "        \"\"\"Format a single span line.\"\"\"",
          "        parts = []",
          "",
          "        title = span.title or \"Unnamed Span\"",
          "        parts.append(title)",
          "        if span.duration:",
          "            parts.append(f\"({span.duration})\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_spans_similar(self, span1: Span, span2: Span) -> bool:",
          "        \"\"\"Check if two spans are similar enough to be grouped together.\"\"\"",
          "        if span1.title != span2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(span1.children) != len(span2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children are similar",
          "        for i in range(len(span1.children)):",
          "            if not self._are_spans_similar(span1.children[i], span2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "",
          "class TraceTree(BaseModel):",
          "    trace_id: str | None = None",
          "    org_id: int | None = None",
          "    events: list[TraceEvent] = Field(default_factory=list)  # only expecting transactions and errors",
          "",
          "    def format_trace_tree(self):",
          "        if not self.events:",
          "            return \"Trace (empty)\"",
          "",
          "        lines = [\"Trace\"]",
          "        self._format_events(self.events, \"\", lines, is_last_child=True)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_events(",
          "        self, events: list[TraceEvent], prefix: str, lines: list[str], is_last_child: bool",
          "    ):",
          "        if not events:",
          "            return",
          "",
          "        # Group consecutive similar events",
          "        grouped_events: list[list[TraceEvent]] = []",
          "        current_group: list[TraceEvent] | None = None",
          "",
          "        for event in events:",
          "            if current_group and self._are_events_similar(current_group[0], event):",
          "                current_group.append(event)",
          "            else:",
          "                if current_group:",
          "                    grouped_events.append(current_group)",
          "                current_group = [event]",
          "",
          "        if current_group:",
          "            grouped_events.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_events):",
          "            is_last = i == len(grouped_events) - 1",
          "            event = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted event line",
          "            event_line = self._format_event_line(event) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {event_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {event_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Process children",
          "            if event.children:",
          "                self._format_events(event.children, child_prefix, lines, is_last)",
          "",
          "    def _format_event_line(self, event: TraceEvent) -> str:",
          "        parts = []",
          "",
          "        # Add ERROR prefix if not a transaction",
          "        prefix = \"ERROR: \" if event.is_error else \"\"",
          "",
          "        # Add title",
          "        title = event.title or \"Unnamed Event\"",
          "        parts.append(f\"{prefix}{title}\")",
          "",
          "        # Add duration if it exists",
          "        if event.duration:",
          "            parts.append(f\"({event.duration})\")",
          "",
          "        # Add event_id (first 7 digits)",
          "        if event.event_id:",
          "            parts.append(f\"(event ID: {event.event_id[:7]})\")",
          "",
          "        # Add project",
          "        if event.project_slug and event.project_id:",
          "            project_str = f\"(project: {event.project_slug})\"",
          "            parts.append(project_str)",
          "",
          "        # Add platform",
          "        if event.platform:",
          "            parts.append(f\"({event.platform})\")",
          "",
          "        # Add profile",
          "        if event.profile_id:",
          "            parts.append(\"(profile available)\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_events_similar(self, event1: TraceEvent, event2: TraceEvent) -> bool:",
          "        \"\"\"Check if two events are similar enough to be grouped together\"\"\"",
          "        if event1.title != event2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(event1.children) != len(event2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children match",
          "        # This is a simplified check - for a full check we'd need to recursively compare children",
          "        for i in range(len(event1.children)):",
          "            if not self._are_events_similar(event1.children[i], event2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "    def get_full_event_id(self, truncated_id: str) -> str | None:",
          "        \"\"\"Return the full event_id given the first 7 characters\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event.event_id",
          "        return None",
          "",
          "    def get_event_by_id(self, truncated_id: str) -> TraceEvent | None:",
          "        \"\"\"Return the full TraceEvent object given a truncated event ID\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event",
          "        return None",
          "",
          "    def _get_all_events(self) -> list[TraceEvent]:",
          "        \"\"\"Return a flattened list of all events in the tree\"\"\"",
          "        all_events = []",
          "",
          "        def collect_events(events):",
          "            for event in events:",
          "                all_events.append(event)",
          "                if event.children:",
          "                    collect_events(event.children)",
          "",
          "        collect_events(self.events)",
          "        return all_events",
          "",
          "    def get_all_project_ids(self) -> list[int]:",
          "        \"\"\"Return a list of all project IDs in the trace tree\"\"\"",
          "        return list(set(event.project_id for event in self._get_all_events() if event.project_id))",
          "",
          "",
          "class RepoDefinition(BaseModel):",
          "    provider: Annotated[str, Examples((\"github\", \"integrations:github\"))]",
          "    owner: str",
          "    name: str",
          "    external_id: Annotated[str, Examples(specialized.ascii_words)]",
          "    branch_name: str | None = Field(",
          "        default=None,",
          "        description=\"The branch that Autofix will work on, otherwise the default branch will be used.\",",
          "    )",
          "    instructions: str | None = Field(",
          "        default=None,",
          "        description=\"Custom instructions when working in this repo.\",",
          "    )",
          "    base_commit_sha: str | None = None",
          "    provider_raw: str | None = None",
          "",
          "    @property",
          "    def full_name(self):",
          "        return f\"{self.owner}/{self.name}\"",
          "",
          "    @model_validator(mode=\"before\")",
          "    @classmethod",
          "    def store_provider_raw(cls, data):",
          "        if isinstance(data, dict) and \"provider\" in data and \"provider_raw\" not in data:",
          "            data[\"provider_raw\"] = data[\"provider\"]",
          "        return data",
          "",
          "    @field_validator(\"provider\", mode=\"after\")",
          "    @classmethod",
          "    def validate_provider(cls, provider: str):",
          "        return process_repo_provider(provider)",
          "",
          "    def __hash__(self):",
          "        return hash((self.provider, self.owner, self.name, self.external_id))",
          "",
          "",
          "class InitializationError(Exception):",
          "    pass",
          "",
          "",
          "class PromptXmlModel(BaseXmlModel):",
          "    def _pad_with_newlines(self, tree: ET.Element) -> None:",
          "        for elem in tree.iter():",
          "            if elem.text:",
          "                stripped = elem.text.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.text = \"\\n\" + stripped + \"\\n\"",
          "            if elem.tail:",
          "                stripped = elem.tail.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.tail = \"\\n\" + stripped + \"\\n\"",
          "",
          "    def to_prompt_str(self) -> str:",
          "        tree: ET.Element = self.to_xml_tree()",
          "",
          "        ET.indent(tree, space=\"\", level=0)",
          "",
          "        self._pad_with_newlines(tree)",
          "",
          "        return unescape_xml_chars(ET.tostring(tree, encoding=\"unicode\"))",
          "",
          "",
          "class Line(BaseModel):",
          "    source_line_no: Optional[int] = None",
          "    target_line_no: Optional[int] = None",
          "    diff_line_no: Optional[int] = None",
          "    value: str",
          "    line_type: Literal[\" \", \"+\", \"-\"]",
          "",
          "",
          "def raw_lines_to_lines(",
          "    lines: list[str],",
          "    source_start: int,",
          "    target_start: int,",
          "    target_line_to_extra: dict[int, str] | None = None,",
          ") -> list[Line]:",
          "    lines_after_header = lines[1:]",
          "    result = []",
          "    current_source_line = source_start",
          "    current_target_line = target_start",
          "    target_line_to_extra = target_line_to_extra or {}",
          "",
          "    for line in lines_after_header:",
          "        line_type = line[0]",
          "",
          "        if current_target_line in target_line_to_extra:",
          "            extra = target_line_to_extra[current_target_line]",
          "            line = line + extra",
          "",
          "        if line_type == \" \":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "            current_target_line += 1",
          "        elif line_type == \"+\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=None,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_target_line += 1",
          "        elif line_type == \"-\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=None,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "        elif line_type == \"\\\\\":",
          "            # Skip the \"\\ No newline at end of file\" marker",
          "            continue",
          "        else:",
          "            raise ValueError(f\"Invalid line type: {line_type}\")",
          "",
          "    return result",
          "",
          "",
          "def right_justified(min_num: int, max_num: int) -> list[str]:",
          "    max_digits = len(str(max_num))",
          "    return [f\"{number:>{max_digits}}\" for number in range(min_num, max_num + 1)]",
          "",
          "",
          "class Hunk(BaseModel):",
          "    source_start: int",
          "    source_length: int",
          "    target_start: int",
          "    target_length: int",
          "    section_header: str",
          "    lines: list[Line]",
          "",
          "    def raw(self) -> str:",
          "        \"\"\"",
          "        The raw hunk, like you see in `git diff` or the original patch string.",
          "        \"\"\"",
          "        return \"\\n\".join((self.section_header, *(line.value for line in self.lines)))",
          "",
          "    def annotated(",
          "        self,",
          "        max_digits_source: int | None = None,",
          "        max_digits_target: int | None = None,",
          "        source_target_delim: str = \"    \",",
          "    ) -> str:",
          "        \"\"\"",
          "        Hunk string with line numbers for the source and target, like you see in GitHub.",
          "        \"\"\"",
          "        if max_digits_source is None:",
          "            max_digits_source = len(str(self.lines[-1].source_line_no))",
          "        if max_digits_target is None:",
          "            max_digits_target = len(str(self.lines[-1].target_line_no))",
          "        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)",
          "        # +2 for the whitespace and then the line type character",
          "        return \"\\n\".join(",
          "            (",
          "                f\"{header_start}{self.section_header}\",",
          "                *(",
          "                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"",
          "                    for line in self.lines",
          "                ),",
          "            )",
          "        )",
          "",
          "",
          "def annotate_hunks(hunks: list[Hunk]) -> list[str]:",
          "    \"\"\"",
          "    Hunks annotated with line numbers for the source and target, like you see in GitHub.",
          "    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.",
          "    \"\"\"",
          "    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)",
          "    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)",
          "    return [",
          "        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)",
          "        for hunk in hunks",
          "    ]",
          "",
          "",
          "class FilePatch(BaseModel):",
          "    type: Literal[\"A\", \"M\", \"D\"]",
          "    path: str",
          "    added: int",
          "    removed: int",
          "    source_file: str",
          "    target_file: str",
          "    hunks: list[Hunk]",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.type == \"A\":",
          "            if file_contents is not None and file_contents.strip():",
          "                raise FileChangeError(\"Cannot add a file that already exists.\")",
          "            return self._apply_hunks([])",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for modify or delete operations.\")",
          "",
          "        if self.type == \"D\":",
          "            return None",
          "",
          "        # For M type",
          "        try:",
          "            new_contents = self._apply_hunks(file_contents.splitlines(keepends=True))",
          "        except Exception as e:",
          "            raise FileChangeError(f\"Error applying hunks: {e}\")",
          "",
          "        # Preserve any trailing characters from original",
          "        if file_contents:",
          "            trailing = file_contents[len(file_contents.rstrip()) :]",
          "            return new_contents + trailing",
          "",
          "        return new_contents",
          "",
          "    def _apply_hunks(self, lines: list[str]) -> str:",
          "        result = []",
          "        current_line = 0",
          "",
          "        for hunk in self.hunks:",
          "            # Add unchanged lines before the hunk",
          "            result.extend(lines[current_line : hunk.source_start - 1])",
          "            current_line = hunk.source_start - 1",
          "",
          "            for line in hunk.lines:",
          "                if line.line_type == \"+\":",
          "                    result.append(line.value + (\"\\n\" if not line.value.endswith(\"\\n\") else \"\"))",
          "                elif line.line_type == \" \":",
          "                    result.append(lines[current_line])",
          "                    current_line += 1",
          "                elif line.line_type == \"-\":",
          "                    current_line += 1",
          "",
          "        # Add any remaining unchanged lines after the last hunk",
          "        result.extend(lines[current_line:])",
          "",
          "        return \"\".join(result).rstrip(\"\\n\")",
          "",
          "    @staticmethod",
          "    def to_hunks(patch: str, target_line_to_extra: dict[int, str] | None = None) -> list[Hunk]:",
          "        hunk_header_pattern = r\"@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@\"",
          "",
          "        hunks: list[Hunk] = []",
          "        lines = patch.splitlines()",
          "        current_hunk = Hunk(",
          "            source_start=0,",
          "            source_length=0,",
          "            target_start=0,",
          "            target_length=0,",
          "            section_header=\"\",",
          "            lines=[],",
          "        )",
          "        current_lines: list[str] = []",
          "",
          "        for line in lines:",
          "            match = re.match(hunk_header_pattern, line)",
          "            if match:",
          "                if current_lines:",
          "                    current_hunk.lines = raw_lines_to_lines(",
          "                        current_lines,",
          "                        current_hunk.source_start,",
          "                        current_hunk.target_start,",
          "                        target_line_to_extra=target_line_to_extra,",
          "                    )",
          "                    hunks.append(current_hunk)",
          "                    current_lines = []",
          "                source_start, source_length, target_start, target_length = map(int, match.groups())",
          "                current_hunk = Hunk(",
          "                    source_start=source_start,",
          "                    source_length=source_length,",
          "                    target_start=target_start,",
          "                    target_length=target_length,",
          "                    section_header=line,",
          "                    lines=[],",
          "                )",
          "                current_lines = [line]  # starts with section header",
          "            elif current_lines:",
          "                current_lines.append(line)",
          "",
          "        if current_lines:",
          "            current_hunk.lines = raw_lines_to_lines(",
          "                current_lines,",
          "                current_hunk.source_start,",
          "                current_hunk.target_start,",
          "                target_line_to_extra=target_line_to_extra,",
          "            )",
          "            hunks.append(current_hunk)",
          "",
          "        return hunks",
          "",
          "",
          "class FileChangeError(Exception):",
          "    pass",
          "",
          "",
          "class FileChange(BaseModel):",
          "    change_type: Literal[\"create\", \"edit\", \"delete\"]",
          "    path: str",
          "    reference_snippet: Optional[str] = None",
          "    new_snippet: Optional[str] = None",
          "    description: Optional[str] = None",
          "    commit_message: Optional[str] = None",
          "    tool_call_id: Optional[str] = None",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.change_type == \"create\":",
          "            if file_contents is not None and file_contents != \"\":",
          "                raise FileChangeError(\"Cannot create a file that already exists.\")",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for creating a file.\")",
          "            return self.new_snippet",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for non-create operations.\")",
          "",
          "        if self.change_type == \"edit\":",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for editing a file.\")",
          "            if self.reference_snippet is None:",
          "                raise FileChangeError(\"Reference snippet must be provided for editing a file.\")",
          "            return file_contents.replace(self.reference_snippet, self.new_snippet)",
          "",
          "        # Delete",
          "        if self.reference_snippet is None:",
          "            return None",
          "",
          "        return file_contents.replace(self.reference_snippet, \"\")",
          "",
          "",
          "class SeerProjectPreference(BaseModel):",
          "    organization_id: int",
          "    project_id: int",
          "    repositories: list[RepoDefinition]",
          "",
          "    def to_db_model(self) -> DbSeerProjectPreference:",
          "        return DbSeerProjectPreference(",
          "            organization_id=self.organization_id,",
          "            project_id=self.project_id,",
          "            repositories=[repo.model_dump() for repo in self.repositories],",
          "        )",
          "",
          "    @classmethod",
          "    def from_db_model(cls, db_model: DbSeerProjectPreference) -> \"SeerProjectPreference\":",
          "        return cls(",
          "            organization_id=db_model.organization_id,",
          "            project_id=db_model.project_id,",
          "            repositories=db_model.repositories,",
          "        )",
          "",
          "",
          "class EAPTrace(BaseModel):",
          "    trace_id: str = Field(..., description=\"ID of the trace\")",
          "    trace: list[dict] = Field(..., description=\"List of spans in the trace\")",
          "",
          "    def _get_transaction_spans(self, trace: list[dict] | dict) -> list[dict]:",
          "        \"\"\"",
          "        Filters the trace to only include the transaction spans.",
          "        \"\"\"",
          "        if not trace:",
          "            return []",
          "",
          "        if isinstance(trace, list):",
          "            transaction_spans = []",
          "            for span in trace:",
          "                transaction_spans.extend(self._get_transaction_spans(span))",
          "            return transaction_spans",
          "",
          "        transaction_spans = []",
          "",
          "        if trace.get(\"is_transaction\"):",
          "            transaction_span = trace.copy()",
          "            transaction_span[\"children\"] = self._get_transaction_spans(trace.get(\"children\", []))",
          "            transaction_spans.append(transaction_span)",
          "        else:",
          "            transaction_spans.extend(self._get_transaction_spans(trace.get(\"children\", [])))",
          "",
          "        return transaction_spans",
          "",
          "    def get_and_format_trace(self, only_transactions=False) -> str:",
          "        \"\"\"",
          "        Formats the trace as a string of tags.",
          "        \"\"\"",
          "        trace = self.trace",
          "        if only_transactions:",
          "            trace = self._get_transaction_spans(self.trace)",
          "",
          "        def format_span_as_tag(span, depth=0):",
          "            indent = \"    \" * depth",
          "",
          "            attrs = []",
          "            for key, value in span.items():",
          "                # Ignore event_type since all events are marked as \"span\"",
          "                if key not in [\"children\", \"event_type\"]:",
          "                    attrs.append(f'{key}=\"{value}\"')",
          "            attrs_str = \" \".join(attrs)",
          "",
          "            tag_name = \"txn\" if span.get(\"is_transaction\") else \"span\"",
          "",
          "            if not span.get(\"children\"):",
          "                return f\"{indent}<{tag_name} {attrs_str} />\"",
          "            else:",
          "                tag_start = f\"{indent}<{tag_name} {attrs_str}>\"",
          "",
          "                # Format children recursively with increased depth",
          "                children = []",
          "                for child in span[\"children\"]:",
          "                    children.append(format_span_as_tag(child, depth + 1))",
          "",
          "                child_content = \"\\n\" + \"\\n\".join(children)",
          "                tag_end = f\"\\n{indent}</{tag_name}>\"",
          "",
          "                return f\"{tag_start}{child_content}{tag_end}\"",
          "",
          "        formatted = []",
          "        for span in trace:",
          "            formatted.append(format_span_as_tag(span, 0))",
          "",
          "        return \"\\n\".join(formatted)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/models.py",
      "image": "seer.automation.models",
      "is_application": true,
      "line": 1135,
      "name": "Hunk.annotated",
      "path": "/app/src/seer/automation/models.py",
      "codeContext": {
        "file": "seer/automation/models.py",
        "line": 1135,
        "name": "Hunk.annotated",
        "code": "        Hunk string with line numbers for the source and target, like you see in GitHub.\n        \"\"\"\n        if max_digits_source is None:\n            max_digits_source = len(str(self.lines[-1].source_line_no))\n        if max_digits_target is None:\n            max_digits_target = len(str(self.lines[-1].target_line_no))\n        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)\n        # +2 for the whitespace and then the line type character\n        return \"\\n\".join(\n            (\n                f\"{header_start}{self.section_header}\",\n                *(\n                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"\n                    for line in self.lines\n                ),\n            )\n        )\n\n\ndef annotate_hunks(hunks: list[Hunk]) -> list[str]:",
        "lineRange": {
          "start": 1126,
          "end": 1145
        },
        "lines": [
          "import json",
          "import re",
          "import textwrap",
          "from typing import Annotated, Any, Literal, NotRequired, Optional",
          "from xml.etree import ElementTree as ET",
          "",
          "import sentry_sdk",
          "from johen.examples import Examples",
          "from johen.generators import specialized",
          "from pydantic import (",
          "    AliasChoices,",
          "    AliasGenerator,",
          "    BaseModel,",
          "    ConfigDict,",
          "    Field,",
          "    ValidationError,",
          "    ValidationInfo,",
          "    field_validator,",
          "    model_validator,",
          ")",
          "from pydantic.alias_generators import to_camel, to_snake",
          "from pydantic_xml import BaseXmlModel",
          "from typing_extensions import TypedDict",
          "",
          "from seer.automation.utils import (",
          "    format_dict,",
          "    format_list,",
          "    process_repo_provider,",
          "    unescape_xml_chars,",
          ")",
          "from seer.db import DbSeerProjectPreference",
          "",
          "",
          "class StacktraceFrame(BaseModel):",
          "    model_config = ConfigDict(",
          "        alias_generator=AliasGenerator(",
          "            validation_alias=lambda k: AliasChoices(to_camel(k), to_snake(k)),",
          "            serialization_alias=to_camel,",
          "        )",
          "    )",
          "",
          "    function: Optional[Annotated[str, Examples(specialized.ascii_words)]] = None",
          "    filename: Optional[Annotated[str, Examples(specialized.file_names)]]",
          "    abs_path: Optional[Annotated[str, Examples(specialized.file_paths)]]",
          "    line_no: Optional[int]",
          "    col_no: Optional[int]",
          "    context: list[tuple[int, Optional[str]]] = []",
          "    repo_name: Optional[str] = None",
          "    in_app: bool | None = False",
          "    vars: Optional[dict[str, Any]] = None",
          "    package: Optional[str] = None",
          "",
          "    @field_validator(\"vars\", mode=\"before\")",
          "    @classmethod",
          "    def validate_vars(cls, vars: Optional[dict[str, Any]], info: ValidationInfo):",
          "        if not vars or \"context\" not in info.data or not info.data[\"context\"]:",
          "            return vars",
          "        code_str = \"\"",
          "        for _, line in info.data[\"context\"]:",
          "            code_str += line + \"\\n\"",
          "        return cls._trim_vars(vars, code_str)",
          "",
          "    @staticmethod",
          "    def _trim_vars(vars: dict[str, Any], code_context: str):",
          "        # only keep variables mentioned in the context of the stacktrace frame",
          "        # and filter out any values containing \"[Filtered]\"",
          "        trimmed_vars = {}",
          "        for key, val in vars.items():",
          "            if key in code_context:",
          "                if isinstance(val, (dict, list)):",
          "                    filtered_val = StacktraceFrame._filter_nested_value(val)",
          "                    if filtered_val is not None:",
          "                        trimmed_vars[key] = filtered_val",
          "                elif not StacktraceFrame._contains_filtered(val):",
          "                    trimmed_vars[key] = val",
          "        return trimmed_vars",
          "",
          "    @staticmethod",
          "    def _filter_nested_value(value: Any) -> Any:",
          "        if isinstance(value, dict):",
          "            filtered_dict = {}",
          "            for k, v in value.items():",
          "                if isinstance(v, (dict, list)):",
          "                    filtered_v = StacktraceFrame._filter_nested_value(v)",
          "                    if filtered_v is not None:",
          "                        filtered_dict[k] = filtered_v",
          "                elif not StacktraceFrame._contains_filtered(v):",
          "                    filtered_dict[k] = v",
          "            return filtered_dict if filtered_dict else None",
          "        elif isinstance(value, list):",
          "            filtered_list = []",
          "            for item in value:",
          "                if isinstance(item, (dict, list)):",
          "                    filtered_item = StacktraceFrame._filter_nested_value(item)",
          "                    if filtered_item is not None:",
          "                        filtered_list.append(filtered_item)",
          "                elif not StacktraceFrame._contains_filtered(item):",
          "                    filtered_list.append(item)",
          "            return filtered_list if filtered_list else None",
          "        return None if StacktraceFrame._contains_filtered(value) else value",
          "",
          "    @staticmethod",
          "    def _contains_filtered(value: Any) -> bool:",
          "        return isinstance(value, str) and \"[Filtered]\" in value",
          "",
          "",
          "class SentryFrame(TypedDict):",
          "    absPath: Optional[str]",
          "    colNo: Optional[int]",
          "    context: list[tuple[int, str]]",
          "    filename: NotRequired[Optional[str]]",
          "    function: NotRequired[Optional[str]]",
          "    inApp: NotRequired[bool]",
          "    instructionAddr: NotRequired[Optional[str]]",
          "    lineNo: NotRequired[Optional[int]]",
          "    module: NotRequired[Optional[str]]",
          "    package: NotRequired[Optional[str]]",
          "    platform: NotRequired[Optional[str]]",
          "    rawFunction: NotRequired[Optional[str]]",
          "    symbol: NotRequired[Optional[str]]",
          "    symbolAddr: NotRequired[Optional[str]]",
          "    trust: NotRequired[Optional[Any]]",
          "    vars: NotRequired[Optional[dict[str, Any]]]",
          "    addrMode: NotRequired[Optional[str]]",
          "    isPrefix: NotRequired[bool]",
          "    isSentinel: NotRequired[bool]",
          "    lock: NotRequired[Optional[Any]]",
          "    map: NotRequired[Optional[str]]",
          "    mapUrl: NotRequired[Optional[str]]",
          "    minGroupingLevel: NotRequired[int]",
          "    origAbsPath: NotRequired[Optional[str]]",
          "    sourceLink: NotRequired[Optional[str]]",
          "    symbolicatorStatus: NotRequired[Optional[Any]]",
          "",
          "",
          "class Stacktrace(BaseModel):",
          "    frames: list[StacktraceFrame]",
          "",
          "    @field_validator(\"frames\", mode=\"before\")",
          "    @classmethod",
          "    def validate_frames(cls, frames: list[StacktraceFrame | SentryFrame]):",
          "        stacktrace_frames = []",
          "        for frame in frames:",
          "            if isinstance(frame, dict):",
          "                if \"function\" not in frame:",
          "                    frame[\"function\"] = None",
          "                try:",
          "                    stacktrace_frames.append(StacktraceFrame.model_validate(frame))",
          "                except ValidationError:",
          "                    sentry_sdk.capture_exception()",
          "                    continue",
          "            else:",
          "                stacktrace_frames.append(frame)",
          "",
          "        return cls._trim_frames(stacktrace_frames)",
          "",
          "    def to_str(",
          "        self,",
          "        max_frames: int = 16,",
          "        in_app_only: bool = False,",
          "        include_context: bool = True,",
          "        include_var_values: bool = True,",
          "    ):",
          "        stack_str = \"\"",
          "",
          "        frames = self.frames",
          "        if in_app_only:",
          "            frames = [frame for frame in frames if frame.in_app]",
          "",
          "        for frame in reversed(frames[-max_frames:]):",
          "            col_no_str = f\", column {frame.col_no}\" if frame.col_no is not None else \"\"",
          "            repo_str = f\" in repo {frame.repo_name}\" if frame.repo_name else \"\"",
          "            line_no_str = (",
          "                f\"[Line {frame.line_no}{col_no_str}]\"",
          "                if frame.line_no is not None",
          "                else \"[Line: Unknown]\"",
          "            )",
          "",
          "            function = frame.function if frame.function else \"Unknown function\"",
          "            if frame.filename:",
          "                stack_str += f\" {function} in file {frame.filename}{repo_str} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            elif frame.package:",
          "                stack_str += f\" {function} in package {frame.package} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            else:",
          "                stack_str += f\" {function} in unknown file {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "",
          "            if include_context:",
          "                for ctx in frame.context:",
          "                    is_suspect_line = ctx[0] == frame.line_no",
          "                    stack_str += f\"{ctx[1]}{'  <-- SUSPECT LINE' if is_suspect_line else ''}\\n\"",
          "",
          "            if frame.vars:",
          "                if include_var_values:",
          "                    vars_title = \"Variable values at the time of the exception:\"",
          "                    vars_str = format_dict(frame.vars)",
          "                else:",
          "                    vars_title = \"Variables at the time of the exception:\"",
          "                    vars_str = \", \".join(frame.vars.keys())",
          "",
          "                stack_str += textwrap.dedent(",
          "                    \"\"\"\\",
          "                    ---",
          "                    {vars_title}:",
          "                    {vars_str}",
          "                    \"\"\"",
          "                ).format(vars_title=vars_title, vars_str=vars_str)",
          "            stack_str += \"------\\n\"",
          "",
          "        return stack_str",
          "",
          "    @staticmethod",
          "    def _trim_frames(frames: list[StacktraceFrame], frame_allowance=16):",
          "        frames_len = len(frames)",
          "        if frames_len <= frame_allowance:",
          "            return frames",
          "",
          "        app_frames = [frame for frame in frames if frame.in_app]",
          "        system_frames = [frame for frame in frames if not frame.in_app]",
          "",
          "        app_count = len(app_frames)",
          "        system_allowance = max(frame_allowance - app_count, 0)",
          "        app_allowance = frame_allowance - system_allowance",
          "",
          "        if system_allowance > 0:",
          "            # prioritize trimming system frames",
          "            half_system = system_allowance // 2",
          "            kept_system_frames = system_frames[:half_system] + system_frames[-half_system:]",
          "        else:",
          "            kept_system_frames = []",
          "",
          "        if app_allowance > 0:",
          "            half_app = app_allowance // 2",
          "            kept_app_frames = app_frames[:half_app] + app_frames[-half_app:]",
          "        else:",
          "            kept_app_frames = []",
          "",
          "        # combine and sort the kept frames based on their original order",
          "        kept_frames = kept_system_frames + kept_app_frames",
          "        kept_frames.sort(key=lambda frame: frames.index(frame))",
          "        return kept_frames",
          "",
          "",
          "class SentryStacktrace(TypedDict):",
          "    frames: list[SentryFrame]",
          "",
          "",
          "class SentryEventEntryDataValue(TypedDict):",
          "    type: str",
          "    value: str",
          "    stacktrace: SentryStacktrace",
          "",
          "",
          "class SentryExceptionEventData(TypedDict):",
          "    values: list[SentryEventEntryDataValue]",
          "",
          "",
          "class SentryExceptionEntry(BaseModel):",
          "    type: Literal[\"exception\"]",
          "    data: SentryExceptionEventData",
          "",
          "",
          "class SentryEventData(TypedDict):",
          "    title: str",
          "    entries: list[dict]",
          "    tags: NotRequired[list[dict[str, str | None]]]",
          "",
          "",
          "class ExceptionMechanism(TypedDict):",
          "    type: str",
          "    handled: NotRequired[bool]",
          "",
          "",
          "class ExceptionDetails(BaseModel):",
          "    type: Optional[str] = \"\"",
          "    value: Optional[str] = \"\"",
          "    stacktrace: Optional[Stacktrace] = None",
          "    mechanism: Optional[ExceptionMechanism] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class ThreadDetails(BaseModel):",
          "    id: Optional[int | str] = None",
          "    name: Optional[str] = None",
          "    crashed: Optional[bool] = False",
          "    current: Optional[bool] = False",
          "    state: Optional[str] = None",
          "    main: Optional[bool] = False",
          "",
          "    stacktrace: Optional[Stacktrace] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace | None):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class BreadcrumbsDetails(BaseModel):",
          "    type: Optional[str] = None",
          "    message: Optional[str] = None",
          "    category: Optional[str] = None",
          "    data: Optional[dict] = None",
          "    level: Optional[str] = None",
          "",
          "",
          "class RequestDetails(BaseModel):",
          "    url: str | None = None",
          "    method: str | None = None",
          "    data: dict[str, Any] | str | list[Any] | None = None",
          "    # not including cookies, headers, env, query, etc. for now",
          "",
          "",
          "class EventDetails(BaseModel):",
          "    title: str",
          "    message: str | None = None",
          "    transaction_name: str | None = None",
          "    exceptions: list[ExceptionDetails] = Field(default_factory=list, exclude=False)",
          "    threads: list[ThreadDetails] = Field(default_factory=list, exclude=False)",
          "    breadcrumbs: list[BreadcrumbsDetails] = Field(default_factory=list, exclude=False)",
          "    stacktraces: list[Stacktrace] = Field(default_factory=list, exclude=False)",
          "    request: RequestDetails | None = None",
          "",
          "    @classmethod",
          "    def from_event(cls, error_event: SentryEventData):",
          "        MAX_THREADS = 8  # TODO: Smarter logic for max threads",
          "",
          "        exceptions: list[ExceptionDetails] = []",
          "        threads: list[ThreadDetails] = []",
          "        breadcrumbs: list[BreadcrumbsDetails] = []",
          "        stacktraces: list[Stacktrace] = []",
          "        transaction_name: str | None = None",
          "        message: str | None = None",
          "        request: RequestDetails | None = None",
          "",
          "        for tag in error_event.get(\"tags\", []):",
          "            if tag.get(\"key\") == \"transaction\":",
          "                transaction_name = tag.get(\"value\")",
          "",
          "        for entry in error_event.get(\"entries\", []):",
          "            if entry.get(\"type\") == \"exception\":",
          "                for exception in entry.get(\"data\", {}).get(\"values\", []):",
          "                    exceptions.append(ExceptionDetails.model_validate(exception))",
          "            elif entry.get(\"type\") == \"threads\":",
          "                for thread in entry.get(\"data\", {}).get(\"values\", []):",
          "                    thread_details = ThreadDetails.model_validate(thread)",
          "                    if (",
          "                        thread_details.stacktrace",
          "                        and thread_details.stacktrace.frames",
          "                        and len(threads) < MAX_THREADS",
          "                    ):",
          "                        threads.append(thread_details)",
          "            elif entry.get(\"type\") == \"breadcrumbs\":",
          "                all_breadcrumbs = entry.get(\"data\", {}).get(\"values\", [])",
          "                for breadcrumb in all_breadcrumbs[-10:]:  # only look at the most recent breadcrumbs",
          "                    # Skip breadcrumbs with filtered content in message or data",
          "                    if StacktraceFrame._contains_filtered(",
          "                        breadcrumb.get(\"message\")",
          "                    ) or StacktraceFrame._contains_filtered(str(breadcrumb.get(\"data\"))):",
          "                        continue",
          "                    crumb_details = BreadcrumbsDetails.model_validate(breadcrumb)",
          "                    breadcrumbs.append(crumb_details)",
          "            elif entry.get(\"type\") == \"stacktrace\":",
          "                stacktraces.append(Stacktrace.model_validate(entry.get(\"data\", {})))",
          "            elif entry.get(\"type\") == \"message\":",
          "                message = entry.get(\"data\", {}).get(\"formatted\", None)",
          "            elif entry.get(\"type\") == \"request\":",
          "                request = RequestDetails.model_validate(entry.get(\"data\", {}))",
          "",
          "        return cls(",
          "            title=error_event.get(\"title\"),",
          "            transaction_name=transaction_name,",
          "            exceptions=exceptions,",
          "            threads=threads,",
          "            breadcrumbs=breadcrumbs,",
          "            message=message,",
          "            stacktraces=stacktraces,",
          "            request=request,",
          "        )",
          "",
          "    def format_event(self):",
          "        exceptions = self.format_exceptions()",
          "        breadcrumbs = self.format_breadcrumbs()",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "        stacktraces = self.format_stacktraces()",
          "        request = self.format_request()",
          "",
          "        return (",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "            {title} {transaction}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            {breadcrumbs}",
          "            {request}",
          "            \"\"\"",
          "            )",
          "            .format(",
          "                title=self.title,",
          "                transaction=(",
          "                    f\"(occurred in: {self.transaction_name})\" if self.transaction_name else \"\"",
          "                ),",
          "                message=f\"\\n<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "                exceptions=(",
          "                    f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\"",
          "                ),",
          "                stacktraces=(",
          "                    f\"\\n<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "                ),",
          "                breadcrumbs=(",
          "                    f\"\\n<breadcrumb_logs>\\n{breadcrumbs}\\n</breadcrumb_logs>\"",
          "                    if breadcrumbs.strip()",
          "                    else \"\"",
          "                ),",
          "                request=f\"\\n<http_request>\\n{request}\\n</http_request>\" if request.strip() else \"\",",
          "            )",
          "            .strip()",
          "        )",
          "",
          "    def format_event_without_breadcrumbs(",
          "        self, include_context: bool = True, include_var_values: bool = True",
          "    ):",
          "        exceptions = self.format_exceptions(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        stacktraces = self.format_stacktraces(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {title}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            \"\"\"",
          "        ).format(",
          "            title=self.title,",
          "            exceptions=f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\",",
          "            stacktraces=(",
          "                f\"<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "            ),",
          "            message=f\"<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "        )",
          "",
          "    def format_exceptions(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <exception_{i}{handled}{exception_type}{exception_message}>",
          "                    {stacktrace}",
          "                    </exception{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                exception_type=f' type=\"{exception.type}\"' if exception.type else \"\",",
          "                exception_message=f' message=\"{exception.value}\"' if exception.value else \"\",",
          "                stacktrace=(",
          "                    exception.stacktrace.to_str(",
          "                        include_context=include_context,",
          "                        include_var_values=include_var_values,",
          "                    )",
          "                    if exception.stacktrace",
          "                    else \"\"",
          "                ),",
          "                handled=(",
          "                    f' is_exception_handled=\"{\"yes\" if exception.mechanism.get(\"handled\") else \"no\"}\"'",
          "                    if exception.mechanism and exception.mechanism.get(\"handled\", None) is not None",
          "                    else \"\"",
          "                ),",
          "            )",
          "            for i, exception in enumerate(self.exceptions)",
          "        )",
          "",
          "    def format_threads(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <thread_{thread_id} name=\"{thread_name}\" is_current=\"{thread_current}\" state=\"{thread_state}\" is_main=\"{thread_main}\" crashed=\"{thread_crashed}\">",
          "                    <stacktrace>",
          "                    {stacktrace}",
          "                    </stacktrace>",
          "                    </thread_{thread_id}>\"\"\"",
          "            ).format(",
          "                thread_id=thread.id,",
          "                thread_name=thread.name,",
          "                thread_state=thread.state,",
          "                thread_current=thread.current,",
          "                thread_crashed=thread.crashed,",
          "                thread_main=thread.main,",
          "                stacktrace=thread.stacktrace.to_str() if thread.stacktrace else \"\",",
          "            )",
          "            for thread in self.threads",
          "        )",
          "",
          "    def format_breadcrumbs(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <breadcrumb_{i}{breadcrumb_type}{breadcrumb_category}{level}>",
          "                {content}",
          "                </breadcrumb_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                breadcrumb_type=f' type=\"{breadcrumb.type}\"' if breadcrumb.type else \"\",",
          "                breadcrumb_category=(",
          "                    f' category=\"{breadcrumb.category}\"' if breadcrumb.category else \"\"",
          "                ),",
          "                content=\"\\n\".join(",
          "                    filter(",
          "                        None,",
          "                        [",
          "                            f\"{breadcrumb.message}\\n\" if breadcrumb.message else \"\",",
          "                            (",
          "                                f\"{str({k: v for k, v in breadcrumb.data.items() if v})}\\n\"",
          "                                if breadcrumb.data",
          "                                else \"\"",
          "                            ),",
          "                        ],",
          "                    )",
          "                ),",
          "                level=f' level=\"{breadcrumb.level}\"' if breadcrumb.level else \"\",",
          "            )",
          "            for i, breadcrumb in enumerate(self.breadcrumbs)",
          "        )",
          "",
          "    def format_stacktraces(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <stacktrace_{i}>",
          "                {stacktrace}",
          "                </stacktrace_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                stacktrace=stacktrace.to_str(",
          "                    include_context=include_context, include_var_values=include_var_values",
          "                ),",
          "            )",
          "            for i, stacktrace in enumerate(self.stacktraces)",
          "        )",
          "",
          "    def format_request(self):",
          "        if not self.request:",
          "            return \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {method} {url}",
          "            {data}",
          "            \"\"\"",
          "        ).format(",
          "            method=self.request.method if self.request.method else \"\",",
          "            url=self.request.url if self.request.url else \"\",",
          "            data=(",
          "                f\"Body:\\n{format_dict(self.request.data)}\"",
          "                if self.request.data and isinstance(self.request.data, dict)",
          "                else (",
          "                    f\"Body:\\n{format_list(self.request.data)}\"",
          "                    if self.request.data and isinstance(self.request.data, list)",
          "                    else (f\"Body:\\n{self.request.data}\" if self.request.data else \"\")",
          "                )",
          "            ),",
          "        )",
          "",
          "",
          "class IssueDetails(BaseModel):",
          "    id: Annotated[int, Examples(specialized.unsigned_ints)]",
          "    title: Annotated[str, Examples(specialized.ascii_words)]",
          "    short_id: Optional[str] = None",
          "    events: list[SentryEventData]",
          "",
          "",
          "class ProfileFrame(TypedDict):",
          "    function: str",
          "    module: str",
          "    filename: str",
          "    lineno: int",
          "    in_app: bool",
          "    duration_ns: NotRequired[float]",
          "    children: NotRequired[list[\"ProfileFrame\"]]",
          "",
          "",
          "class Profile(BaseModel):",
          "    profile_matches_issue: bool = Field(default=False)",
          "    execution_tree: list[ProfileFrame] = Field(default_factory=list)",
          "    relevant_functions: set[str] = Field(default_factory=set)",
          "",
          "    def format_profile(",
          "        self,",
          "        context_before: int = 20,",
          "        context_after: int = 3,",
          "    ) -> str:",
          "        \"\"\"",
          "        Format the profile tree, focusing on relevant functions from the stacktrace.",
          "",
          "        Args:",
          "            context_before: Number of lines to include before first relevant function",
          "            context_after: Number of lines to include after last relevant function",
          "",
          "        Returns:",
          "            str: Formatted profile string, showing relevant sections of the execution tree",
          "        \"\"\"",
          "        full_profile = self._format_profile_helper(self.execution_tree)",
          "",
          "        if self.relevant_functions:",
          "            relevant_window = self._get_relevant_code_window(",
          "                full_profile, context_before=context_before, context_after=context_after",
          "            )",
          "            if relevant_window:",
          "                return relevant_window",
          "",
          "        return full_profile",
          "",
          "    def _get_relevant_code_window(",
          "        self, code: str, context_before: int = 20, context_after: int = 3",
          "    ) -> str | None:",
          "        \"\"\"",
          "        Find the relevant section of code containing functions from the stacktrace.",
          "        Expands the selection to include context lines before and after.",
          "",
          "        Args:",
          "            code: Multi-line string of formatted profile to analyze",
          "            context_before: Number of lines to include before first relevant line",
          "            context_after: Number of lines to include after last relevant line",
          "",
          "        Returns:",
          "            str | None: Selected profile window with context, or None if no relevant functions found",
          "        \"\"\"",
          "        if not self.relevant_functions or not code:",
          "            return None",
          "",
          "        lines = code.splitlines()",
          "        first_relevant_line = None",
          "        last_relevant_line = None",
          "",
          "        # Find first and last lines containing relevant functions",
          "        for i, line in enumerate(lines):",
          "            if any(func in line for func in self.relevant_functions):",
          "                if first_relevant_line is None:",
          "                    first_relevant_line = i",
          "                last_relevant_line = i",
          "",
          "        if first_relevant_line is None:",
          "            first_relevant_line = 0",
          "        if last_relevant_line is None:",
          "            last_relevant_line = len(lines) - 1",
          "",
          "        # Calculate window boundaries with context",
          "        start_line = max(0, first_relevant_line - context_before)",
          "        end_line = min(len(lines), last_relevant_line + context_after + 1)",
          "",
          "        result = []",
          "        if start_line > 0:",
          "            result.append(\"...\")",
          "        result.extend(lines[start_line:end_line])",
          "        if end_line < len(lines):",
          "            result.append(\"...\")",
          "",
          "        return \"\\n\".join(result)",
          "",
          "    def _format_profile_helper(self, tree: list[ProfileFrame], prefix: str = \"\") -> str:",
          "        \"\"\"",
          "        Returns a pretty-printed string representation of the execution tree using tree formatting.",
          "",
          "        Args:",
          "            tree: List of dictionaries representing the execution tree",
          "            prefix: Current line prefix for tree structure",
          "",
          "        Returns:",
          "            str: Formatted string representation of the tree",
          "        \"\"\"",
          "        if not tree:",
          "            return \"\"",
          "",
          "        result = []",
          "",
          "        for i, node in enumerate(tree):",
          "            is_last = i == len(tree) - 1",
          "",
          "            # Format the current node",
          "            function_name = node.get(\"function\", \"Unknown function\")",
          "            filename = node.get(\"filename\", \"Unknown file\")",
          "            location_info = f\" ({filename}:{node.get('lineno', '')})\"",
          "            duration_ms = node.get(\"duration_ns\", -1) / 1_000_000",
          "            duration_info = f\" - {duration_ms:.0f}ms\" if duration_ms >= 0 else \"\"",
          "",
          "            # Add tree structure characters",
          "            if is_last:",
          "                result.append(f\"{prefix}└─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                result.append(f\"{prefix}├─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Recursively format children",
          "            if node.get(\"children\"):",
          "                children_str = self._format_profile_helper(node.get(\"children\", []), child_prefix)",
          "                if children_str:",
          "                    result.append(children_str)",
          "",
          "        return \"\\n\".join(result)",
          "",
          "",
          "class Span(BaseModel):",
          "    span_id: str",
          "    title: str | None = None",
          "    data: dict[str, Any] | None = None",
          "    duration: str | None = None",
          "    children: list[\"Span\"] = Field(default_factory=list)",
          "",
          "",
          "class TraceEvent(BaseModel):",
          "    event_id: str | None = None",
          "    title: str | None = None",
          "    is_transaction: bool = False",
          "    is_error: bool = False",
          "    platform: str | None = None",
          "    is_current_project: bool = True",
          "    project_slug: str | None = None",
          "    project_id: int | None = None",
          "    duration: str | None = None",
          "    profile_id: str | None = None",
          "    children: list[\"TraceEvent\"] = Field(default_factory=list)",
          "    spans: list[Span] = Field(default_factory=list)",
          "",
          "    def format_spans_tree(self) -> str:",
          "        \"\"\"",
          "        Returns a formatted string representation of the span tree.",
          "        \"\"\"",
          "        if not self.spans:",
          "            return \"No spans available\"",
          "",
          "        lines = [f\"Spans for {self.title or 'Unnamed Event'}\"]",
          "        self._format_spans(self.spans, \"\", lines)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_spans(self, spans: list[Span], prefix: str, lines: list[str]) -> None:",
          "        \"\"\"",
          "        Helper method to recursively format spans.",
          "",
          "        Args:",
          "            spans: List of spans to format",
          "            prefix: Current prefix for tree structure",
          "            lines: List of lines being built",
          "        \"\"\"",
          "        if not spans:",
          "            return",
          "",
          "        # Group consecutive similar spans",
          "        grouped_spans: list[list[Span]] = []",
          "        current_group: list[Span] | None = None",
          "",
          "        for span in spans:",
          "            if current_group and self._are_spans_similar(current_group[0], span):",
          "                current_group.append(span)",
          "            else:",
          "                if current_group:",
          "                    grouped_spans.append(current_group)",
          "                current_group = [span]",
          "",
          "        if current_group:",
          "            grouped_spans.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_spans):",
          "            is_last = i == len(grouped_spans) - 1",
          "            span = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted span line",
          "            span_line = self._format_span_line(span) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {span_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "                data_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {span_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "                data_prefix = f\"{prefix}│  \"",
          "",
          "            # Add the span data as JSON if available",
          "            if span.data:",
          "                data_str = json.dumps(span.data, indent=2)",
          "                data_lines = data_str.split(\"\\n\")",
          "                for data_line in data_lines:",
          "                    lines.append(f\"{data_prefix} {data_line}\")",
          "",
          "            # Process children",
          "            if span.children:",
          "                self._format_spans(span.children, child_prefix, lines)",
          "",
          "    def _format_span_line(self, span: Span) -> str:",
          "        \"\"\"Format a single span line.\"\"\"",
          "        parts = []",
          "",
          "        title = span.title or \"Unnamed Span\"",
          "        parts.append(title)",
          "        if span.duration:",
          "            parts.append(f\"({span.duration})\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_spans_similar(self, span1: Span, span2: Span) -> bool:",
          "        \"\"\"Check if two spans are similar enough to be grouped together.\"\"\"",
          "        if span1.title != span2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(span1.children) != len(span2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children are similar",
          "        for i in range(len(span1.children)):",
          "            if not self._are_spans_similar(span1.children[i], span2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "",
          "class TraceTree(BaseModel):",
          "    trace_id: str | None = None",
          "    org_id: int | None = None",
          "    events: list[TraceEvent] = Field(default_factory=list)  # only expecting transactions and errors",
          "",
          "    def format_trace_tree(self):",
          "        if not self.events:",
          "            return \"Trace (empty)\"",
          "",
          "        lines = [\"Trace\"]",
          "        self._format_events(self.events, \"\", lines, is_last_child=True)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_events(",
          "        self, events: list[TraceEvent], prefix: str, lines: list[str], is_last_child: bool",
          "    ):",
          "        if not events:",
          "            return",
          "",
          "        # Group consecutive similar events",
          "        grouped_events: list[list[TraceEvent]] = []",
          "        current_group: list[TraceEvent] | None = None",
          "",
          "        for event in events:",
          "            if current_group and self._are_events_similar(current_group[0], event):",
          "                current_group.append(event)",
          "            else:",
          "                if current_group:",
          "                    grouped_events.append(current_group)",
          "                current_group = [event]",
          "",
          "        if current_group:",
          "            grouped_events.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_events):",
          "            is_last = i == len(grouped_events) - 1",
          "            event = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted event line",
          "            event_line = self._format_event_line(event) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {event_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {event_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Process children",
          "            if event.children:",
          "                self._format_events(event.children, child_prefix, lines, is_last)",
          "",
          "    def _format_event_line(self, event: TraceEvent) -> str:",
          "        parts = []",
          "",
          "        # Add ERROR prefix if not a transaction",
          "        prefix = \"ERROR: \" if event.is_error else \"\"",
          "",
          "        # Add title",
          "        title = event.title or \"Unnamed Event\"",
          "        parts.append(f\"{prefix}{title}\")",
          "",
          "        # Add duration if it exists",
          "        if event.duration:",
          "            parts.append(f\"({event.duration})\")",
          "",
          "        # Add event_id (first 7 digits)",
          "        if event.event_id:",
          "            parts.append(f\"(event ID: {event.event_id[:7]})\")",
          "",
          "        # Add project",
          "        if event.project_slug and event.project_id:",
          "            project_str = f\"(project: {event.project_slug})\"",
          "            parts.append(project_str)",
          "",
          "        # Add platform",
          "        if event.platform:",
          "            parts.append(f\"({event.platform})\")",
          "",
          "        # Add profile",
          "        if event.profile_id:",
          "            parts.append(\"(profile available)\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_events_similar(self, event1: TraceEvent, event2: TraceEvent) -> bool:",
          "        \"\"\"Check if two events are similar enough to be grouped together\"\"\"",
          "        if event1.title != event2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(event1.children) != len(event2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children match",
          "        # This is a simplified check - for a full check we'd need to recursively compare children",
          "        for i in range(len(event1.children)):",
          "            if not self._are_events_similar(event1.children[i], event2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "    def get_full_event_id(self, truncated_id: str) -> str | None:",
          "        \"\"\"Return the full event_id given the first 7 characters\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event.event_id",
          "        return None",
          "",
          "    def get_event_by_id(self, truncated_id: str) -> TraceEvent | None:",
          "        \"\"\"Return the full TraceEvent object given a truncated event ID\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event",
          "        return None",
          "",
          "    def _get_all_events(self) -> list[TraceEvent]:",
          "        \"\"\"Return a flattened list of all events in the tree\"\"\"",
          "        all_events = []",
          "",
          "        def collect_events(events):",
          "            for event in events:",
          "                all_events.append(event)",
          "                if event.children:",
          "                    collect_events(event.children)",
          "",
          "        collect_events(self.events)",
          "        return all_events",
          "",
          "    def get_all_project_ids(self) -> list[int]:",
          "        \"\"\"Return a list of all project IDs in the trace tree\"\"\"",
          "        return list(set(event.project_id for event in self._get_all_events() if event.project_id))",
          "",
          "",
          "class RepoDefinition(BaseModel):",
          "    provider: Annotated[str, Examples((\"github\", \"integrations:github\"))]",
          "    owner: str",
          "    name: str",
          "    external_id: Annotated[str, Examples(specialized.ascii_words)]",
          "    branch_name: str | None = Field(",
          "        default=None,",
          "        description=\"The branch that Autofix will work on, otherwise the default branch will be used.\",",
          "    )",
          "    instructions: str | None = Field(",
          "        default=None,",
          "        description=\"Custom instructions when working in this repo.\",",
          "    )",
          "    base_commit_sha: str | None = None",
          "    provider_raw: str | None = None",
          "",
          "    @property",
          "    def full_name(self):",
          "        return f\"{self.owner}/{self.name}\"",
          "",
          "    @model_validator(mode=\"before\")",
          "    @classmethod",
          "    def store_provider_raw(cls, data):",
          "        if isinstance(data, dict) and \"provider\" in data and \"provider_raw\" not in data:",
          "            data[\"provider_raw\"] = data[\"provider\"]",
          "        return data",
          "",
          "    @field_validator(\"provider\", mode=\"after\")",
          "    @classmethod",
          "    def validate_provider(cls, provider: str):",
          "        return process_repo_provider(provider)",
          "",
          "    def __hash__(self):",
          "        return hash((self.provider, self.owner, self.name, self.external_id))",
          "",
          "",
          "class InitializationError(Exception):",
          "    pass",
          "",
          "",
          "class PromptXmlModel(BaseXmlModel):",
          "    def _pad_with_newlines(self, tree: ET.Element) -> None:",
          "        for elem in tree.iter():",
          "            if elem.text:",
          "                stripped = elem.text.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.text = \"\\n\" + stripped + \"\\n\"",
          "            if elem.tail:",
          "                stripped = elem.tail.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.tail = \"\\n\" + stripped + \"\\n\"",
          "",
          "    def to_prompt_str(self) -> str:",
          "        tree: ET.Element = self.to_xml_tree()",
          "",
          "        ET.indent(tree, space=\"\", level=0)",
          "",
          "        self._pad_with_newlines(tree)",
          "",
          "        return unescape_xml_chars(ET.tostring(tree, encoding=\"unicode\"))",
          "",
          "",
          "class Line(BaseModel):",
          "    source_line_no: Optional[int] = None",
          "    target_line_no: Optional[int] = None",
          "    diff_line_no: Optional[int] = None",
          "    value: str",
          "    line_type: Literal[\" \", \"+\", \"-\"]",
          "",
          "",
          "def raw_lines_to_lines(",
          "    lines: list[str],",
          "    source_start: int,",
          "    target_start: int,",
          "    target_line_to_extra: dict[int, str] | None = None,",
          ") -> list[Line]:",
          "    lines_after_header = lines[1:]",
          "    result = []",
          "    current_source_line = source_start",
          "    current_target_line = target_start",
          "    target_line_to_extra = target_line_to_extra or {}",
          "",
          "    for line in lines_after_header:",
          "        line_type = line[0]",
          "",
          "        if current_target_line in target_line_to_extra:",
          "            extra = target_line_to_extra[current_target_line]",
          "            line = line + extra",
          "",
          "        if line_type == \" \":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "            current_target_line += 1",
          "        elif line_type == \"+\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=None,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_target_line += 1",
          "        elif line_type == \"-\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=None,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "        elif line_type == \"\\\\\":",
          "            # Skip the \"\\ No newline at end of file\" marker",
          "            continue",
          "        else:",
          "            raise ValueError(f\"Invalid line type: {line_type}\")",
          "",
          "    return result",
          "",
          "",
          "def right_justified(min_num: int, max_num: int) -> list[str]:",
          "    max_digits = len(str(max_num))",
          "    return [f\"{number:>{max_digits}}\" for number in range(min_num, max_num + 1)]",
          "",
          "",
          "class Hunk(BaseModel):",
          "    source_start: int",
          "    source_length: int",
          "    target_start: int",
          "    target_length: int",
          "    section_header: str",
          "    lines: list[Line]",
          "",
          "    def raw(self) -> str:",
          "        \"\"\"",
          "        The raw hunk, like you see in `git diff` or the original patch string.",
          "        \"\"\"",
          "        return \"\\n\".join((self.section_header, *(line.value for line in self.lines)))",
          "",
          "    def annotated(",
          "        self,",
          "        max_digits_source: int | None = None,",
          "        max_digits_target: int | None = None,",
          "        source_target_delim: str = \"    \",",
          "    ) -> str:",
          "        \"\"\"",
          "        Hunk string with line numbers for the source and target, like you see in GitHub.",
          "        \"\"\"",
          "        if max_digits_source is None:",
          "            max_digits_source = len(str(self.lines[-1].source_line_no))",
          "        if max_digits_target is None:",
          "            max_digits_target = len(str(self.lines[-1].target_line_no))",
          "        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)",
          "        # +2 for the whitespace and then the line type character",
          "        return \"\\n\".join(",
          "            (",
          "                f\"{header_start}{self.section_header}\",",
          "                *(",
          "                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"",
          "                    for line in self.lines",
          "                ),",
          "            )",
          "        )",
          "",
          "",
          "def annotate_hunks(hunks: list[Hunk]) -> list[str]:",
          "    \"\"\"",
          "    Hunks annotated with line numbers for the source and target, like you see in GitHub.",
          "    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.",
          "    \"\"\"",
          "    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)",
          "    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)",
          "    return [",
          "        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)",
          "        for hunk in hunks",
          "    ]",
          "",
          "",
          "class FilePatch(BaseModel):",
          "    type: Literal[\"A\", \"M\", \"D\"]",
          "    path: str",
          "    added: int",
          "    removed: int",
          "    source_file: str",
          "    target_file: str",
          "    hunks: list[Hunk]",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.type == \"A\":",
          "            if file_contents is not None and file_contents.strip():",
          "                raise FileChangeError(\"Cannot add a file that already exists.\")",
          "            return self._apply_hunks([])",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for modify or delete operations.\")",
          "",
          "        if self.type == \"D\":",
          "            return None",
          "",
          "        # For M type",
          "        try:",
          "            new_contents = self._apply_hunks(file_contents.splitlines(keepends=True))",
          "        except Exception as e:",
          "            raise FileChangeError(f\"Error applying hunks: {e}\")",
          "",
          "        # Preserve any trailing characters from original",
          "        if file_contents:",
          "            trailing = file_contents[len(file_contents.rstrip()) :]",
          "            return new_contents + trailing",
          "",
          "        return new_contents",
          "",
          "    def _apply_hunks(self, lines: list[str]) -> str:",
          "        result = []",
          "        current_line = 0",
          "",
          "        for hunk in self.hunks:",
          "            # Add unchanged lines before the hunk",
          "            result.extend(lines[current_line : hunk.source_start - 1])",
          "            current_line = hunk.source_start - 1",
          "",
          "            for line in hunk.lines:",
          "                if line.line_type == \"+\":",
          "                    result.append(line.value + (\"\\n\" if not line.value.endswith(\"\\n\") else \"\"))",
          "                elif line.line_type == \" \":",
          "                    result.append(lines[current_line])",
          "                    current_line += 1",
          "                elif line.line_type == \"-\":",
          "                    current_line += 1",
          "",
          "        # Add any remaining unchanged lines after the last hunk",
          "        result.extend(lines[current_line:])",
          "",
          "        return \"\".join(result).rstrip(\"\\n\")",
          "",
          "    @staticmethod",
          "    def to_hunks(patch: str, target_line_to_extra: dict[int, str] | None = None) -> list[Hunk]:",
          "        hunk_header_pattern = r\"@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@\"",
          "",
          "        hunks: list[Hunk] = []",
          "        lines = patch.splitlines()",
          "        current_hunk = Hunk(",
          "            source_start=0,",
          "            source_length=0,",
          "            target_start=0,",
          "            target_length=0,",
          "            section_header=\"\",",
          "            lines=[],",
          "        )",
          "        current_lines: list[str] = []",
          "",
          "        for line in lines:",
          "            match = re.match(hunk_header_pattern, line)",
          "            if match:",
          "                if current_lines:",
          "                    current_hunk.lines = raw_lines_to_lines(",
          "                        current_lines,",
          "                        current_hunk.source_start,",
          "                        current_hunk.target_start,",
          "                        target_line_to_extra=target_line_to_extra,",
          "                    )",
          "                    hunks.append(current_hunk)",
          "                    current_lines = []",
          "                source_start, source_length, target_start, target_length = map(int, match.groups())",
          "                current_hunk = Hunk(",
          "                    source_start=source_start,",
          "                    source_length=source_length,",
          "                    target_start=target_start,",
          "                    target_length=target_length,",
          "                    section_header=line,",
          "                    lines=[],",
          "                )",
          "                current_lines = [line]  # starts with section header",
          "            elif current_lines:",
          "                current_lines.append(line)",
          "",
          "        if current_lines:",
          "            current_hunk.lines = raw_lines_to_lines(",
          "                current_lines,",
          "                current_hunk.source_start,",
          "                current_hunk.target_start,",
          "                target_line_to_extra=target_line_to_extra,",
          "            )",
          "            hunks.append(current_hunk)",
          "",
          "        return hunks",
          "",
          "",
          "class FileChangeError(Exception):",
          "    pass",
          "",
          "",
          "class FileChange(BaseModel):",
          "    change_type: Literal[\"create\", \"edit\", \"delete\"]",
          "    path: str",
          "    reference_snippet: Optional[str] = None",
          "    new_snippet: Optional[str] = None",
          "    description: Optional[str] = None",
          "    commit_message: Optional[str] = None",
          "    tool_call_id: Optional[str] = None",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.change_type == \"create\":",
          "            if file_contents is not None and file_contents != \"\":",
          "                raise FileChangeError(\"Cannot create a file that already exists.\")",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for creating a file.\")",
          "            return self.new_snippet",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for non-create operations.\")",
          "",
          "        if self.change_type == \"edit\":",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for editing a file.\")",
          "            if self.reference_snippet is None:",
          "                raise FileChangeError(\"Reference snippet must be provided for editing a file.\")",
          "            return file_contents.replace(self.reference_snippet, self.new_snippet)",
          "",
          "        # Delete",
          "        if self.reference_snippet is None:",
          "            return None",
          "",
          "        return file_contents.replace(self.reference_snippet, \"\")",
          "",
          "",
          "class SeerProjectPreference(BaseModel):",
          "    organization_id: int",
          "    project_id: int",
          "    repositories: list[RepoDefinition]",
          "",
          "    def to_db_model(self) -> DbSeerProjectPreference:",
          "        return DbSeerProjectPreference(",
          "            organization_id=self.organization_id,",
          "            project_id=self.project_id,",
          "            repositories=[repo.model_dump() for repo in self.repositories],",
          "        )",
          "",
          "    @classmethod",
          "    def from_db_model(cls, db_model: DbSeerProjectPreference) -> \"SeerProjectPreference\":",
          "        return cls(",
          "            organization_id=db_model.organization_id,",
          "            project_id=db_model.project_id,",
          "            repositories=db_model.repositories,",
          "        )",
          "",
          "",
          "class EAPTrace(BaseModel):",
          "    trace_id: str = Field(..., description=\"ID of the trace\")",
          "    trace: list[dict] = Field(..., description=\"List of spans in the trace\")",
          "",
          "    def _get_transaction_spans(self, trace: list[dict] | dict) -> list[dict]:",
          "        \"\"\"",
          "        Filters the trace to only include the transaction spans.",
          "        \"\"\"",
          "        if not trace:",
          "            return []",
          "",
          "        if isinstance(trace, list):",
          "            transaction_spans = []",
          "            for span in trace:",
          "                transaction_spans.extend(self._get_transaction_spans(span))",
          "            return transaction_spans",
          "",
          "        transaction_spans = []",
          "",
          "        if trace.get(\"is_transaction\"):",
          "            transaction_span = trace.copy()",
          "            transaction_span[\"children\"] = self._get_transaction_spans(trace.get(\"children\", []))",
          "            transaction_spans.append(transaction_span)",
          "        else:",
          "            transaction_spans.extend(self._get_transaction_spans(trace.get(\"children\", [])))",
          "",
          "        return transaction_spans",
          "",
          "    def get_and_format_trace(self, only_transactions=False) -> str:",
          "        \"\"\"",
          "        Formats the trace as a string of tags.",
          "        \"\"\"",
          "        trace = self.trace",
          "        if only_transactions:",
          "            trace = self._get_transaction_spans(self.trace)",
          "",
          "        def format_span_as_tag(span, depth=0):",
          "            indent = \"    \" * depth",
          "",
          "            attrs = []",
          "            for key, value in span.items():",
          "                # Ignore event_type since all events are marked as \"span\"",
          "                if key not in [\"children\", \"event_type\"]:",
          "                    attrs.append(f'{key}=\"{value}\"')",
          "            attrs_str = \" \".join(attrs)",
          "",
          "            tag_name = \"txn\" if span.get(\"is_transaction\") else \"span\"",
          "",
          "            if not span.get(\"children\"):",
          "                return f\"{indent}<{tag_name} {attrs_str} />\"",
          "            else:",
          "                tag_start = f\"{indent}<{tag_name} {attrs_str}>\"",
          "",
          "                # Format children recursively with increased depth",
          "                children = []",
          "                for child in span[\"children\"]:",
          "                    children.append(format_span_as_tag(child, depth + 1))",
          "",
          "                child_content = \"\\n\" + \"\\n\".join(children)",
          "                tag_end = f\"\\n{indent}</{tag_name}>\"",
          "",
          "                return f\"{tag_start}{child_content}{tag_end}\"",
          "",
          "        formatted = []",
          "        for span in trace:",
          "            formatted.append(format_span_as_tag(span, 0))",
          "",
          "        return \"\\n\".join(formatted)",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/models.py",
      "image": "seer.automation.models",
      "is_application": true,
      "line": 1137,
      "name": "Hunk.annotated.<locals>.<genexpr>",
      "path": "/app/src/seer/automation/models.py",
      "codeContext": {
        "file": "seer/automation/models.py",
        "line": 1137,
        "name": "Hunk.annotated.<locals>.<genexpr>",
        "code": "        if max_digits_source is None:\n            max_digits_source = len(str(self.lines[-1].source_line_no))\n        if max_digits_target is None:\n            max_digits_target = len(str(self.lines[-1].target_line_no))\n        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)\n        # +2 for the whitespace and then the line type character\n        return \"\\n\".join(\n            (\n                f\"{header_start}{self.section_header}\",\n                *(\n                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"\n                    for line in self.lines\n                ),\n            )\n        )\n\n\ndef annotate_hunks(hunks: list[Hunk]) -> list[str]:\n    \"\"\"\n    Hunks annotated with line numbers for the source and target, like you see in GitHub.",
        "lineRange": {
          "start": 1128,
          "end": 1147
        },
        "lines": [
          "import json",
          "import re",
          "import textwrap",
          "from typing import Annotated, Any, Literal, NotRequired, Optional",
          "from xml.etree import ElementTree as ET",
          "",
          "import sentry_sdk",
          "from johen.examples import Examples",
          "from johen.generators import specialized",
          "from pydantic import (",
          "    AliasChoices,",
          "    AliasGenerator,",
          "    BaseModel,",
          "    ConfigDict,",
          "    Field,",
          "    ValidationError,",
          "    ValidationInfo,",
          "    field_validator,",
          "    model_validator,",
          ")",
          "from pydantic.alias_generators import to_camel, to_snake",
          "from pydantic_xml import BaseXmlModel",
          "from typing_extensions import TypedDict",
          "",
          "from seer.automation.utils import (",
          "    format_dict,",
          "    format_list,",
          "    process_repo_provider,",
          "    unescape_xml_chars,",
          ")",
          "from seer.db import DbSeerProjectPreference",
          "",
          "",
          "class StacktraceFrame(BaseModel):",
          "    model_config = ConfigDict(",
          "        alias_generator=AliasGenerator(",
          "            validation_alias=lambda k: AliasChoices(to_camel(k), to_snake(k)),",
          "            serialization_alias=to_camel,",
          "        )",
          "    )",
          "",
          "    function: Optional[Annotated[str, Examples(specialized.ascii_words)]] = None",
          "    filename: Optional[Annotated[str, Examples(specialized.file_names)]]",
          "    abs_path: Optional[Annotated[str, Examples(specialized.file_paths)]]",
          "    line_no: Optional[int]",
          "    col_no: Optional[int]",
          "    context: list[tuple[int, Optional[str]]] = []",
          "    repo_name: Optional[str] = None",
          "    in_app: bool | None = False",
          "    vars: Optional[dict[str, Any]] = None",
          "    package: Optional[str] = None",
          "",
          "    @field_validator(\"vars\", mode=\"before\")",
          "    @classmethod",
          "    def validate_vars(cls, vars: Optional[dict[str, Any]], info: ValidationInfo):",
          "        if not vars or \"context\" not in info.data or not info.data[\"context\"]:",
          "            return vars",
          "        code_str = \"\"",
          "        for _, line in info.data[\"context\"]:",
          "            code_str += line + \"\\n\"",
          "        return cls._trim_vars(vars, code_str)",
          "",
          "    @staticmethod",
          "    def _trim_vars(vars: dict[str, Any], code_context: str):",
          "        # only keep variables mentioned in the context of the stacktrace frame",
          "        # and filter out any values containing \"[Filtered]\"",
          "        trimmed_vars = {}",
          "        for key, val in vars.items():",
          "            if key in code_context:",
          "                if isinstance(val, (dict, list)):",
          "                    filtered_val = StacktraceFrame._filter_nested_value(val)",
          "                    if filtered_val is not None:",
          "                        trimmed_vars[key] = filtered_val",
          "                elif not StacktraceFrame._contains_filtered(val):",
          "                    trimmed_vars[key] = val",
          "        return trimmed_vars",
          "",
          "    @staticmethod",
          "    def _filter_nested_value(value: Any) -> Any:",
          "        if isinstance(value, dict):",
          "            filtered_dict = {}",
          "            for k, v in value.items():",
          "                if isinstance(v, (dict, list)):",
          "                    filtered_v = StacktraceFrame._filter_nested_value(v)",
          "                    if filtered_v is not None:",
          "                        filtered_dict[k] = filtered_v",
          "                elif not StacktraceFrame._contains_filtered(v):",
          "                    filtered_dict[k] = v",
          "            return filtered_dict if filtered_dict else None",
          "        elif isinstance(value, list):",
          "            filtered_list = []",
          "            for item in value:",
          "                if isinstance(item, (dict, list)):",
          "                    filtered_item = StacktraceFrame._filter_nested_value(item)",
          "                    if filtered_item is not None:",
          "                        filtered_list.append(filtered_item)",
          "                elif not StacktraceFrame._contains_filtered(item):",
          "                    filtered_list.append(item)",
          "            return filtered_list if filtered_list else None",
          "        return None if StacktraceFrame._contains_filtered(value) else value",
          "",
          "    @staticmethod",
          "    def _contains_filtered(value: Any) -> bool:",
          "        return isinstance(value, str) and \"[Filtered]\" in value",
          "",
          "",
          "class SentryFrame(TypedDict):",
          "    absPath: Optional[str]",
          "    colNo: Optional[int]",
          "    context: list[tuple[int, str]]",
          "    filename: NotRequired[Optional[str]]",
          "    function: NotRequired[Optional[str]]",
          "    inApp: NotRequired[bool]",
          "    instructionAddr: NotRequired[Optional[str]]",
          "    lineNo: NotRequired[Optional[int]]",
          "    module: NotRequired[Optional[str]]",
          "    package: NotRequired[Optional[str]]",
          "    platform: NotRequired[Optional[str]]",
          "    rawFunction: NotRequired[Optional[str]]",
          "    symbol: NotRequired[Optional[str]]",
          "    symbolAddr: NotRequired[Optional[str]]",
          "    trust: NotRequired[Optional[Any]]",
          "    vars: NotRequired[Optional[dict[str, Any]]]",
          "    addrMode: NotRequired[Optional[str]]",
          "    isPrefix: NotRequired[bool]",
          "    isSentinel: NotRequired[bool]",
          "    lock: NotRequired[Optional[Any]]",
          "    map: NotRequired[Optional[str]]",
          "    mapUrl: NotRequired[Optional[str]]",
          "    minGroupingLevel: NotRequired[int]",
          "    origAbsPath: NotRequired[Optional[str]]",
          "    sourceLink: NotRequired[Optional[str]]",
          "    symbolicatorStatus: NotRequired[Optional[Any]]",
          "",
          "",
          "class Stacktrace(BaseModel):",
          "    frames: list[StacktraceFrame]",
          "",
          "    @field_validator(\"frames\", mode=\"before\")",
          "    @classmethod",
          "    def validate_frames(cls, frames: list[StacktraceFrame | SentryFrame]):",
          "        stacktrace_frames = []",
          "        for frame in frames:",
          "            if isinstance(frame, dict):",
          "                if \"function\" not in frame:",
          "                    frame[\"function\"] = None",
          "                try:",
          "                    stacktrace_frames.append(StacktraceFrame.model_validate(frame))",
          "                except ValidationError:",
          "                    sentry_sdk.capture_exception()",
          "                    continue",
          "            else:",
          "                stacktrace_frames.append(frame)",
          "",
          "        return cls._trim_frames(stacktrace_frames)",
          "",
          "    def to_str(",
          "        self,",
          "        max_frames: int = 16,",
          "        in_app_only: bool = False,",
          "        include_context: bool = True,",
          "        include_var_values: bool = True,",
          "    ):",
          "        stack_str = \"\"",
          "",
          "        frames = self.frames",
          "        if in_app_only:",
          "            frames = [frame for frame in frames if frame.in_app]",
          "",
          "        for frame in reversed(frames[-max_frames:]):",
          "            col_no_str = f\", column {frame.col_no}\" if frame.col_no is not None else \"\"",
          "            repo_str = f\" in repo {frame.repo_name}\" if frame.repo_name else \"\"",
          "            line_no_str = (",
          "                f\"[Line {frame.line_no}{col_no_str}]\"",
          "                if frame.line_no is not None",
          "                else \"[Line: Unknown]\"",
          "            )",
          "",
          "            function = frame.function if frame.function else \"Unknown function\"",
          "            if frame.filename:",
          "                stack_str += f\" {function} in file {frame.filename}{repo_str} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            elif frame.package:",
          "                stack_str += f\" {function} in package {frame.package} {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "            else:",
          "                stack_str += f\" {function} in unknown file {line_no_str} ({'In app' if frame.in_app else 'Not in app'})\\n\"",
          "",
          "            if include_context:",
          "                for ctx in frame.context:",
          "                    is_suspect_line = ctx[0] == frame.line_no",
          "                    stack_str += f\"{ctx[1]}{'  <-- SUSPECT LINE' if is_suspect_line else ''}\\n\"",
          "",
          "            if frame.vars:",
          "                if include_var_values:",
          "                    vars_title = \"Variable values at the time of the exception:\"",
          "                    vars_str = format_dict(frame.vars)",
          "                else:",
          "                    vars_title = \"Variables at the time of the exception:\"",
          "                    vars_str = \", \".join(frame.vars.keys())",
          "",
          "                stack_str += textwrap.dedent(",
          "                    \"\"\"\\",
          "                    ---",
          "                    {vars_title}:",
          "                    {vars_str}",
          "                    \"\"\"",
          "                ).format(vars_title=vars_title, vars_str=vars_str)",
          "            stack_str += \"------\\n\"",
          "",
          "        return stack_str",
          "",
          "    @staticmethod",
          "    def _trim_frames(frames: list[StacktraceFrame], frame_allowance=16):",
          "        frames_len = len(frames)",
          "        if frames_len <= frame_allowance:",
          "            return frames",
          "",
          "        app_frames = [frame for frame in frames if frame.in_app]",
          "        system_frames = [frame for frame in frames if not frame.in_app]",
          "",
          "        app_count = len(app_frames)",
          "        system_allowance = max(frame_allowance - app_count, 0)",
          "        app_allowance = frame_allowance - system_allowance",
          "",
          "        if system_allowance > 0:",
          "            # prioritize trimming system frames",
          "            half_system = system_allowance // 2",
          "            kept_system_frames = system_frames[:half_system] + system_frames[-half_system:]",
          "        else:",
          "            kept_system_frames = []",
          "",
          "        if app_allowance > 0:",
          "            half_app = app_allowance // 2",
          "            kept_app_frames = app_frames[:half_app] + app_frames[-half_app:]",
          "        else:",
          "            kept_app_frames = []",
          "",
          "        # combine and sort the kept frames based on their original order",
          "        kept_frames = kept_system_frames + kept_app_frames",
          "        kept_frames.sort(key=lambda frame: frames.index(frame))",
          "        return kept_frames",
          "",
          "",
          "class SentryStacktrace(TypedDict):",
          "    frames: list[SentryFrame]",
          "",
          "",
          "class SentryEventEntryDataValue(TypedDict):",
          "    type: str",
          "    value: str",
          "    stacktrace: SentryStacktrace",
          "",
          "",
          "class SentryExceptionEventData(TypedDict):",
          "    values: list[SentryEventEntryDataValue]",
          "",
          "",
          "class SentryExceptionEntry(BaseModel):",
          "    type: Literal[\"exception\"]",
          "    data: SentryExceptionEventData",
          "",
          "",
          "class SentryEventData(TypedDict):",
          "    title: str",
          "    entries: list[dict]",
          "    tags: NotRequired[list[dict[str, str | None]]]",
          "",
          "",
          "class ExceptionMechanism(TypedDict):",
          "    type: str",
          "    handled: NotRequired[bool]",
          "",
          "",
          "class ExceptionDetails(BaseModel):",
          "    type: Optional[str] = \"\"",
          "    value: Optional[str] = \"\"",
          "    stacktrace: Optional[Stacktrace] = None",
          "    mechanism: Optional[ExceptionMechanism] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class ThreadDetails(BaseModel):",
          "    id: Optional[int | str] = None",
          "    name: Optional[str] = None",
          "    crashed: Optional[bool] = False",
          "    current: Optional[bool] = False",
          "    state: Optional[str] = None",
          "    main: Optional[bool] = False",
          "",
          "    stacktrace: Optional[Stacktrace] = None",
          "",
          "    @field_validator(\"stacktrace\", mode=\"before\")",
          "    @classmethod",
          "    def validate_stacktrace(cls, sentry_stacktrace: SentryStacktrace | Stacktrace | None):",
          "        return (",
          "            Stacktrace.model_validate(sentry_stacktrace)",
          "            if isinstance(sentry_stacktrace, dict)",
          "            else sentry_stacktrace",
          "        )",
          "",
          "",
          "class BreadcrumbsDetails(BaseModel):",
          "    type: Optional[str] = None",
          "    message: Optional[str] = None",
          "    category: Optional[str] = None",
          "    data: Optional[dict] = None",
          "    level: Optional[str] = None",
          "",
          "",
          "class RequestDetails(BaseModel):",
          "    url: str | None = None",
          "    method: str | None = None",
          "    data: dict[str, Any] | str | list[Any] | None = None",
          "    # not including cookies, headers, env, query, etc. for now",
          "",
          "",
          "class EventDetails(BaseModel):",
          "    title: str",
          "    message: str | None = None",
          "    transaction_name: str | None = None",
          "    exceptions: list[ExceptionDetails] = Field(default_factory=list, exclude=False)",
          "    threads: list[ThreadDetails] = Field(default_factory=list, exclude=False)",
          "    breadcrumbs: list[BreadcrumbsDetails] = Field(default_factory=list, exclude=False)",
          "    stacktraces: list[Stacktrace] = Field(default_factory=list, exclude=False)",
          "    request: RequestDetails | None = None",
          "",
          "    @classmethod",
          "    def from_event(cls, error_event: SentryEventData):",
          "        MAX_THREADS = 8  # TODO: Smarter logic for max threads",
          "",
          "        exceptions: list[ExceptionDetails] = []",
          "        threads: list[ThreadDetails] = []",
          "        breadcrumbs: list[BreadcrumbsDetails] = []",
          "        stacktraces: list[Stacktrace] = []",
          "        transaction_name: str | None = None",
          "        message: str | None = None",
          "        request: RequestDetails | None = None",
          "",
          "        for tag in error_event.get(\"tags\", []):",
          "            if tag.get(\"key\") == \"transaction\":",
          "                transaction_name = tag.get(\"value\")",
          "",
          "        for entry in error_event.get(\"entries\", []):",
          "            if entry.get(\"type\") == \"exception\":",
          "                for exception in entry.get(\"data\", {}).get(\"values\", []):",
          "                    exceptions.append(ExceptionDetails.model_validate(exception))",
          "            elif entry.get(\"type\") == \"threads\":",
          "                for thread in entry.get(\"data\", {}).get(\"values\", []):",
          "                    thread_details = ThreadDetails.model_validate(thread)",
          "                    if (",
          "                        thread_details.stacktrace",
          "                        and thread_details.stacktrace.frames",
          "                        and len(threads) < MAX_THREADS",
          "                    ):",
          "                        threads.append(thread_details)",
          "            elif entry.get(\"type\") == \"breadcrumbs\":",
          "                all_breadcrumbs = entry.get(\"data\", {}).get(\"values\", [])",
          "                for breadcrumb in all_breadcrumbs[-10:]:  # only look at the most recent breadcrumbs",
          "                    # Skip breadcrumbs with filtered content in message or data",
          "                    if StacktraceFrame._contains_filtered(",
          "                        breadcrumb.get(\"message\")",
          "                    ) or StacktraceFrame._contains_filtered(str(breadcrumb.get(\"data\"))):",
          "                        continue",
          "                    crumb_details = BreadcrumbsDetails.model_validate(breadcrumb)",
          "                    breadcrumbs.append(crumb_details)",
          "            elif entry.get(\"type\") == \"stacktrace\":",
          "                stacktraces.append(Stacktrace.model_validate(entry.get(\"data\", {})))",
          "            elif entry.get(\"type\") == \"message\":",
          "                message = entry.get(\"data\", {}).get(\"formatted\", None)",
          "            elif entry.get(\"type\") == \"request\":",
          "                request = RequestDetails.model_validate(entry.get(\"data\", {}))",
          "",
          "        return cls(",
          "            title=error_event.get(\"title\"),",
          "            transaction_name=transaction_name,",
          "            exceptions=exceptions,",
          "            threads=threads,",
          "            breadcrumbs=breadcrumbs,",
          "            message=message,",
          "            stacktraces=stacktraces,",
          "            request=request,",
          "        )",
          "",
          "    def format_event(self):",
          "        exceptions = self.format_exceptions()",
          "        breadcrumbs = self.format_breadcrumbs()",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "        stacktraces = self.format_stacktraces()",
          "        request = self.format_request()",
          "",
          "        return (",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "            {title} {transaction}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            {breadcrumbs}",
          "            {request}",
          "            \"\"\"",
          "            )",
          "            .format(",
          "                title=self.title,",
          "                transaction=(",
          "                    f\"(occurred in: {self.transaction_name})\" if self.transaction_name else \"\"",
          "                ),",
          "                message=f\"\\n<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "                exceptions=(",
          "                    f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\"",
          "                ),",
          "                stacktraces=(",
          "                    f\"\\n<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "                ),",
          "                breadcrumbs=(",
          "                    f\"\\n<breadcrumb_logs>\\n{breadcrumbs}\\n</breadcrumb_logs>\"",
          "                    if breadcrumbs.strip()",
          "                    else \"\"",
          "                ),",
          "                request=f\"\\n<http_request>\\n{request}\\n</http_request>\" if request.strip() else \"\",",
          "            )",
          "            .strip()",
          "        )",
          "",
          "    def format_event_without_breadcrumbs(",
          "        self, include_context: bool = True, include_var_values: bool = True",
          "    ):",
          "        exceptions = self.format_exceptions(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        stacktraces = self.format_stacktraces(",
          "            include_context=include_context, include_var_values=include_var_values",
          "        )",
          "        message = self.message if self.message and self.message not in self.title else \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {title}",
          "            {message}",
          "            {exceptions}",
          "            {stacktraces}",
          "            \"\"\"",
          "        ).format(",
          "            title=self.title,",
          "            exceptions=f\"<exceptions>\\n{exceptions}\\n</exceptions>\" if exceptions.strip() else \"\",",
          "            stacktraces=(",
          "                f\"<stacktraces>\\n{stacktraces}\\n</stacktraces>\" if stacktraces.strip() else \"\"",
          "            ),",
          "            message=f\"<message>\\n{message}\\n</message>\" if message.strip() else \"\",",
          "        )",
          "",
          "    def format_exceptions(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <exception_{i}{handled}{exception_type}{exception_message}>",
          "                    {stacktrace}",
          "                    </exception{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                exception_type=f' type=\"{exception.type}\"' if exception.type else \"\",",
          "                exception_message=f' message=\"{exception.value}\"' if exception.value else \"\",",
          "                stacktrace=(",
          "                    exception.stacktrace.to_str(",
          "                        include_context=include_context,",
          "                        include_var_values=include_var_values,",
          "                    )",
          "                    if exception.stacktrace",
          "                    else \"\"",
          "                ),",
          "                handled=(",
          "                    f' is_exception_handled=\"{\"yes\" if exception.mechanism.get(\"handled\") else \"no\"}\"'",
          "                    if exception.mechanism and exception.mechanism.get(\"handled\", None) is not None",
          "                    else \"\"",
          "                ),",
          "            )",
          "            for i, exception in enumerate(self.exceptions)",
          "        )",
          "",
          "    def format_threads(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                    <thread_{thread_id} name=\"{thread_name}\" is_current=\"{thread_current}\" state=\"{thread_state}\" is_main=\"{thread_main}\" crashed=\"{thread_crashed}\">",
          "                    <stacktrace>",
          "                    {stacktrace}",
          "                    </stacktrace>",
          "                    </thread_{thread_id}>\"\"\"",
          "            ).format(",
          "                thread_id=thread.id,",
          "                thread_name=thread.name,",
          "                thread_state=thread.state,",
          "                thread_current=thread.current,",
          "                thread_crashed=thread.crashed,",
          "                thread_main=thread.main,",
          "                stacktrace=thread.stacktrace.to_str() if thread.stacktrace else \"\",",
          "            )",
          "            for thread in self.threads",
          "        )",
          "",
          "    def format_breadcrumbs(self):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <breadcrumb_{i}{breadcrumb_type}{breadcrumb_category}{level}>",
          "                {content}",
          "                </breadcrumb_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                breadcrumb_type=f' type=\"{breadcrumb.type}\"' if breadcrumb.type else \"\",",
          "                breadcrumb_category=(",
          "                    f' category=\"{breadcrumb.category}\"' if breadcrumb.category else \"\"",
          "                ),",
          "                content=\"\\n\".join(",
          "                    filter(",
          "                        None,",
          "                        [",
          "                            f\"{breadcrumb.message}\\n\" if breadcrumb.message else \"\",",
          "                            (",
          "                                f\"{str({k: v for k, v in breadcrumb.data.items() if v})}\\n\"",
          "                                if breadcrumb.data",
          "                                else \"\"",
          "                            ),",
          "                        ],",
          "                    )",
          "                ),",
          "                level=f' level=\"{breadcrumb.level}\"' if breadcrumb.level else \"\",",
          "            )",
          "            for i, breadcrumb in enumerate(self.breadcrumbs)",
          "        )",
          "",
          "    def format_stacktraces(self, include_context: bool = True, include_var_values: bool = True):",
          "        return \"\\n\".join(",
          "            textwrap.dedent(",
          "                \"\"\"\\",
          "                <stacktrace_{i}>",
          "                {stacktrace}",
          "                </stacktrace_{i}>\"\"\"",
          "            ).format(",
          "                i=i,",
          "                stacktrace=stacktrace.to_str(",
          "                    include_context=include_context, include_var_values=include_var_values",
          "                ),",
          "            )",
          "            for i, stacktrace in enumerate(self.stacktraces)",
          "        )",
          "",
          "    def format_request(self):",
          "        if not self.request:",
          "            return \"\"",
          "",
          "        return textwrap.dedent(",
          "            \"\"\"\\",
          "            {method} {url}",
          "            {data}",
          "            \"\"\"",
          "        ).format(",
          "            method=self.request.method if self.request.method else \"\",",
          "            url=self.request.url if self.request.url else \"\",",
          "            data=(",
          "                f\"Body:\\n{format_dict(self.request.data)}\"",
          "                if self.request.data and isinstance(self.request.data, dict)",
          "                else (",
          "                    f\"Body:\\n{format_list(self.request.data)}\"",
          "                    if self.request.data and isinstance(self.request.data, list)",
          "                    else (f\"Body:\\n{self.request.data}\" if self.request.data else \"\")",
          "                )",
          "            ),",
          "        )",
          "",
          "",
          "class IssueDetails(BaseModel):",
          "    id: Annotated[int, Examples(specialized.unsigned_ints)]",
          "    title: Annotated[str, Examples(specialized.ascii_words)]",
          "    short_id: Optional[str] = None",
          "    events: list[SentryEventData]",
          "",
          "",
          "class ProfileFrame(TypedDict):",
          "    function: str",
          "    module: str",
          "    filename: str",
          "    lineno: int",
          "    in_app: bool",
          "    duration_ns: NotRequired[float]",
          "    children: NotRequired[list[\"ProfileFrame\"]]",
          "",
          "",
          "class Profile(BaseModel):",
          "    profile_matches_issue: bool = Field(default=False)",
          "    execution_tree: list[ProfileFrame] = Field(default_factory=list)",
          "    relevant_functions: set[str] = Field(default_factory=set)",
          "",
          "    def format_profile(",
          "        self,",
          "        context_before: int = 20,",
          "        context_after: int = 3,",
          "    ) -> str:",
          "        \"\"\"",
          "        Format the profile tree, focusing on relevant functions from the stacktrace.",
          "",
          "        Args:",
          "            context_before: Number of lines to include before first relevant function",
          "            context_after: Number of lines to include after last relevant function",
          "",
          "        Returns:",
          "            str: Formatted profile string, showing relevant sections of the execution tree",
          "        \"\"\"",
          "        full_profile = self._format_profile_helper(self.execution_tree)",
          "",
          "        if self.relevant_functions:",
          "            relevant_window = self._get_relevant_code_window(",
          "                full_profile, context_before=context_before, context_after=context_after",
          "            )",
          "            if relevant_window:",
          "                return relevant_window",
          "",
          "        return full_profile",
          "",
          "    def _get_relevant_code_window(",
          "        self, code: str, context_before: int = 20, context_after: int = 3",
          "    ) -> str | None:",
          "        \"\"\"",
          "        Find the relevant section of code containing functions from the stacktrace.",
          "        Expands the selection to include context lines before and after.",
          "",
          "        Args:",
          "            code: Multi-line string of formatted profile to analyze",
          "            context_before: Number of lines to include before first relevant line",
          "            context_after: Number of lines to include after last relevant line",
          "",
          "        Returns:",
          "            str | None: Selected profile window with context, or None if no relevant functions found",
          "        \"\"\"",
          "        if not self.relevant_functions or not code:",
          "            return None",
          "",
          "        lines = code.splitlines()",
          "        first_relevant_line = None",
          "        last_relevant_line = None",
          "",
          "        # Find first and last lines containing relevant functions",
          "        for i, line in enumerate(lines):",
          "            if any(func in line for func in self.relevant_functions):",
          "                if first_relevant_line is None:",
          "                    first_relevant_line = i",
          "                last_relevant_line = i",
          "",
          "        if first_relevant_line is None:",
          "            first_relevant_line = 0",
          "        if last_relevant_line is None:",
          "            last_relevant_line = len(lines) - 1",
          "",
          "        # Calculate window boundaries with context",
          "        start_line = max(0, first_relevant_line - context_before)",
          "        end_line = min(len(lines), last_relevant_line + context_after + 1)",
          "",
          "        result = []",
          "        if start_line > 0:",
          "            result.append(\"...\")",
          "        result.extend(lines[start_line:end_line])",
          "        if end_line < len(lines):",
          "            result.append(\"...\")",
          "",
          "        return \"\\n\".join(result)",
          "",
          "    def _format_profile_helper(self, tree: list[ProfileFrame], prefix: str = \"\") -> str:",
          "        \"\"\"",
          "        Returns a pretty-printed string representation of the execution tree using tree formatting.",
          "",
          "        Args:",
          "            tree: List of dictionaries representing the execution tree",
          "            prefix: Current line prefix for tree structure",
          "",
          "        Returns:",
          "            str: Formatted string representation of the tree",
          "        \"\"\"",
          "        if not tree:",
          "            return \"\"",
          "",
          "        result = []",
          "",
          "        for i, node in enumerate(tree):",
          "            is_last = i == len(tree) - 1",
          "",
          "            # Format the current node",
          "            function_name = node.get(\"function\", \"Unknown function\")",
          "            filename = node.get(\"filename\", \"Unknown file\")",
          "            location_info = f\" ({filename}:{node.get('lineno', '')})\"",
          "            duration_ms = node.get(\"duration_ns\", -1) / 1_000_000",
          "            duration_info = f\" - {duration_ms:.0f}ms\" if duration_ms >= 0 else \"\"",
          "",
          "            # Add tree structure characters",
          "            if is_last:",
          "                result.append(f\"{prefix}└─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                result.append(f\"{prefix}├─ {function_name}{location_info}{duration_info}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Recursively format children",
          "            if node.get(\"children\"):",
          "                children_str = self._format_profile_helper(node.get(\"children\", []), child_prefix)",
          "                if children_str:",
          "                    result.append(children_str)",
          "",
          "        return \"\\n\".join(result)",
          "",
          "",
          "class Span(BaseModel):",
          "    span_id: str",
          "    title: str | None = None",
          "    data: dict[str, Any] | None = None",
          "    duration: str | None = None",
          "    children: list[\"Span\"] = Field(default_factory=list)",
          "",
          "",
          "class TraceEvent(BaseModel):",
          "    event_id: str | None = None",
          "    title: str | None = None",
          "    is_transaction: bool = False",
          "    is_error: bool = False",
          "    platform: str | None = None",
          "    is_current_project: bool = True",
          "    project_slug: str | None = None",
          "    project_id: int | None = None",
          "    duration: str | None = None",
          "    profile_id: str | None = None",
          "    children: list[\"TraceEvent\"] = Field(default_factory=list)",
          "    spans: list[Span] = Field(default_factory=list)",
          "",
          "    def format_spans_tree(self) -> str:",
          "        \"\"\"",
          "        Returns a formatted string representation of the span tree.",
          "        \"\"\"",
          "        if not self.spans:",
          "            return \"No spans available\"",
          "",
          "        lines = [f\"Spans for {self.title or 'Unnamed Event'}\"]",
          "        self._format_spans(self.spans, \"\", lines)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_spans(self, spans: list[Span], prefix: str, lines: list[str]) -> None:",
          "        \"\"\"",
          "        Helper method to recursively format spans.",
          "",
          "        Args:",
          "            spans: List of spans to format",
          "            prefix: Current prefix for tree structure",
          "            lines: List of lines being built",
          "        \"\"\"",
          "        if not spans:",
          "            return",
          "",
          "        # Group consecutive similar spans",
          "        grouped_spans: list[list[Span]] = []",
          "        current_group: list[Span] | None = None",
          "",
          "        for span in spans:",
          "            if current_group and self._are_spans_similar(current_group[0], span):",
          "                current_group.append(span)",
          "            else:",
          "                if current_group:",
          "                    grouped_spans.append(current_group)",
          "                current_group = [span]",
          "",
          "        if current_group:",
          "            grouped_spans.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_spans):",
          "            is_last = i == len(grouped_spans) - 1",
          "            span = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted span line",
          "            span_line = self._format_span_line(span) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {span_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "                data_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {span_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "                data_prefix = f\"{prefix}│  \"",
          "",
          "            # Add the span data as JSON if available",
          "            if span.data:",
          "                data_str = json.dumps(span.data, indent=2)",
          "                data_lines = data_str.split(\"\\n\")",
          "                for data_line in data_lines:",
          "                    lines.append(f\"{data_prefix} {data_line}\")",
          "",
          "            # Process children",
          "            if span.children:",
          "                self._format_spans(span.children, child_prefix, lines)",
          "",
          "    def _format_span_line(self, span: Span) -> str:",
          "        \"\"\"Format a single span line.\"\"\"",
          "        parts = []",
          "",
          "        title = span.title or \"Unnamed Span\"",
          "        parts.append(title)",
          "        if span.duration:",
          "            parts.append(f\"({span.duration})\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_spans_similar(self, span1: Span, span2: Span) -> bool:",
          "        \"\"\"Check if two spans are similar enough to be grouped together.\"\"\"",
          "        if span1.title != span2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(span1.children) != len(span2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children are similar",
          "        for i in range(len(span1.children)):",
          "            if not self._are_spans_similar(span1.children[i], span2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "",
          "class TraceTree(BaseModel):",
          "    trace_id: str | None = None",
          "    org_id: int | None = None",
          "    events: list[TraceEvent] = Field(default_factory=list)  # only expecting transactions and errors",
          "",
          "    def format_trace_tree(self):",
          "        if not self.events:",
          "            return \"Trace (empty)\"",
          "",
          "        lines = [\"Trace\"]",
          "        self._format_events(self.events, \"\", lines, is_last_child=True)",
          "        return \"\\n\".join(lines)",
          "",
          "    def _format_events(",
          "        self, events: list[TraceEvent], prefix: str, lines: list[str], is_last_child: bool",
          "    ):",
          "        if not events:",
          "            return",
          "",
          "        # Group consecutive similar events",
          "        grouped_events: list[list[TraceEvent]] = []",
          "        current_group: list[TraceEvent] | None = None",
          "",
          "        for event in events:",
          "            if current_group and self._are_events_similar(current_group[0], event):",
          "                current_group.append(event)",
          "            else:",
          "                if current_group:",
          "                    grouped_events.append(current_group)",
          "                current_group = [event]",
          "",
          "        if current_group:",
          "            grouped_events.append(current_group)",
          "",
          "        # Format each group",
          "        for i, group in enumerate(grouped_events):",
          "            is_last = i == len(grouped_events) - 1",
          "            event = group[0]",
          "            count_suffix = f\" (repeated {len(group)} times)\" if len(group) > 1 else \"\"",
          "",
          "            # Create the formatted event line",
          "            event_line = self._format_event_line(event) + count_suffix",
          "",
          "            # Add the appropriate prefix based on position in tree",
          "            if is_last:",
          "                lines.append(f\"{prefix}└─ {event_line}\")",
          "                child_prefix = f\"{prefix}   \"",
          "            else:",
          "                lines.append(f\"{prefix}├─ {event_line}\")",
          "                child_prefix = f\"{prefix}│  \"",
          "",
          "            # Process children",
          "            if event.children:",
          "                self._format_events(event.children, child_prefix, lines, is_last)",
          "",
          "    def _format_event_line(self, event: TraceEvent) -> str:",
          "        parts = []",
          "",
          "        # Add ERROR prefix if not a transaction",
          "        prefix = \"ERROR: \" if event.is_error else \"\"",
          "",
          "        # Add title",
          "        title = event.title or \"Unnamed Event\"",
          "        parts.append(f\"{prefix}{title}\")",
          "",
          "        # Add duration if it exists",
          "        if event.duration:",
          "            parts.append(f\"({event.duration})\")",
          "",
          "        # Add event_id (first 7 digits)",
          "        if event.event_id:",
          "            parts.append(f\"(event ID: {event.event_id[:7]})\")",
          "",
          "        # Add project",
          "        if event.project_slug and event.project_id:",
          "            project_str = f\"(project: {event.project_slug})\"",
          "            parts.append(project_str)",
          "",
          "        # Add platform",
          "        if event.platform:",
          "            parts.append(f\"({event.platform})\")",
          "",
          "        # Add profile",
          "        if event.profile_id:",
          "            parts.append(\"(profile available)\")",
          "",
          "        return \" \".join(parts)",
          "",
          "    def _are_events_similar(self, event1: TraceEvent, event2: TraceEvent) -> bool:",
          "        \"\"\"Check if two events are similar enough to be grouped together\"\"\"",
          "        if event1.title != event2.title:",
          "            return False",
          "",
          "        # Check if children structures are the same",
          "        if len(event1.children) != len(event2.children):",
          "            return False",
          "",
          "        # If they have children, we consider them similar only if all children match",
          "        # This is a simplified check - for a full check we'd need to recursively compare children",
          "        for i in range(len(event1.children)):",
          "            if not self._are_events_similar(event1.children[i], event2.children[i]):",
          "                return False",
          "",
          "        return True",
          "",
          "    def get_full_event_id(self, truncated_id: str) -> str | None:",
          "        \"\"\"Return the full event_id given the first 7 characters\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event.event_id",
          "        return None",
          "",
          "    def get_event_by_id(self, truncated_id: str) -> TraceEvent | None:",
          "        \"\"\"Return the full TraceEvent object given a truncated event ID\"\"\"",
          "        for event in self._get_all_events():",
          "            if event.event_id and event.event_id.startswith(truncated_id):",
          "                return event",
          "        return None",
          "",
          "    def _get_all_events(self) -> list[TraceEvent]:",
          "        \"\"\"Return a flattened list of all events in the tree\"\"\"",
          "        all_events = []",
          "",
          "        def collect_events(events):",
          "            for event in events:",
          "                all_events.append(event)",
          "                if event.children:",
          "                    collect_events(event.children)",
          "",
          "        collect_events(self.events)",
          "        return all_events",
          "",
          "    def get_all_project_ids(self) -> list[int]:",
          "        \"\"\"Return a list of all project IDs in the trace tree\"\"\"",
          "        return list(set(event.project_id for event in self._get_all_events() if event.project_id))",
          "",
          "",
          "class RepoDefinition(BaseModel):",
          "    provider: Annotated[str, Examples((\"github\", \"integrations:github\"))]",
          "    owner: str",
          "    name: str",
          "    external_id: Annotated[str, Examples(specialized.ascii_words)]",
          "    branch_name: str | None = Field(",
          "        default=None,",
          "        description=\"The branch that Autofix will work on, otherwise the default branch will be used.\",",
          "    )",
          "    instructions: str | None = Field(",
          "        default=None,",
          "        description=\"Custom instructions when working in this repo.\",",
          "    )",
          "    base_commit_sha: str | None = None",
          "    provider_raw: str | None = None",
          "",
          "    @property",
          "    def full_name(self):",
          "        return f\"{self.owner}/{self.name}\"",
          "",
          "    @model_validator(mode=\"before\")",
          "    @classmethod",
          "    def store_provider_raw(cls, data):",
          "        if isinstance(data, dict) and \"provider\" in data and \"provider_raw\" not in data:",
          "            data[\"provider_raw\"] = data[\"provider\"]",
          "        return data",
          "",
          "    @field_validator(\"provider\", mode=\"after\")",
          "    @classmethod",
          "    def validate_provider(cls, provider: str):",
          "        return process_repo_provider(provider)",
          "",
          "    def __hash__(self):",
          "        return hash((self.provider, self.owner, self.name, self.external_id))",
          "",
          "",
          "class InitializationError(Exception):",
          "    pass",
          "",
          "",
          "class PromptXmlModel(BaseXmlModel):",
          "    def _pad_with_newlines(self, tree: ET.Element) -> None:",
          "        for elem in tree.iter():",
          "            if elem.text:",
          "                stripped = elem.text.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.text = \"\\n\" + stripped + \"\\n\"",
          "            if elem.tail:",
          "                stripped = elem.tail.strip(\"\\n\")",
          "                if stripped:",
          "                    elem.tail = \"\\n\" + stripped + \"\\n\"",
          "",
          "    def to_prompt_str(self) -> str:",
          "        tree: ET.Element = self.to_xml_tree()",
          "",
          "        ET.indent(tree, space=\"\", level=0)",
          "",
          "        self._pad_with_newlines(tree)",
          "",
          "        return unescape_xml_chars(ET.tostring(tree, encoding=\"unicode\"))",
          "",
          "",
          "class Line(BaseModel):",
          "    source_line_no: Optional[int] = None",
          "    target_line_no: Optional[int] = None",
          "    diff_line_no: Optional[int] = None",
          "    value: str",
          "    line_type: Literal[\" \", \"+\", \"-\"]",
          "",
          "",
          "def raw_lines_to_lines(",
          "    lines: list[str],",
          "    source_start: int,",
          "    target_start: int,",
          "    target_line_to_extra: dict[int, str] | None = None,",
          ") -> list[Line]:",
          "    lines_after_header = lines[1:]",
          "    result = []",
          "    current_source_line = source_start",
          "    current_target_line = target_start",
          "    target_line_to_extra = target_line_to_extra or {}",
          "",
          "    for line in lines_after_header:",
          "        line_type = line[0]",
          "",
          "        if current_target_line in target_line_to_extra:",
          "            extra = target_line_to_extra[current_target_line]",
          "            line = line + extra",
          "",
          "        if line_type == \" \":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "            current_target_line += 1",
          "        elif line_type == \"+\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=None,",
          "                    target_line_no=current_target_line,",
          "                )",
          "            )",
          "            current_target_line += 1",
          "        elif line_type == \"-\":",
          "            result.append(",
          "                Line(",
          "                    value=line,",
          "                    line_type=line_type,",
          "                    source_line_no=current_source_line,",
          "                    target_line_no=None,",
          "                )",
          "            )",
          "            current_source_line += 1",
          "        elif line_type == \"\\\\\":",
          "            # Skip the \"\\ No newline at end of file\" marker",
          "            continue",
          "        else:",
          "            raise ValueError(f\"Invalid line type: {line_type}\")",
          "",
          "    return result",
          "",
          "",
          "def right_justified(min_num: int, max_num: int) -> list[str]:",
          "    max_digits = len(str(max_num))",
          "    return [f\"{number:>{max_digits}}\" for number in range(min_num, max_num + 1)]",
          "",
          "",
          "class Hunk(BaseModel):",
          "    source_start: int",
          "    source_length: int",
          "    target_start: int",
          "    target_length: int",
          "    section_header: str",
          "    lines: list[Line]",
          "",
          "    def raw(self) -> str:",
          "        \"\"\"",
          "        The raw hunk, like you see in `git diff` or the original patch string.",
          "        \"\"\"",
          "        return \"\\n\".join((self.section_header, *(line.value for line in self.lines)))",
          "",
          "    def annotated(",
          "        self,",
          "        max_digits_source: int | None = None,",
          "        max_digits_target: int | None = None,",
          "        source_target_delim: str = \"    \",",
          "    ) -> str:",
          "        \"\"\"",
          "        Hunk string with line numbers for the source and target, like you see in GitHub.",
          "        \"\"\"",
          "        if max_digits_source is None:",
          "            max_digits_source = len(str(self.lines[-1].source_line_no))",
          "        if max_digits_target is None:",
          "            max_digits_target = len(str(self.lines[-1].target_line_no))",
          "        header_start = \" \" * (max_digits_source + len(source_target_delim) + max_digits_target + 2)",
          "        # +2 for the whitespace and then the line type character",
          "        return \"\\n\".join(",
          "            (",
          "                f\"{header_start}{self.section_header}\",",
          "                *(",
          "                    f\"{line.source_line_no or '':>{max_digits_source}}{source_target_delim}{line.target_line_no or '':>{max_digits_target}} {line.value}\"",
          "                    for line in self.lines",
          "                ),",
          "            )",
          "        )",
          "",
          "",
          "def annotate_hunks(hunks: list[Hunk]) -> list[str]:",
          "    \"\"\"",
          "    Hunks annotated with line numbers for the source and target, like you see in GitHub.",
          "    Join via `\"\\\\n\\\\n\"` to get the full annotated patch.",
          "    \"\"\"",
          "    max_digits_source = max(len(str(hunk.lines[-1].source_line_no)) for hunk in hunks)",
          "    max_digits_target = max(len(str(hunk.lines[-1].target_line_no)) for hunk in hunks)",
          "    return [",
          "        hunk.annotated(max_digits_source=max_digits_source, max_digits_target=max_digits_target)",
          "        for hunk in hunks",
          "    ]",
          "",
          "",
          "class FilePatch(BaseModel):",
          "    type: Literal[\"A\", \"M\", \"D\"]",
          "    path: str",
          "    added: int",
          "    removed: int",
          "    source_file: str",
          "    target_file: str",
          "    hunks: list[Hunk]",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.type == \"A\":",
          "            if file_contents is not None and file_contents.strip():",
          "                raise FileChangeError(\"Cannot add a file that already exists.\")",
          "            return self._apply_hunks([])",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for modify or delete operations.\")",
          "",
          "        if self.type == \"D\":",
          "            return None",
          "",
          "        # For M type",
          "        try:",
          "            new_contents = self._apply_hunks(file_contents.splitlines(keepends=True))",
          "        except Exception as e:",
          "            raise FileChangeError(f\"Error applying hunks: {e}\")",
          "",
          "        # Preserve any trailing characters from original",
          "        if file_contents:",
          "            trailing = file_contents[len(file_contents.rstrip()) :]",
          "            return new_contents + trailing",
          "",
          "        return new_contents",
          "",
          "    def _apply_hunks(self, lines: list[str]) -> str:",
          "        result = []",
          "        current_line = 0",
          "",
          "        for hunk in self.hunks:",
          "            # Add unchanged lines before the hunk",
          "            result.extend(lines[current_line : hunk.source_start - 1])",
          "            current_line = hunk.source_start - 1",
          "",
          "            for line in hunk.lines:",
          "                if line.line_type == \"+\":",
          "                    result.append(line.value + (\"\\n\" if not line.value.endswith(\"\\n\") else \"\"))",
          "                elif line.line_type == \" \":",
          "                    result.append(lines[current_line])",
          "                    current_line += 1",
          "                elif line.line_type == \"-\":",
          "                    current_line += 1",
          "",
          "        # Add any remaining unchanged lines after the last hunk",
          "        result.extend(lines[current_line:])",
          "",
          "        return \"\".join(result).rstrip(\"\\n\")",
          "",
          "    @staticmethod",
          "    def to_hunks(patch: str, target_line_to_extra: dict[int, str] | None = None) -> list[Hunk]:",
          "        hunk_header_pattern = r\"@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@\"",
          "",
          "        hunks: list[Hunk] = []",
          "        lines = patch.splitlines()",
          "        current_hunk = Hunk(",
          "            source_start=0,",
          "            source_length=0,",
          "            target_start=0,",
          "            target_length=0,",
          "            section_header=\"\",",
          "            lines=[],",
          "        )",
          "        current_lines: list[str] = []",
          "",
          "        for line in lines:",
          "            match = re.match(hunk_header_pattern, line)",
          "            if match:",
          "                if current_lines:",
          "                    current_hunk.lines = raw_lines_to_lines(",
          "                        current_lines,",
          "                        current_hunk.source_start,",
          "                        current_hunk.target_start,",
          "                        target_line_to_extra=target_line_to_extra,",
          "                    )",
          "                    hunks.append(current_hunk)",
          "                    current_lines = []",
          "                source_start, source_length, target_start, target_length = map(int, match.groups())",
          "                current_hunk = Hunk(",
          "                    source_start=source_start,",
          "                    source_length=source_length,",
          "                    target_start=target_start,",
          "                    target_length=target_length,",
          "                    section_header=line,",
          "                    lines=[],",
          "                )",
          "                current_lines = [line]  # starts with section header",
          "            elif current_lines:",
          "                current_lines.append(line)",
          "",
          "        if current_lines:",
          "            current_hunk.lines = raw_lines_to_lines(",
          "                current_lines,",
          "                current_hunk.source_start,",
          "                current_hunk.target_start,",
          "                target_line_to_extra=target_line_to_extra,",
          "            )",
          "            hunks.append(current_hunk)",
          "",
          "        return hunks",
          "",
          "",
          "class FileChangeError(Exception):",
          "    pass",
          "",
          "",
          "class FileChange(BaseModel):",
          "    change_type: Literal[\"create\", \"edit\", \"delete\"]",
          "    path: str",
          "    reference_snippet: Optional[str] = None",
          "    new_snippet: Optional[str] = None",
          "    description: Optional[str] = None",
          "    commit_message: Optional[str] = None",
          "    tool_call_id: Optional[str] = None",
          "",
          "    def apply(self, file_contents: str | None) -> str | None:",
          "        if self.change_type == \"create\":",
          "            if file_contents is not None and file_contents != \"\":",
          "                raise FileChangeError(\"Cannot create a file that already exists.\")",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for creating a file.\")",
          "            return self.new_snippet",
          "",
          "        if file_contents is None:",
          "            raise FileChangeError(\"File contents must be provided for non-create operations.\")",
          "",
          "        if self.change_type == \"edit\":",
          "            if self.new_snippet is None:",
          "                raise FileChangeError(\"New snippet must be provided for editing a file.\")",
          "            if self.reference_snippet is None:",
          "                raise FileChangeError(\"Reference snippet must be provided for editing a file.\")",
          "            return file_contents.replace(self.reference_snippet, self.new_snippet)",
          "",
          "        # Delete",
          "        if self.reference_snippet is None:",
          "            return None",
          "",
          "        return file_contents.replace(self.reference_snippet, \"\")",
          "",
          "",
          "class SeerProjectPreference(BaseModel):",
          "    organization_id: int",
          "    project_id: int",
          "    repositories: list[RepoDefinition]",
          "",
          "    def to_db_model(self) -> DbSeerProjectPreference:",
          "        return DbSeerProjectPreference(",
          "            organization_id=self.organization_id,",
          "            project_id=self.project_id,",
          "            repositories=[repo.model_dump() for repo in self.repositories],",
          "        )",
          "",
          "    @classmethod",
          "    def from_db_model(cls, db_model: DbSeerProjectPreference) -> \"SeerProjectPreference\":",
          "        return cls(",
          "            organization_id=db_model.organization_id,",
          "            project_id=db_model.project_id,",
          "            repositories=db_model.repositories,",
          "        )",
          "",
          "",
          "class EAPTrace(BaseModel):",
          "    trace_id: str = Field(..., description=\"ID of the trace\")",
          "    trace: list[dict] = Field(..., description=\"List of spans in the trace\")",
          "",
          "    def _get_transaction_spans(self, trace: list[dict] | dict) -> list[dict]:",
          "        \"\"\"",
          "        Filters the trace to only include the transaction spans.",
          "        \"\"\"",
          "        if not trace:",
          "            return []",
          "",
          "        if isinstance(trace, list):",
          "            transaction_spans = []",
          "            for span in trace:",
          "                transaction_spans.extend(self._get_transaction_spans(span))",
          "            return transaction_spans",
          "",
          "        transaction_spans = []",
          "",
          "        if trace.get(\"is_transaction\"):",
          "            transaction_span = trace.copy()",
          "            transaction_span[\"children\"] = self._get_transaction_spans(trace.get(\"children\", []))",
          "            transaction_spans.append(transaction_span)",
          "        else:",
          "            transaction_spans.extend(self._get_transaction_spans(trace.get(\"children\", [])))",
          "",
          "        return transaction_spans",
          "",
          "    def get_and_format_trace(self, only_transactions=False) -> str:",
          "        \"\"\"",
          "        Formats the trace as a string of tags.",
          "        \"\"\"",
          "        trace = self.trace",
          "        if only_transactions:",
          "            trace = self._get_transaction_spans(self.trace)",
          "",
          "        def format_span_as_tag(span, depth=0):",
          "            indent = \"    \" * depth",
          "",
          "            attrs = []",
          "            for key, value in span.items():",
          "                # Ignore event_type since all events are marked as \"span\"",
          "                if key not in [\"children\", \"event_type\"]:",
          "                    attrs.append(f'{key}=\"{value}\"')",
          "            attrs_str = \" \".join(attrs)",
          "",
          "            tag_name = \"txn\" if span.get(\"is_transaction\") else \"span\"",
          "",
          "            if not span.get(\"children\"):",
          "                return f\"{indent}<{tag_name} {attrs_str} />\"",
          "            else:",
          "                tag_start = f\"{indent}<{tag_name} {attrs_str}>\"",
          "",
          "                # Format children recursively with increased depth",
          "                children = []",
          "                for child in span[\"children\"]:",
          "                    children.append(format_span_as_tag(child, depth + 1))",
          "",
          "                child_content = \"\\n\" + \"\\n\".join(children)",
          "                tag_end = f\"\\n{indent}</{tag_name}>\"",
          "",
          "                return f\"{tag_start}{child_content}{tag_end}\"",
          "",
          "        formatted = []",
          "        for span in trace:",
          "            formatted.append(format_span_as_tag(span, 0))",
          "",
          "        return \"\\n\".join(formatted)",
          ""
        ]
      }
    },
    {
      "file": "langfuse/decorators/langfuse_decorator.py",
      "image": "langfuse.decorators.langfuse_decorator",
      "is_application": false,
      "line": 313,
      "name": "LangfuseDecorator._prepare_call",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/decorators/langfuse_decorator.py"
    },
    {
      "file": "langfuse/decorators/langfuse_decorator.py",
      "image": "langfuse.decorators.langfuse_decorator",
      "is_application": false,
      "line": 407,
      "name": "LangfuseDecorator._get_input_from_func_args",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/decorators/langfuse_decorator.py"
    },
    {
      "file": "__init__.py",
      "image": "json",
      "is_application": false,
      "line": 238,
      "name": "dumps",
      "path": "/usr/lib/python3.11/json/__init__.py"
    },
    {
      "file": "langfuse/serializer.py",
      "image": "langfuse.serializer",
      "is_application": false,
      "line": 166,
      "name": "EventSerializer.encode",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/serializer.py"
    },
    {
      "file": "json/encoder.py",
      "image": "json.encoder",
      "is_application": false,
      "line": 200,
      "name": "JSONEncoder.encode",
      "path": "/usr/lib/python3.11/json/encoder.py"
    },
    {
      "file": "json/encoder.py",
      "image": "json.encoder",
      "is_application": false,
      "line": 258,
      "name": "JSONEncoder.iterencode",
      "path": "/usr/lib/python3.11/json/encoder.py"
    },
    {
      "file": "seer/automation/agent/client.py",
      "image": "seer.automation.agent.client",
      "is_application": true,
      "line": 1354,
      "name": "LlmClient.generate_structured",
      "path": "/app/src/seer/automation/agent/client.py",
      "codeContext": {
        "file": "seer/automation/agent/client.py",
        "line": 1354,
        "name": "LlmClient.generate_structured",
        "code": "                    prompt=prompt,\n                    response_format=response_format,\n                    system_prompt=system_prompt,\n                    temperature=temperature,\n                    tools=cast(list[FunctionTool], tools),\n                    timeout=timeout,\n                    reasoning_effort=reasoning_effort,\n                )\n            elif model.provider_name == LlmProviderType.ANTHROPIC:\n                raise NotImplementedError(\"Anthropic structured outputs are not yet supported\")\n            elif model.provider_name == LlmProviderType.GEMINI:\n                model = cast(GeminiProvider, model)\n\n                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):\n                    raise ValueError(\"Claude tools are not supported for Gemini\")\n                return model.generate_structured(\n                    max_tokens=max_tokens,\n                    messages=messages,\n                    prompt=prompt,\n                    response_format=response_format,",
        "lineRange": {
          "start": 1345,
          "end": 1364
        },
        "lines": [
          "import json",
          "import logging",
          "import re",
          "import time",
          "from dataclasses import dataclass",
          "from typing import Any, ClassVar, Iterable, Iterator, Tuple, Type, Union, cast",
          "",
          "import anthropic",
          "from anthropic import NOT_GIVEN",
          "from anthropic.types import (",
          "    MessageParam,",
          "    TextBlockParam,",
          "    ThinkingBlockParam,",
          "    ThinkingConfigEnabledParam,",
          "    ToolParam,",
          "    ToolResultBlockParam,",
          "    ToolUseBlockParam,",
          ")",
          "from google import genai  # type: ignore[attr-defined]",
          "from google.genai.errors import ClientError, ServerError",
          "from google.genai.types import (",
          "    Content,",
          "    FunctionDeclaration,",
          "    GenerateContentConfig,",
          "    GenerateContentResponse,",
          "    GoogleSearch,",
          "    Part,",
          ")",
          "from google.genai.types import Tool as GeminiTool",
          "from langfuse.decorators import langfuse_context, observe",
          "from langfuse.openai import openai",
          "from openai.types.chat import ChatCompletionMessageParam, ChatCompletionToolParam",
          "",
          "from seer.automation.agent.models import (",
          "    LlmGenerateStructuredResponse,",
          "    LlmGenerateTextResponse,",
          "    LlmModelDefaultConfig,",
          "    LlmNoCompletionTokensError,",
          "    LlmProviderDefaults,",
          "    LlmProviderType,",
          "    LlmRefusalError,",
          "    LlmResponseMetadata,",
          "    LlmStreamFirstTokenTimeoutError,",
          "    LlmStreamInactivityTimeoutError,",
          "    LlmStreamTimeoutError,",
          "    Message,",
          "    StructuredOutputType,",
          "    ToolCall,",
          "    Usage,",
          ")",
          "from seer.automation.agent.tools import ClaudeTool, FunctionTool",
          "from seer.bootup import module",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "from seer.utils import backoff_on_exception",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "@dataclass",
          "class OpenAiProvider:",
          "    model_name: str",
          "    provider_name = LlmProviderType.OPENAI",
          "    defaults: LlmProviderDefaults | None = None",
          "",
          "    default_configs: ClassVar[list[LlmModelDefaultConfig]] = [",
          "        LlmModelDefaultConfig(",
          "            match=r\"^o1-mini.*\",",
          "            defaults=LlmProviderDefaults(temperature=1.0),",
          "        ),",
          "        LlmModelDefaultConfig(",
          "            match=r\"^o1-preview.*\",",
          "            defaults=LlmProviderDefaults(temperature=1.0),",
          "        ),",
          "        LlmModelDefaultConfig(",
          "            match=r\"^o3-mini.*\",",
          "            defaults=LlmProviderDefaults(temperature=1.0),",
          "        ),",
          "        LlmModelDefaultConfig(",
          "            match=r\".*\",",
          "            defaults=LlmProviderDefaults(temperature=0.0),",
          "        ),",
          "    ]",
          "",
          "    @staticmethod",
          "    def get_client() -> openai.Client:",
          "        return openai.Client(max_retries=4)",
          "",
          "    @classmethod",
          "    def model(cls, model_name: str) -> \"OpenAiProvider\":",
          "        model_config = cls._get_config(model_name)",
          "        return cls(",
          "            model_name=model_name,",
          "            defaults=model_config.defaults if model_config else None,",
          "        )",
          "",
          "    @classmethod",
          "    def _get_config(cls, model_name: str):",
          "        for config in cls.default_configs:",
          "            if re.match(config.match, model_name):",
          "                return config",
          "        return None",
          "",
          "    @staticmethod",
          "    def is_completion_exception_retryable(exception: Exception) -> bool:",
          "        return isinstance(exception, openai.InternalServerError) or isinstance(",
          "            exception, LlmStreamTimeoutError",
          "        )",
          "",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        predicted_output: str | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ):",
          "        message_dicts, tool_dicts = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "            reasoning_effort=reasoning_effort,",
          "        )",
          "",
          "        openai_client = self.get_client()",
          "",
          "        completion = openai_client.chat.completions.create(",
          "            model=self.model_name,",
          "            messages=cast(Iterable[ChatCompletionMessageParam], message_dicts),",
          "            temperature=temperature,",
          "            tools=(",
          "                cast(Iterable[ChatCompletionToolParam], tool_dicts)",
          "                if tool_dicts",
          "                else openai.NotGiven()",
          "            ),",
          "            max_tokens=max_tokens or openai.NotGiven(),",
          "            timeout=timeout or openai.NotGiven(),",
          "            prediction=(",
          "                {",
          "                    \"type\": \"content\",",
          "                    \"content\": predicted_output,",
          "                }",
          "                if predicted_output",
          "                else openai.NotGiven()",
          "            ),",
          "            reasoning_effort=reasoning_effort if reasoning_effort else openai.NotGiven(),",
          "        )",
          "",
          "        openai_message = completion.choices[0].message",
          "        if openai_message.refusal:",
          "            raise LlmRefusalError(completion.choices[0].message.refusal)",
          "",
          "        message = Message(",
          "            content=openai_message.content,",
          "            role=openai_message.role,",
          "            tool_calls=(",
          "                [",
          "                    ToolCall(id=call.id, function=call.function.name, args=call.function.arguments)",
          "                    for call in openai_message.tool_calls",
          "                ]",
          "                if openai_message.tool_calls",
          "                else None",
          "            ),",
          "        )",
          "",
          "        usage = Usage(",
          "            completion_tokens=completion.usage.completion_tokens if completion.usage else 0,",
          "            prompt_tokens=completion.usage.prompt_tokens if completion.usage else 0,",
          "            total_tokens=completion.usage.total_tokens if completion.usage else 0,",
          "        )",
          "",
          "        return LlmGenerateTextResponse(",
          "            message=message,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    def generate_structured(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        response_format: Type[StructuredOutputType],",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> LlmGenerateStructuredResponse[StructuredOutputType]:",
          "        message_dicts, tool_dicts = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "            reasoning_effort=reasoning_effort,",
          "        )",
          "",
          "        openai_client = self.get_client()",
          "",
          "        completion = openai_client.beta.chat.completions.parse(",
          "            model=self.model_name,",
          "            messages=cast(Iterable[ChatCompletionMessageParam], message_dicts),",
          "            temperature=temperature,",
          "            tools=(",
          "                cast(Iterable[ChatCompletionToolParam], tool_dicts)",
          "                if tool_dicts",
          "                else openai.NotGiven()",
          "            ),",
          "            response_format=response_format,",
          "            max_tokens=max_tokens or openai.NotGiven(),",
          "            timeout=timeout or openai.NotGiven(),",
          "            reasoning_effort=reasoning_effort if reasoning_effort else openai.NotGiven(),",
          "        )",
          "",
          "        openai_message = completion.choices[0].message",
          "        if openai_message.refusal:",
          "            raise LlmRefusalError(completion.choices[0].message.refusal)",
          "",
          "        parsed = cast(StructuredOutputType, completion.choices[0].message.parsed)",
          "",
          "        usage = Usage(",
          "            completion_tokens=completion.usage.completion_tokens if completion.usage else 0,",
          "            prompt_tokens=completion.usage.prompt_tokens if completion.usage else 0,",
          "            total_tokens=completion.usage.total_tokens if completion.usage else 0,",
          "        )",
          "",
          "        return LlmGenerateStructuredResponse(",
          "            parsed=parsed,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @staticmethod",
          "    def to_message_dict(message: Message) -> ChatCompletionMessageParam:",
          "        message_dict: dict[str, Any] = {",
          "            \"content\": message.content if message.content else \"\",",
          "            \"role\": message.role,",
          "        }",
          "",
          "        if message.tool_calls:",
          "            tool_calls = [tool_call.model_dump(mode=\"json\") for tool_call in message.tool_calls]",
          "            parsed_tool_calls = []",
          "            for item in tool_calls:",
          "                new_item = item.copy()",
          "                new_item[\"function\"] = {\"name\": item[\"function\"], \"arguments\": item[\"args\"]}",
          "                new_item[\"type\"] = \"function\"",
          "                parsed_tool_calls.append(new_item)",
          "            message_dict[\"tool_calls\"] = parsed_tool_calls",
          "            message_dict[\"role\"] = \"assistant\"",
          "",
          "        if message.tool_call_id:",
          "            message_dict[\"tool_call_id\"] = message.tool_call_id",
          "",
          "        return cast(ChatCompletionMessageParam, message_dict)",
          "",
          "    @staticmethod",
          "    def to_tool_dict(tool: FunctionTool) -> ChatCompletionToolParam:",
          "        return ChatCompletionToolParam(",
          "            type=\"function\",",
          "            function={",
          "                \"name\": tool.name,",
          "                \"description\": tool.description,",
          "                \"parameters\": {",
          "                    \"type\": \"object\",",
          "                    \"properties\": {",
          "                        param[\"name\"]: {",
          "                            key: value",
          "                            for key, value in {",
          "                                \"type\": param[\"type\"],",
          "                                \"description\": param.get(\"description\", \"\"),",
          "                                \"items\": param.get(\"items\"),",
          "                            }.items()",
          "                            if value is not None",
          "                        }",
          "                        for param in tool.parameters",
          "                    },",
          "                    \"required\": tool.required,",
          "                },",
          "            },",
          "        )",
          "",
          "    @classmethod",
          "    def _prep_message_and_tools(",
          "        cls,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ):",
          "        message_dicts = [cls.to_message_dict(message) for message in messages] if messages else []",
          "        if system_prompt:",
          "            message_dicts.insert(",
          "                0,",
          "                cls.to_message_dict(",
          "                    Message(",
          "                        role=\"system\" if not reasoning_effort else \"developer\",",
          "                        content=system_prompt,",
          "                    )",
          "                ),",
          "            )",
          "        if prompt:",
          "            message_dicts.append(cls.to_message_dict(Message(role=\"user\", content=prompt)))",
          "",
          "        tool_dicts = (",
          "            [cls.to_tool_dict(tool) for tool in tools] if tools and len(tools) > 0 else None",
          "        )",
          "",
          "        return message_dicts, tool_dicts",
          "",
          "    @observe(as_type=\"generation\", name=\"OpenAI Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> Iterator[Tuple[str, str] | ToolCall | Usage]:",
          "        message_dicts, tool_dicts = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "            reasoning_effort=reasoning_effort,",
          "        )",
          "",
          "        openai_client = self.get_client()",
          "",
          "        stream = openai_client.chat.completions.create(",
          "            model=self.model_name,",
          "            messages=cast(Iterable[ChatCompletionMessageParam], message_dicts),",
          "            temperature=temperature,",
          "            tools=(",
          "                cast(Iterable[ChatCompletionToolParam], tool_dicts)",
          "                if tool_dicts",
          "                else openai.NotGiven()",
          "            ),",
          "            max_tokens=max_tokens or openai.NotGiven(),",
          "            timeout=timeout or openai.NotGiven(),",
          "            stream=True,",
          "            stream_options={\"include_usage\": True},",
          "            reasoning_effort=reasoning_effort if reasoning_effort else openai.NotGiven(),",
          "        )",
          "",
          "        try:",
          "            current_tool_call: dict[str, Any] | None = None",
          "            current_tool_call_index = 0",
          "",
          "            for chunk in stream:",
          "                if not chunk.choices and chunk.usage:",
          "                    usage = Usage(",
          "                        completion_tokens=chunk.usage.completion_tokens,",
          "                        prompt_tokens=chunk.usage.prompt_tokens,",
          "                        total_tokens=chunk.usage.total_tokens,",
          "                    )",
          "                    yield usage",
          "                    langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "                    break",
          "",
          "                delta = chunk.choices[0].delta",
          "                if delta.tool_calls:",
          "                    tool_call = delta.tool_calls[0]",
          "",
          "                    if (",
          "                        not current_tool_call or current_tool_call_index != tool_call.index",
          "                    ):  # Start of new tool call",
          "                        current_tool_call_index = tool_call.index",
          "                        if current_tool_call:",
          "                            yield ToolCall(**current_tool_call)",
          "                        current_tool_call = None",
          "                        current_tool_call = {",
          "                            \"id\": tool_call.id,",
          "                            \"function\": tool_call.function.name if tool_call.function.name else \"\",",
          "                            \"args\": (",
          "                                tool_call.function.arguments if tool_call.function.arguments else \"\"",
          "                            ),",
          "                        }",
          "                    else:",
          "                        if tool_call.function.arguments:",
          "                            current_tool_call[\"args\"] += tool_call.function.arguments",
          "                if chunk.choices[0].finish_reason == \"tool_calls\" and current_tool_call:",
          "                    yield ToolCall(**current_tool_call)",
          "                if delta.content:",
          "                    yield \"content\", delta.content",
          "        finally:",
          "            stream.response.close()",
          "",
          "    def construct_message_from_stream(",
          "        self, content_chunks: list[str], tool_calls: list[ToolCall]",
          "    ) -> Message:",
          "        return Message(",
          "            role=\"assistant\",",
          "            content=\"\".join(content_chunks) if content_chunks else None,",
          "            tool_calls=tool_calls if tool_calls else None,",
          "        )",
          "",
          "",
          "@dataclass",
          "class AnthropicProvider:",
          "    model_name: str",
          "    provider_name = LlmProviderType.ANTHROPIC",
          "    defaults: LlmProviderDefaults | None = None",
          "",
          "    default_configs: ClassVar[list[LlmModelDefaultConfig]] = [",
          "        LlmModelDefaultConfig(",
          "            match=r\".*\",",
          "            defaults=LlmProviderDefaults(temperature=0.0),",
          "        ),",
          "    ]",
          "",
          "    @staticmethod",
          "    @inject",
          "    def get_client(app_config: AppConfig = injected) -> anthropic.AnthropicVertex:",
          "        if app_config.SENTRY_REGION == \"de\":",
          "            region = \"europe-west1\"",
          "        elif app_config.DEV:",
          "            region = \"us-east5\"",
          "        else:",
          "            region = \"global\"",
          "        return anthropic.AnthropicVertex(",
          "            project_id=app_config.GOOGLE_CLOUD_PROJECT,",
          "            region=region,",
          "            max_retries=8,",
          "        )",
          "",
          "    @classmethod",
          "    def model(cls, model_name: str) -> \"AnthropicProvider\":",
          "        model_config = cls._get_config(model_name)",
          "        return cls(",
          "            model_name=model_name,",
          "            defaults=model_config.defaults if model_config else None,",
          "        )",
          "",
          "    @classmethod",
          "    def _get_config(cls, model_name: str):",
          "        for config in cls.default_configs:",
          "            if re.match(config.match, model_name):",
          "                return config",
          "        return None",
          "",
          "    @staticmethod",
          "    def is_completion_exception_retryable(exception: Exception) -> bool:",
          "        retryable_errors = (",
          "            \"overloaded_error\",",
          "            \"Internal server error\",",
          "            \"not_found_error\",",
          "        )",
          "        return (",
          "            (",
          "                isinstance(exception, anthropic.AnthropicError)",
          "                and any(error in str(exception) for error in retryable_errors)",
          "            )",
          "            or isinstance(exception, LlmStreamTimeoutError)",
          "            or \"incomplete chunked read\" in str(exception)",
          "        )",
          "",
          "    @observe(as_type=\"generation\", name=\"Anthropic Generation\")",
          "    @inject",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ):",
          "        message_dicts, tool_dicts, system_prompt_block = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        anthropic_client = self.get_client()",
          "",
          "        completion = anthropic_client.messages.create(",
          "            system=system_prompt_block or NOT_GIVEN,",
          "            model=self.model_name,",
          "            tools=cast(Iterable[ToolParam], tool_dicts) if tool_dicts else NOT_GIVEN,",
          "            messages=cast(Iterable[MessageParam], message_dicts),",
          "            max_tokens=max_tokens or 8192,",
          "            temperature=temperature or NOT_GIVEN,",
          "            timeout=timeout or NOT_GIVEN,",
          "            thinking=(",
          "                ThinkingConfigEnabledParam(",
          "                    type=\"enabled\",",
          "                    budget_tokens=(",
          "                        1024",
          "                        if reasoning_effort == \"low\"",
          "                        else 4092 if reasoning_effort == \"medium\" else 8192",
          "                    ),",
          "                )",
          "                if reasoning_effort",
          "                else NOT_GIVEN",
          "            ),",
          "        )",
          "",
          "        message = self._format_claude_response_to_message(completion)",
          "",
          "        usage = Usage(",
          "            completion_tokens=completion.usage.output_tokens,",
          "            prompt_tokens=completion.usage.input_tokens,",
          "            total_tokens=completion.usage.input_tokens + completion.usage.output_tokens,",
          "        )",
          "",
          "        langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "        return LlmGenerateTextResponse(",
          "            message=message,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @staticmethod",
          "    def _format_claude_response_to_message(completion: anthropic.types.Message) -> Message:",
          "        message = Message(role=completion.role)",
          "        for block in completion.content:",
          "            if block.type == \"text\":",
          "                message.content = (",
          "                    block.text",
          "                )  # we're assuming there's only one text block per message",
          "            elif block.type == \"tool_use\":",
          "                if not message.tool_calls:",
          "                    message.tool_calls = []",
          "                message.tool_calls.append(",
          "                    ToolCall(id=block.id, function=block.name, args=json.dumps(block.input))",
          "                )",
          "                message.role = \"tool_use\"",
          "                message.tool_call_id = message.tool_calls[",
          "                    0",
          "                ].id  # assumes we get only 1 tool call at a time, but we really don't use this field for tool_use blocks",
          "            elif block.type == \"thinking\":",
          "                message.thinking_content = block.thinking",
          "                message.thinking_signature = block.signature",
          "        return message",
          "",
          "    @staticmethod",
          "    def to_message_param(message: Message) -> MessageParam:",
          "        if message.role == \"tool\":",
          "            return MessageParam(",
          "                role=\"user\",",
          "                content=[",
          "                    ToolResultBlockParam(",
          "                        type=\"tool_result\",",
          "                        content=message.content or \"\",",
          "                        tool_use_id=message.tool_call_id or \"\",",
          "                    )",
          "                ],",
          "            )",
          "        elif message.role == \"tool_use\" or (message.role == \"assistant\" and message.tool_calls):",
          "            assistant_msg_content: list[ThinkingBlockParam | ToolUseBlockParam] = []",
          "            if message.thinking_content and message.thinking_signature:",
          "                assistant_msg_content.append(",
          "                    ThinkingBlockParam(",
          "                        type=\"thinking\",",
          "                        thinking=message.thinking_content,",
          "                        signature=message.thinking_signature,",
          "                    )",
          "                )",
          "            if message.tool_calls:",
          "                tool_call = message.tool_calls[0]  # Assuming only one tool call per message",
          "                assistant_msg_content.append(",
          "                    ToolUseBlockParam(",
          "                        type=\"tool_use\",",
          "                        id=tool_call.id or \"\",",
          "                        name=tool_call.function,",
          "                        input=json.loads(tool_call.args),",
          "                    )",
          "                )",
          "            return MessageParam(",
          "                role=\"assistant\",",
          "                content=assistant_msg_content,",
          "            )",
          "        else:",
          "            other_content: list[ThinkingBlockParam | TextBlockParam] = []",
          "            if message.thinking_content and message.thinking_signature:",
          "                other_content.append(",
          "                    ThinkingBlockParam(",
          "                        type=\"thinking\",",
          "                        thinking=message.thinking_content,",
          "                        signature=message.thinking_signature,",
          "                    )",
          "                )",
          "            other_content.append(TextBlockParam(type=\"text\", text=message.content or \"\"))",
          "            return MessageParam(",
          "                role=message.role,  # type: ignore",
          "                content=other_content,",
          "            )",
          "",
          "    @staticmethod",
          "    def to_tool_dict(tool: FunctionTool | ClaudeTool) -> ToolParam:",
          "        if isinstance(tool, ClaudeTool):",
          "            return ToolParam(  # type: ignore",
          "                name=tool.name,",
          "                type=tool.type,",
          "            )",
          "",
          "        return ToolParam(",
          "            name=tool.name,",
          "            description=tool.description,",
          "            input_schema={",
          "                \"type\": \"object\",",
          "                \"properties\": {",
          "                    param[\"name\"]: {",
          "                        key: value",
          "                        for key, value in {",
          "                            \"type\": param[\"type\"],",
          "                            \"description\": param.get(\"description\", \"\"),",
          "                            \"items\": param.get(\"items\"),",
          "                        }.items()",
          "                        if value is not None",
          "                    }",
          "                    for param in tool.parameters",
          "                },",
          "                \"required\": tool.required,",
          "            },",
          "        )",
          "",
          "    @classmethod",
          "    def _prep_message_and_tools(",
          "        cls,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "    ) -> tuple[list[MessageParam], list[ToolParam] | None, list[TextBlockParam] | None]:",
          "        message_dicts = [cls.to_message_param(message) for message in messages] if messages else []",
          "        if prompt:",
          "            message_dicts.append(cls.to_message_param(Message(role=\"user\", content=prompt)))",
          "        if message_dicts:",
          "            message_dicts[-1][\"content\"][0][\"cache_control\"] = {\"type\": \"ephemeral\"}  # type: ignore[index]",
          "",
          "        tool_dicts = (",
          "            [cls.to_tool_dict(tool) for tool in tools] if tools and len(tools) > 0 else None",
          "        )",
          "",
          "        system_prompt_block = (",
          "            [TextBlockParam(type=\"text\", text=system_prompt, cache_control={\"type\": \"ephemeral\"})]",
          "            if system_prompt",
          "            else None",
          "        )",
          "",
          "        return message_dicts, tool_dicts, system_prompt_block",
          "",
          "    @observe(as_type=\"generation\", name=\"Anthropic Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> Iterator[Tuple[str, str] | ToolCall | Usage]:",
          "        message_dicts, tool_dicts, system_prompt_block = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        anthropic_client = self.get_client()",
          "",
          "        stream = anthropic_client.messages.create(",
          "            system=system_prompt_block or NOT_GIVEN,",
          "            model=self.model_name,",
          "            tools=cast(Iterable[ToolParam], tool_dicts) if tool_dicts else NOT_GIVEN,",
          "            messages=cast(Iterable[MessageParam], message_dicts),",
          "            max_tokens=max_tokens or 8192,",
          "            temperature=temperature or NOT_GIVEN,",
          "            timeout=timeout or NOT_GIVEN,",
          "            stream=True,",
          "            thinking=(",
          "                ThinkingConfigEnabledParam(",
          "                    type=\"enabled\",",
          "                    budget_tokens=(",
          "                        1024",
          "                        if reasoning_effort == \"low\"",
          "                        else 4092 if reasoning_effort == \"medium\" else 8192",
          "                    ),",
          "                )",
          "                if reasoning_effort",
          "                else NOT_GIVEN",
          "            ),",
          "        )",
          "",
          "        try:",
          "            current_tool_call: dict[str, Any] | None = None",
          "            current_input_json = []",
          "            total_input_write_tokens = 0",
          "            total_input_read_tokens = 0",
          "            total_input_tokens = 0",
          "            total_output_tokens = 0",
          "",
          "            for chunk in stream:",
          "                if chunk.type == \"message_start\" and chunk.message.usage:",
          "                    if chunk.message.usage.cache_creation_input_tokens:",
          "                        total_input_write_tokens += chunk.message.usage.cache_creation_input_tokens",
          "                    if chunk.message.usage.cache_read_input_tokens:",
          "                        total_input_read_tokens += chunk.message.usage.cache_read_input_tokens",
          "                    total_input_tokens += chunk.message.usage.input_tokens",
          "                    total_output_tokens += chunk.message.usage.output_tokens",
          "                elif chunk.type == \"message_delta\" and chunk.usage:",
          "                    total_output_tokens += chunk.usage.output_tokens",
          "",
          "                if chunk.type == \"message_stop\":",
          "                    break",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"text_delta\":",
          "                    yield \"content\", chunk.delta.text",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"thinking_delta\":",
          "                    yield \"thinking_content\", chunk.delta.thinking",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"signature_delta\":",
          "                    yield \"thinking_signature\", chunk.delta.signature",
          "                elif chunk.type == \"content_block_start\" and chunk.content_block.type == \"tool_use\":",
          "                    # Start accumulating a new tool call",
          "                    current_tool_call = {",
          "                        \"id\": chunk.content_block.id,",
          "                        \"function\": chunk.content_block.name,",
          "                        \"args\": \"\",",
          "                    }",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"input_json_delta\":",
          "                    # Accumulate the input JSON",
          "                    if current_tool_call:",
          "                        current_input_json.append(chunk.delta.partial_json)",
          "                elif chunk.type == \"content_block_stop\" and current_tool_call:",
          "                    # Tool call is complete, yield it",
          "                    current_tool_call[\"args\"] = \"\".join(current_input_json)",
          "                    yield ToolCall(**current_tool_call)",
          "                    current_tool_call = None",
          "                    current_input_json = []",
          "        finally:",
          "            usage = Usage(",
          "                completion_tokens=total_output_tokens,",
          "                prompt_tokens=total_input_tokens,",
          "                total_tokens=total_input_tokens + total_output_tokens,",
          "                prompt_cache_write_tokens=total_input_write_tokens,",
          "                prompt_cache_read_tokens=total_input_read_tokens,",
          "            )",
          "            yield usage",
          "            langfuse_context.update_current_observation(",
          "                model=self.model_name, usage=usage.to_langfuse_usage()",
          "            )",
          "            stream.response.close()",
          "",
          "    def construct_message_from_stream(",
          "        self,",
          "        content_chunks: list[str],",
          "        tool_calls: list[ToolCall],",
          "        thinking_content_chunks: list[str],",
          "        thinking_signature: str | None,",
          "    ) -> Message:",
          "        message = Message(",
          "            role=\"tool_use\" if tool_calls else \"assistant\",",
          "            content=\"\".join(content_chunks) if content_chunks else None,",
          "            thinking_content=\"\".join(thinking_content_chunks) if thinking_content_chunks else None,",
          "            thinking_signature=thinking_signature,",
          "        )",
          "",
          "        if tool_calls:",
          "            message.tool_calls = tool_calls",
          "            message.tool_call_id = tool_calls[0].id",
          "",
          "        return message",
          "",
          "",
          "@dataclass",
          "class GeminiProvider:",
          "    model_name: str",
          "    provider_name = LlmProviderType.GEMINI",
          "    defaults: LlmProviderDefaults | None = None",
          "",
          "    default_configs: ClassVar[list[LlmModelDefaultConfig]] = [",
          "        LlmModelDefaultConfig(",
          "            match=r\".*\",",
          "            defaults=LlmProviderDefaults(temperature=0.0),",
          "        ),",
          "    ]",
          "",
          "    @staticmethod",
          "    def get_client() -> genai.Client:",
          "        client = genai.Client(",
          "            vertexai=True,",
          "            location=\"us-central1\",",
          "        )",
          "        # The gemini client currently doesn't have a built-in retry mechanism.",
          "        retrier = backoff_on_exception(",
          "            GeminiProvider.is_completion_exception_retryable, max_tries=4",
          "        )",
          "        client.models.generate_content = retrier(client.models.generate_content)",
          "        return client",
          "",
          "    @classmethod",
          "    def model(cls, model_name: str) -> \"GeminiProvider\":",
          "        model_config = cls._get_config(model_name)",
          "        return cls(",
          "            model_name=model_name,",
          "            defaults=model_config.defaults if model_config else None,",
          "        )",
          "",
          "    @classmethod",
          "    def _get_config(cls, model_name: str):",
          "        for config in cls.default_configs:",
          "            if re.match(config.match, model_name):",
          "                return config",
          "        return None",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Generation with Grounding\")",
          "    def search_the_web(self, prompt: str, temperature: float | None = None) -> str:",
          "        client = self.get_client()",
          "        google_search_tool = GeminiTool(google_search=GoogleSearch())",
          "",
          "        response = client.models.generate_content(",
          "            model=self.model_name,",
          "            contents=prompt,",
          "            config=GenerateContentConfig(",
          "                tools=[google_search_tool],",
          "                response_modalities=[\"TEXT\"],",
          "                temperature=temperature or 0.0,",
          "            ),",
          "        )",
          "        answer = \"\"",
          "        if (",
          "            response.candidates",
          "            and response.candidates[0].content",
          "            and response.candidates[0].content.parts",
          "        ):",
          "            for each in response.candidates[0].content.parts:",
          "                if each.text:",
          "                    answer += each.text",
          "        return answer",
          "",
          "    @staticmethod",
          "    def is_completion_exception_retryable(exception: Exception) -> bool:",
          "        retryable_errors = (",
          "            \"Resource exhausted. Please try again later.\",",
          "            \"429 RESOURCE_EXHAUSTED\",",
          "            # https://sentry.sentry.io/issues/6301072208",
          "            \"TLS/SSL connection has been closed\",",
          "            \"Max retries exceeded with url\",",
          "            \"Internal error\",",
          "        )",
          "        return (",
          "            isinstance(exception, ServerError)",
          "            or (",
          "                isinstance(exception, ClientError)",
          "                and any(error in str(exception) for error in retryable_errors)",
          "            )",
          "            or isinstance(exception, LlmNoCompletionTokensError)",
          "        ) or isinstance(exception, LlmStreamTimeoutError)",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Generation\")",
          "    def generate_structured(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        response_format: Type[StructuredOutputType],",
          "        max_tokens: int | None = None,",
          "    ) -> LlmGenerateStructuredResponse[StructuredOutputType]:",
          "        message_dicts, tool_dicts, system_prompt = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        client = self.get_client()",
          "",
          "        max_retries = 2  # Gemini sometimes doesn't fill in response.parsed",
          "        for _ in range(max_retries + 1):",
          "            response = client.models.generate_content(",
          "                model=self.model_name,",
          "                contents=message_dicts,",
          "                config=GenerateContentConfig(",
          "                    tools=tool_dicts,",
          "                    response_modalities=[\"TEXT\"],",
          "                    temperature=temperature or 0.0,",
          "                    response_mime_type=\"application/json\",",
          "                    max_output_tokens=max_tokens or 8192,",
          "                    response_schema=response_format,",
          "                ),",
          "            )",
          "            if response.parsed is not None:",
          "                break",
          "",
          "        usage = Usage(",
          "            completion_tokens=response.usage_metadata.candidates_token_count or 0,",
          "            prompt_tokens=response.usage_metadata.prompt_token_count or 0,",
          "            total_tokens=response.usage_metadata.total_token_count or 0,",
          "        )",
          "        langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "        return LlmGenerateStructuredResponse(",
          "            parsed=response.parsed,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "    ) -> Iterator[str | ToolCall | Usage]:",
          "        message_dicts, tool_dicts, system_prompt = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        client = self.get_client()",
          "",
          "        total_prompt_tokens = 0",
          "        total_completion_tokens = 0",
          "",
          "        try:",
          "            stream = client.models.generate_content_stream(",
          "                model=self.model_name,",
          "                contents=message_dicts,",
          "                config=GenerateContentConfig(",
          "                    tools=tool_dicts,",
          "                    system_instruction=system_prompt,",
          "                    response_modalities=[\"TEXT\"],",
          "                    temperature=temperature or 0.0,",
          "                    max_output_tokens=max_tokens or 8192,",
          "                ),",
          "            )",
          "",
          "            current_tool_call: dict[str, Any] | None = None",
          "",
          "            for chunk in stream:",
          "                # Handle function calls",
          "                if (",
          "                    chunk.candidates[0].content",
          "                    and chunk.candidates[0].content.parts[0].function_call",
          "                ):",
          "                    function_call = chunk.candidates[0].content.parts[0].function_call",
          "                    if not current_tool_call:",
          "                        current_tool_call = {",
          "                            \"id\": str(hash(function_call.name + str(function_call.args))),",
          "                            \"function\": function_call.name,",
          "                            \"args\": json.dumps(function_call.args),",
          "                        }",
          "                        yield ToolCall(**current_tool_call)",
          "                        current_tool_call = None",
          "                # Handle text chunks",
          "                elif chunk.text is not None:",
          "                    yield \"content\", str(chunk.text)  # type: ignore[misc]",
          "",
          "                # Update token counts if available",
          "                if chunk.usage_metadata:",
          "                    if chunk.usage_metadata.prompt_token_count:",
          "                        total_prompt_tokens = chunk.usage_metadata.prompt_token_count",
          "                    if chunk.usage_metadata.candidates_token_count:",
          "                        total_completion_tokens = chunk.usage_metadata.candidates_token_count",
          "",
          "            if total_completion_tokens == 0:",
          "                raise LlmNoCompletionTokensError(\"No completion tokens returned from Gemini\")",
          "        finally:",
          "            # Yield final usage statistics",
          "            usage = Usage(",
          "                completion_tokens=total_completion_tokens,",
          "                prompt_tokens=total_prompt_tokens,",
          "                total_tokens=total_prompt_tokens + total_completion_tokens,",
          "            )",
          "            yield usage",
          "            langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Generation\")",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "    ):",
          "        message_dicts, tool_dicts, system_prompt = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        client = self.get_client()",
          "        response = client.models.generate_content(",
          "            model=self.model_name,",
          "            contents=message_dicts,",
          "            config=GenerateContentConfig(",
          "                tools=tool_dicts,",
          "                system_instruction=system_prompt,",
          "                temperature=temperature or 0.0,",
          "                max_output_tokens=max_tokens or 8192,",
          "            ),",
          "        )",
          "",
          "        message = self._format_gemini_response_to_message(response)",
          "",
          "        usage = Usage(",
          "            completion_tokens=response.usage_metadata.candidates_token_count or 0,",
          "            prompt_tokens=response.usage_metadata.prompt_token_count or 0,",
          "            total_tokens=response.usage_metadata.total_token_count or 0,",
          "        )",
          "",
          "        langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "        return LlmGenerateTextResponse(",
          "            message=message,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @classmethod",
          "    def _prep_message_and_tools(",
          "        cls,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "    ) -> tuple[list[Content], list[GeminiTool] | None, str | None]:",
          "        contents: list[Content] = []",
          "",
          "        if messages:",
          "            # Group consecutive tool messages together",
          "            grouped_messages: list[list[Message]] = []",
          "            current_group: list[Message] = []",
          "",
          "            for message in messages:",
          "                if message.role == \"tool\":",
          "                    current_group.append(message)",
          "                else:",
          "                    if current_group:",
          "                        grouped_messages.append(current_group)",
          "                        current_group = []",
          "                    grouped_messages.append([message])",
          "",
          "            if current_group:",
          "                grouped_messages.append(current_group)",
          "",
          "            # Convert each group into a Content object",
          "            for group in grouped_messages:",
          "                if len(group) == 1 and group[0].role != \"tool\":",
          "                    contents.append(cls.to_content(group[0]))",
          "                elif group[0].role == \"tool\":",
          "                    # Combine multiple tool messages into a single Content",
          "                    parts = [",
          "                        Part.from_function_response(",
          "                            name=msg.tool_call_function or \"\",",
          "                            response={\"response\": msg.content},",
          "                        )",
          "                        for msg in group",
          "                    ]",
          "                    contents.append(Content(role=\"user\", parts=parts))",
          "",
          "        if prompt:",
          "            contents.append(",
          "                Content(",
          "                    role=\"user\",",
          "                    parts=[Part(text=prompt)],",
          "                )",
          "            )",
          "",
          "        processed_tools = [cls.to_tool(tool) for tool in tools] if tools else []",
          "",
          "        return contents, processed_tools, system_prompt",
          "",
          "    @staticmethod",
          "    def to_content(message: Message) -> Content:",
          "        if message.role == \"tool_use\" or (message.role == \"assistant\" and message.tool_calls):",
          "            if not message.tool_calls:",
          "                return Content(",
          "                    role=\"model\",",
          "                    parts=[Part(text=message.content or \"\")],",
          "                )",
          "",
          "            parts = []",
          "            if message.content:",
          "                parts.append(Part(text=message.content))",
          "            for tool_call in message.tool_calls:",
          "                parts.append(",
          "                    Part.from_function_call(",
          "                        name=tool_call.function,",
          "                        args=json.loads(tool_call.args),",
          "                    )",
          "                )",
          "            return Content(role=\"model\", parts=parts)",
          "",
          "        elif message.role == \"assistant\":",
          "            return Content(",
          "                role=\"model\",",
          "                parts=[Part(text=message.content or \"\")],",
          "            )",
          "        else:",
          "            return Content(",
          "                role=\"user\",",
          "                parts=[Part(text=message.content or \"\")],",
          "            )",
          "",
          "    @staticmethod",
          "    def to_tool(tool: FunctionTool) -> GeminiTool:",
          "        return GeminiTool(",
          "            function_declarations=[",
          "                FunctionDeclaration(",
          "                    name=tool.name,",
          "                    description=tool.description,",
          "                    parameters={",
          "                        \"type\": \"OBJECT\",",
          "                        \"properties\": {",
          "                            param[\"name\"]: {",
          "                                key: value",
          "                                for key, value in {",
          "                                    \"type\": param[\"type\"].upper(),  # type: ignore",
          "                                    \"description\": param.get(\"description\", \"\"),",
          "                                    \"items\": (",
          "                                        {",
          "                                            **param.get(\"items\", {}),  # type: ignore",
          "                                            \"type\": param.get(\"items\", {}).get(\"type\", \"\").upper(),  # type: ignore",
          "                                        }",
          "                                        if param.get(\"items\") and \"type\" in param.get(\"items\", {})",
          "                                        else param.get(\"items\")",
          "                                    ),",
          "                                }.items()",
          "                                if value is not None",
          "                            }",
          "                            for param in tool.parameters",
          "                        },",
          "                        \"required\": tool.required,",
          "                    },",
          "                )",
          "            ],",
          "        )",
          "",
          "    def construct_message_from_stream(",
          "        self, content_chunks: list[str], tool_calls: list[ToolCall]",
          "    ) -> Message:",
          "        message = Message(",
          "            role=\"tool_use\" if tool_calls else \"assistant\",",
          "            content=\"\".join(content_chunks) if content_chunks else None,",
          "        )",
          "",
          "        if tool_calls:",
          "            message.tool_calls = tool_calls",
          "            message.tool_call_id = tool_calls[0].id",
          "",
          "        return message",
          "",
          "    def _format_gemini_response_to_message(self, response: GenerateContentResponse) -> Message:",
          "        parts = (",
          "            response.candidates[0].content.parts",
          "            if (",
          "                response.candidates",
          "                and len(response.candidates) > 0",
          "                and response.candidates[0].content",
          "                and response.candidates[0].content.parts",
          "            )",
          "            else []",
          "        )",
          "",
          "        message = Message(",
          "            role=\"assistant\",",
          "            content=(parts[0].text if parts and parts[0].text else None),",
          "        )",
          "",
          "        for part in parts:",
          "            if part.function_call:",
          "                if not message.tool_calls:",
          "                    message.tool_calls = []",
          "                message.tool_calls.append(",
          "                    ToolCall(",
          "                        id=part.function_call.id,",
          "                        function=part.function_call.name or \"\",",
          "                        args=json.dumps(part.function_call.args),",
          "                    )",
          "                )",
          "                message.role = \"tool_use\"",
          "                message.tool_call_id = part.function_call.id",
          "            if part.text:",
          "                message.content = part.text",
          "",
          "        return message",
          "",
          "",
          "LlmProvider = Union[OpenAiProvider, AnthropicProvider, GeminiProvider]",
          "",
          "",
          "class LlmClient:",
          "    @observe(name=\"Generate Text\")",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        model: LlmProvider,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        run_name: str | None = None,",
          "        timeout: float | None = None,",
          "        predicted_output: str | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> LlmGenerateTextResponse:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(name=run_name + \" - Generate Text\")",
          "",
          "            defaults = model.defaults",
          "            default_temperature = defaults.temperature if defaults else None",
          "",
          "            messages = LlmClient.clean_message_content(messages if messages else [])",
          "            if not tools:",
          "                messages = LlmClient.clean_tool_call_assistant_messages(messages)",
          "",
          "            if model.provider_name == LlmProviderType.OPENAI:",
          "                model = cast(OpenAiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for OpenAI\")",
          "",
          "                return model.generate_text(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                    timeout=timeout,",
          "                    predicted_output=predicted_output,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "                model = cast(AnthropicProvider, model)",
          "                return model.generate_text(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=tools,",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for Gemini\")",
          "",
          "                return model.generate_text(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                )",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "        except Exception as e:",
          "            logger.exception(f\"Text generation failed with provider {model.provider_name}: {e}\")",
          "            raise e",
          "",
          "    @observe(name=\"Generate Structured\")",
          "    def generate_structured(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        model: LlmProvider,",
          "        system_prompt: str | None = None,",
          "        response_format: Type[StructuredOutputType],",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        run_name: str | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> LlmGenerateStructuredResponse[StructuredOutputType]:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(",
          "                    name=run_name + \" - Generate Structured\"",
          "                )",
          "",
          "            messages = LlmClient.clean_message_content(messages if messages else [])",
          "",
          "            if model.provider_name == LlmProviderType.OPENAI:",
          "                model = cast(OpenAiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for OpenAI\")",
          "",
          "                messages = LlmClient.clean_tool_call_assistant_messages(messages)",
          "                return model.generate_structured(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    response_format=response_format,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "                raise NotImplementedError(\"Anthropic structured outputs are not yet supported\")",
          "            elif model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for Gemini\")",
          "                return model.generate_structured(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    response_format=response_format,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                )",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "        except Exception as e:",
          "            logger.exception(f\"Text generation failed with provider {model.provider_name}: {e}\")",
          "            raise e",
          "",
          "    @observe(name=\"Generate Text Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        model: LlmProvider,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        run_name: str | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "        first_token_timeout: float = 40.0,  # Time to first token timeout",
          "        inactivity_timeout: float = 20.0,  # Timeout for inactivity after first token",
          "    ) -> Iterator[Tuple[str, str] | ToolCall | Usage]:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(",
          "                    name=run_name + \" - Generate Text Stream\"",
          "                )",
          "",
          "            defaults = model.defaults",
          "            default_temperature = defaults.temperature if defaults else None",
          "",
          "            messages = LlmClient.clean_message_content(messages if messages else [])",
          "            if not tools:",
          "                messages = LlmClient.clean_tool_call_assistant_messages(messages)",
          "",
          "            # Get the appropriate stream generator based on provider",
          "            if model.provider_name == LlmProviderType.OPENAI:",
          "                model = cast(OpenAiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for OpenAI\")",
          "",
          "                stream_generator = model.generate_text_stream(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "                model = cast(AnthropicProvider, model)",
          "                stream_generator = model.generate_text_stream(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=tools,",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for Gemini\")",
          "",
          "                stream_generator = model.generate_text_stream(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                )",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "",
          "            # Add timeout check to the stream with differentiated timeouts",
          "            # for first token and subsequent tokens",
          "            last_yield_time = time.time()",
          "            first_token_received = False",
          "",
          "            for item in stream_generator:",
          "                current_time = time.time()",
          "                # Use first_token_timeout for the first token, inactivity_timeout for subsequent tokens",
          "                timeout_to_use = (",
          "                    first_token_timeout if not first_token_received else inactivity_timeout",
          "                )",
          "",
          "                if current_time - last_yield_time > timeout_to_use:",
          "                    if first_token_received:",
          "                        raise LlmStreamInactivityTimeoutError(",
          "                            f\"Stream inactivity timeout after {timeout_to_use} seconds\"",
          "                        )",
          "                    else:",
          "                        raise LlmStreamFirstTokenTimeoutError(",
          "                            f\"Stream time to first token timeout after {timeout_to_use} seconds\"",
          "                        )",
          "",
          "                # Mark that we've received at least one token",
          "                first_token_received = True",
          "                last_yield_time = current_time",
          "                yield item",
          "",
          "        except Exception as e:",
          "            logger.exception(",
          "                f\"Text stream generation failed with provider {model.provider_name}: {e}\"",
          "            )",
          "            raise e",
          "",
          "    @observe(name=\"Generate Text from Web Search\")",
          "    def generate_text_from_web_search(",
          "        self,",
          "        *,",
          "        prompt: str,",
          "        model: LlmProvider,",
          "        temperature: float | None = None,",
          "        run_name: str | None = None,",
          "    ) -> str:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(name=run_name + \" - Generate Text\")",
          "",
          "            defaults = model.defaults",
          "            default_temperature = defaults.temperature if defaults else None",
          "",
          "            if model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "                return model.search_the_web(prompt, temperature or default_temperature)",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "        except Exception as e:",
          "            logger.exception(",
          "                f\"Text generation from web failed with provider {model.provider_name}: {e}\"",
          "            )",
          "            raise e",
          "",
          "    @staticmethod",
          "    def clean_tool_call_assistant_messages(messages: list[Message]) -> list[Message]:",
          "        new_messages = []",
          "        for message in messages:",
          "            if message.role == \"assistant\" and message.tool_calls:",
          "                new_messages.append(",
          "                    Message(role=\"assistant\", content=message.content, tool_calls=[])",
          "                )",
          "            elif message.role == \"tool\":",
          "                new_messages.append(Message(role=\"user\", content=message.content, tool_calls=[]))",
          "            elif message.role == \"tool_use\":",
          "                new_messages.append(",
          "                    Message(role=\"assistant\", content=message.content, tool_calls=[])",
          "                )",
          "            else:",
          "                new_messages.append(message)",
          "        return new_messages",
          "",
          "    @staticmethod",
          "    def clean_assistant_messages(messages: list[Message]) -> list[Message]:",
          "        new_messages = []",
          "        for message in messages:",
          "            if message.role == \"assistant\" or message.role == \"tool_use\":",
          "                message.content = \".\"",
          "                new_messages.append(message)",
          "            else:",
          "                new_messages.append(message)",
          "        return new_messages",
          "",
          "    @staticmethod",
          "    def clean_message_content(messages: list[Message]) -> list[Message]:",
          "        new_messages = []",
          "        for message in messages:",
          "            if not message.content:",
          "                message.content = \".\"",
          "            new_messages.append(message)",
          "        return new_messages",
          "",
          "    def construct_message_from_stream(",
          "        self,",
          "        content_chunks: list[str],",
          "        tool_calls: list[ToolCall],",
          "        model: LlmProvider,",
          "        thinking_content_chunks: list[str] = [],",
          "        thinking_signature: str | None = None,",
          "    ) -> Message:",
          "        if model.provider_name == LlmProviderType.OPENAI:",
          "            model = cast(OpenAiProvider, model)",
          "            return model.construct_message_from_stream(content_chunks, tool_calls)",
          "        elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "            model = cast(AnthropicProvider, model)",
          "            return model.construct_message_from_stream(",
          "                content_chunks, tool_calls, thinking_content_chunks, thinking_signature",
          "            )",
          "        elif model.provider_name == LlmProviderType.GEMINI:",
          "            model = cast(GeminiProvider, model)",
          "            return model.construct_message_from_stream(content_chunks, tool_calls)",
          "        else:",
          "            raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "",
          "",
          "@module.provider",
          "def provide_llm_client() -> LlmClient:",
          "    return LlmClient()",
          ""
        ]
      }
    },
    {
      "file": "seer/automation/agent/client.py",
      "image": "seer.automation.agent.client",
      "is_application": true,
      "line": 898,
      "name": "GeminiProvider.generate_structured",
      "path": "/app/src/seer/automation/agent/client.py",
      "codeContext": {
        "file": "seer/automation/agent/client.py",
        "line": 898,
        "name": "GeminiProvider.generate_structured",
        "code": "        temperature: float | None = None,\n        response_format: Type[StructuredOutputType],\n        max_tokens: int | None = None,\n    ) -> LlmGenerateStructuredResponse[StructuredOutputType]:\n        message_dicts, tool_dicts, system_prompt = self._prep_message_and_tools(\n            messages=messages,\n            prompt=prompt,\n            system_prompt=system_prompt,\n            tools=tools,\n        )\n\n        client = self.get_client()\n\n        max_retries = 2  # Gemini sometimes doesn't fill in response.parsed\n        for _ in range(max_retries + 1):\n            response = client.models.generate_content(\n                model=self.model_name,\n                contents=message_dicts,\n                config=GenerateContentConfig(\n                    tools=tool_dicts,",
        "lineRange": {
          "start": 889,
          "end": 908
        },
        "lines": [
          "import json",
          "import logging",
          "import re",
          "import time",
          "from dataclasses import dataclass",
          "from typing import Any, ClassVar, Iterable, Iterator, Tuple, Type, Union, cast",
          "",
          "import anthropic",
          "from anthropic import NOT_GIVEN",
          "from anthropic.types import (",
          "    MessageParam,",
          "    TextBlockParam,",
          "    ThinkingBlockParam,",
          "    ThinkingConfigEnabledParam,",
          "    ToolParam,",
          "    ToolResultBlockParam,",
          "    ToolUseBlockParam,",
          ")",
          "from google import genai  # type: ignore[attr-defined]",
          "from google.genai.errors import ClientError, ServerError",
          "from google.genai.types import (",
          "    Content,",
          "    FunctionDeclaration,",
          "    GenerateContentConfig,",
          "    GenerateContentResponse,",
          "    GoogleSearch,",
          "    Part,",
          ")",
          "from google.genai.types import Tool as GeminiTool",
          "from langfuse.decorators import langfuse_context, observe",
          "from langfuse.openai import openai",
          "from openai.types.chat import ChatCompletionMessageParam, ChatCompletionToolParam",
          "",
          "from seer.automation.agent.models import (",
          "    LlmGenerateStructuredResponse,",
          "    LlmGenerateTextResponse,",
          "    LlmModelDefaultConfig,",
          "    LlmNoCompletionTokensError,",
          "    LlmProviderDefaults,",
          "    LlmProviderType,",
          "    LlmRefusalError,",
          "    LlmResponseMetadata,",
          "    LlmStreamFirstTokenTimeoutError,",
          "    LlmStreamInactivityTimeoutError,",
          "    LlmStreamTimeoutError,",
          "    Message,",
          "    StructuredOutputType,",
          "    ToolCall,",
          "    Usage,",
          ")",
          "from seer.automation.agent.tools import ClaudeTool, FunctionTool",
          "from seer.bootup import module",
          "from seer.configuration import AppConfig",
          "from seer.dependency_injection import inject, injected",
          "from seer.utils import backoff_on_exception",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "@dataclass",
          "class OpenAiProvider:",
          "    model_name: str",
          "    provider_name = LlmProviderType.OPENAI",
          "    defaults: LlmProviderDefaults | None = None",
          "",
          "    default_configs: ClassVar[list[LlmModelDefaultConfig]] = [",
          "        LlmModelDefaultConfig(",
          "            match=r\"^o1-mini.*\",",
          "            defaults=LlmProviderDefaults(temperature=1.0),",
          "        ),",
          "        LlmModelDefaultConfig(",
          "            match=r\"^o1-preview.*\",",
          "            defaults=LlmProviderDefaults(temperature=1.0),",
          "        ),",
          "        LlmModelDefaultConfig(",
          "            match=r\"^o3-mini.*\",",
          "            defaults=LlmProviderDefaults(temperature=1.0),",
          "        ),",
          "        LlmModelDefaultConfig(",
          "            match=r\".*\",",
          "            defaults=LlmProviderDefaults(temperature=0.0),",
          "        ),",
          "    ]",
          "",
          "    @staticmethod",
          "    def get_client() -> openai.Client:",
          "        return openai.Client(max_retries=4)",
          "",
          "    @classmethod",
          "    def model(cls, model_name: str) -> \"OpenAiProvider\":",
          "        model_config = cls._get_config(model_name)",
          "        return cls(",
          "            model_name=model_name,",
          "            defaults=model_config.defaults if model_config else None,",
          "        )",
          "",
          "    @classmethod",
          "    def _get_config(cls, model_name: str):",
          "        for config in cls.default_configs:",
          "            if re.match(config.match, model_name):",
          "                return config",
          "        return None",
          "",
          "    @staticmethod",
          "    def is_completion_exception_retryable(exception: Exception) -> bool:",
          "        return isinstance(exception, openai.InternalServerError) or isinstance(",
          "            exception, LlmStreamTimeoutError",
          "        )",
          "",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        predicted_output: str | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ):",
          "        message_dicts, tool_dicts = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "            reasoning_effort=reasoning_effort,",
          "        )",
          "",
          "        openai_client = self.get_client()",
          "",
          "        completion = openai_client.chat.completions.create(",
          "            model=self.model_name,",
          "            messages=cast(Iterable[ChatCompletionMessageParam], message_dicts),",
          "            temperature=temperature,",
          "            tools=(",
          "                cast(Iterable[ChatCompletionToolParam], tool_dicts)",
          "                if tool_dicts",
          "                else openai.NotGiven()",
          "            ),",
          "            max_tokens=max_tokens or openai.NotGiven(),",
          "            timeout=timeout or openai.NotGiven(),",
          "            prediction=(",
          "                {",
          "                    \"type\": \"content\",",
          "                    \"content\": predicted_output,",
          "                }",
          "                if predicted_output",
          "                else openai.NotGiven()",
          "            ),",
          "            reasoning_effort=reasoning_effort if reasoning_effort else openai.NotGiven(),",
          "        )",
          "",
          "        openai_message = completion.choices[0].message",
          "        if openai_message.refusal:",
          "            raise LlmRefusalError(completion.choices[0].message.refusal)",
          "",
          "        message = Message(",
          "            content=openai_message.content,",
          "            role=openai_message.role,",
          "            tool_calls=(",
          "                [",
          "                    ToolCall(id=call.id, function=call.function.name, args=call.function.arguments)",
          "                    for call in openai_message.tool_calls",
          "                ]",
          "                if openai_message.tool_calls",
          "                else None",
          "            ),",
          "        )",
          "",
          "        usage = Usage(",
          "            completion_tokens=completion.usage.completion_tokens if completion.usage else 0,",
          "            prompt_tokens=completion.usage.prompt_tokens if completion.usage else 0,",
          "            total_tokens=completion.usage.total_tokens if completion.usage else 0,",
          "        )",
          "",
          "        return LlmGenerateTextResponse(",
          "            message=message,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    def generate_structured(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        response_format: Type[StructuredOutputType],",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> LlmGenerateStructuredResponse[StructuredOutputType]:",
          "        message_dicts, tool_dicts = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "            reasoning_effort=reasoning_effort,",
          "        )",
          "",
          "        openai_client = self.get_client()",
          "",
          "        completion = openai_client.beta.chat.completions.parse(",
          "            model=self.model_name,",
          "            messages=cast(Iterable[ChatCompletionMessageParam], message_dicts),",
          "            temperature=temperature,",
          "            tools=(",
          "                cast(Iterable[ChatCompletionToolParam], tool_dicts)",
          "                if tool_dicts",
          "                else openai.NotGiven()",
          "            ),",
          "            response_format=response_format,",
          "            max_tokens=max_tokens or openai.NotGiven(),",
          "            timeout=timeout or openai.NotGiven(),",
          "            reasoning_effort=reasoning_effort if reasoning_effort else openai.NotGiven(),",
          "        )",
          "",
          "        openai_message = completion.choices[0].message",
          "        if openai_message.refusal:",
          "            raise LlmRefusalError(completion.choices[0].message.refusal)",
          "",
          "        parsed = cast(StructuredOutputType, completion.choices[0].message.parsed)",
          "",
          "        usage = Usage(",
          "            completion_tokens=completion.usage.completion_tokens if completion.usage else 0,",
          "            prompt_tokens=completion.usage.prompt_tokens if completion.usage else 0,",
          "            total_tokens=completion.usage.total_tokens if completion.usage else 0,",
          "        )",
          "",
          "        return LlmGenerateStructuredResponse(",
          "            parsed=parsed,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @staticmethod",
          "    def to_message_dict(message: Message) -> ChatCompletionMessageParam:",
          "        message_dict: dict[str, Any] = {",
          "            \"content\": message.content if message.content else \"\",",
          "            \"role\": message.role,",
          "        }",
          "",
          "        if message.tool_calls:",
          "            tool_calls = [tool_call.model_dump(mode=\"json\") for tool_call in message.tool_calls]",
          "            parsed_tool_calls = []",
          "            for item in tool_calls:",
          "                new_item = item.copy()",
          "                new_item[\"function\"] = {\"name\": item[\"function\"], \"arguments\": item[\"args\"]}",
          "                new_item[\"type\"] = \"function\"",
          "                parsed_tool_calls.append(new_item)",
          "            message_dict[\"tool_calls\"] = parsed_tool_calls",
          "            message_dict[\"role\"] = \"assistant\"",
          "",
          "        if message.tool_call_id:",
          "            message_dict[\"tool_call_id\"] = message.tool_call_id",
          "",
          "        return cast(ChatCompletionMessageParam, message_dict)",
          "",
          "    @staticmethod",
          "    def to_tool_dict(tool: FunctionTool) -> ChatCompletionToolParam:",
          "        return ChatCompletionToolParam(",
          "            type=\"function\",",
          "            function={",
          "                \"name\": tool.name,",
          "                \"description\": tool.description,",
          "                \"parameters\": {",
          "                    \"type\": \"object\",",
          "                    \"properties\": {",
          "                        param[\"name\"]: {",
          "                            key: value",
          "                            for key, value in {",
          "                                \"type\": param[\"type\"],",
          "                                \"description\": param.get(\"description\", \"\"),",
          "                                \"items\": param.get(\"items\"),",
          "                            }.items()",
          "                            if value is not None",
          "                        }",
          "                        for param in tool.parameters",
          "                    },",
          "                    \"required\": tool.required,",
          "                },",
          "            },",
          "        )",
          "",
          "    @classmethod",
          "    def _prep_message_and_tools(",
          "        cls,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ):",
          "        message_dicts = [cls.to_message_dict(message) for message in messages] if messages else []",
          "        if system_prompt:",
          "            message_dicts.insert(",
          "                0,",
          "                cls.to_message_dict(",
          "                    Message(",
          "                        role=\"system\" if not reasoning_effort else \"developer\",",
          "                        content=system_prompt,",
          "                    )",
          "                ),",
          "            )",
          "        if prompt:",
          "            message_dicts.append(cls.to_message_dict(Message(role=\"user\", content=prompt)))",
          "",
          "        tool_dicts = (",
          "            [cls.to_tool_dict(tool) for tool in tools] if tools and len(tools) > 0 else None",
          "        )",
          "",
          "        return message_dicts, tool_dicts",
          "",
          "    @observe(as_type=\"generation\", name=\"OpenAI Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> Iterator[Tuple[str, str] | ToolCall | Usage]:",
          "        message_dicts, tool_dicts = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "            reasoning_effort=reasoning_effort,",
          "        )",
          "",
          "        openai_client = self.get_client()",
          "",
          "        stream = openai_client.chat.completions.create(",
          "            model=self.model_name,",
          "            messages=cast(Iterable[ChatCompletionMessageParam], message_dicts),",
          "            temperature=temperature,",
          "            tools=(",
          "                cast(Iterable[ChatCompletionToolParam], tool_dicts)",
          "                if tool_dicts",
          "                else openai.NotGiven()",
          "            ),",
          "            max_tokens=max_tokens or openai.NotGiven(),",
          "            timeout=timeout or openai.NotGiven(),",
          "            stream=True,",
          "            stream_options={\"include_usage\": True},",
          "            reasoning_effort=reasoning_effort if reasoning_effort else openai.NotGiven(),",
          "        )",
          "",
          "        try:",
          "            current_tool_call: dict[str, Any] | None = None",
          "            current_tool_call_index = 0",
          "",
          "            for chunk in stream:",
          "                if not chunk.choices and chunk.usage:",
          "                    usage = Usage(",
          "                        completion_tokens=chunk.usage.completion_tokens,",
          "                        prompt_tokens=chunk.usage.prompt_tokens,",
          "                        total_tokens=chunk.usage.total_tokens,",
          "                    )",
          "                    yield usage",
          "                    langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "                    break",
          "",
          "                delta = chunk.choices[0].delta",
          "                if delta.tool_calls:",
          "                    tool_call = delta.tool_calls[0]",
          "",
          "                    if (",
          "                        not current_tool_call or current_tool_call_index != tool_call.index",
          "                    ):  # Start of new tool call",
          "                        current_tool_call_index = tool_call.index",
          "                        if current_tool_call:",
          "                            yield ToolCall(**current_tool_call)",
          "                        current_tool_call = None",
          "                        current_tool_call = {",
          "                            \"id\": tool_call.id,",
          "                            \"function\": tool_call.function.name if tool_call.function.name else \"\",",
          "                            \"args\": (",
          "                                tool_call.function.arguments if tool_call.function.arguments else \"\"",
          "                            ),",
          "                        }",
          "                    else:",
          "                        if tool_call.function.arguments:",
          "                            current_tool_call[\"args\"] += tool_call.function.arguments",
          "                if chunk.choices[0].finish_reason == \"tool_calls\" and current_tool_call:",
          "                    yield ToolCall(**current_tool_call)",
          "                if delta.content:",
          "                    yield \"content\", delta.content",
          "        finally:",
          "            stream.response.close()",
          "",
          "    def construct_message_from_stream(",
          "        self, content_chunks: list[str], tool_calls: list[ToolCall]",
          "    ) -> Message:",
          "        return Message(",
          "            role=\"assistant\",",
          "            content=\"\".join(content_chunks) if content_chunks else None,",
          "            tool_calls=tool_calls if tool_calls else None,",
          "        )",
          "",
          "",
          "@dataclass",
          "class AnthropicProvider:",
          "    model_name: str",
          "    provider_name = LlmProviderType.ANTHROPIC",
          "    defaults: LlmProviderDefaults | None = None",
          "",
          "    default_configs: ClassVar[list[LlmModelDefaultConfig]] = [",
          "        LlmModelDefaultConfig(",
          "            match=r\".*\",",
          "            defaults=LlmProviderDefaults(temperature=0.0),",
          "        ),",
          "    ]",
          "",
          "    @staticmethod",
          "    @inject",
          "    def get_client(app_config: AppConfig = injected) -> anthropic.AnthropicVertex:",
          "        if app_config.SENTRY_REGION == \"de\":",
          "            region = \"europe-west1\"",
          "        elif app_config.DEV:",
          "            region = \"us-east5\"",
          "        else:",
          "            region = \"global\"",
          "        return anthropic.AnthropicVertex(",
          "            project_id=app_config.GOOGLE_CLOUD_PROJECT,",
          "            region=region,",
          "            max_retries=8,",
          "        )",
          "",
          "    @classmethod",
          "    def model(cls, model_name: str) -> \"AnthropicProvider\":",
          "        model_config = cls._get_config(model_name)",
          "        return cls(",
          "            model_name=model_name,",
          "            defaults=model_config.defaults if model_config else None,",
          "        )",
          "",
          "    @classmethod",
          "    def _get_config(cls, model_name: str):",
          "        for config in cls.default_configs:",
          "            if re.match(config.match, model_name):",
          "                return config",
          "        return None",
          "",
          "    @staticmethod",
          "    def is_completion_exception_retryable(exception: Exception) -> bool:",
          "        retryable_errors = (",
          "            \"overloaded_error\",",
          "            \"Internal server error\",",
          "            \"not_found_error\",",
          "        )",
          "        return (",
          "            (",
          "                isinstance(exception, anthropic.AnthropicError)",
          "                and any(error in str(exception) for error in retryable_errors)",
          "            )",
          "            or isinstance(exception, LlmStreamTimeoutError)",
          "            or \"incomplete chunked read\" in str(exception)",
          "        )",
          "",
          "    @observe(as_type=\"generation\", name=\"Anthropic Generation\")",
          "    @inject",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ):",
          "        message_dicts, tool_dicts, system_prompt_block = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        anthropic_client = self.get_client()",
          "",
          "        completion = anthropic_client.messages.create(",
          "            system=system_prompt_block or NOT_GIVEN,",
          "            model=self.model_name,",
          "            tools=cast(Iterable[ToolParam], tool_dicts) if tool_dicts else NOT_GIVEN,",
          "            messages=cast(Iterable[MessageParam], message_dicts),",
          "            max_tokens=max_tokens or 8192,",
          "            temperature=temperature or NOT_GIVEN,",
          "            timeout=timeout or NOT_GIVEN,",
          "            thinking=(",
          "                ThinkingConfigEnabledParam(",
          "                    type=\"enabled\",",
          "                    budget_tokens=(",
          "                        1024",
          "                        if reasoning_effort == \"low\"",
          "                        else 4092 if reasoning_effort == \"medium\" else 8192",
          "                    ),",
          "                )",
          "                if reasoning_effort",
          "                else NOT_GIVEN",
          "            ),",
          "        )",
          "",
          "        message = self._format_claude_response_to_message(completion)",
          "",
          "        usage = Usage(",
          "            completion_tokens=completion.usage.output_tokens,",
          "            prompt_tokens=completion.usage.input_tokens,",
          "            total_tokens=completion.usage.input_tokens + completion.usage.output_tokens,",
          "        )",
          "",
          "        langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "        return LlmGenerateTextResponse(",
          "            message=message,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @staticmethod",
          "    def _format_claude_response_to_message(completion: anthropic.types.Message) -> Message:",
          "        message = Message(role=completion.role)",
          "        for block in completion.content:",
          "            if block.type == \"text\":",
          "                message.content = (",
          "                    block.text",
          "                )  # we're assuming there's only one text block per message",
          "            elif block.type == \"tool_use\":",
          "                if not message.tool_calls:",
          "                    message.tool_calls = []",
          "                message.tool_calls.append(",
          "                    ToolCall(id=block.id, function=block.name, args=json.dumps(block.input))",
          "                )",
          "                message.role = \"tool_use\"",
          "                message.tool_call_id = message.tool_calls[",
          "                    0",
          "                ].id  # assumes we get only 1 tool call at a time, but we really don't use this field for tool_use blocks",
          "            elif block.type == \"thinking\":",
          "                message.thinking_content = block.thinking",
          "                message.thinking_signature = block.signature",
          "        return message",
          "",
          "    @staticmethod",
          "    def to_message_param(message: Message) -> MessageParam:",
          "        if message.role == \"tool\":",
          "            return MessageParam(",
          "                role=\"user\",",
          "                content=[",
          "                    ToolResultBlockParam(",
          "                        type=\"tool_result\",",
          "                        content=message.content or \"\",",
          "                        tool_use_id=message.tool_call_id or \"\",",
          "                    )",
          "                ],",
          "            )",
          "        elif message.role == \"tool_use\" or (message.role == \"assistant\" and message.tool_calls):",
          "            assistant_msg_content: list[ThinkingBlockParam | ToolUseBlockParam] = []",
          "            if message.thinking_content and message.thinking_signature:",
          "                assistant_msg_content.append(",
          "                    ThinkingBlockParam(",
          "                        type=\"thinking\",",
          "                        thinking=message.thinking_content,",
          "                        signature=message.thinking_signature,",
          "                    )",
          "                )",
          "            if message.tool_calls:",
          "                tool_call = message.tool_calls[0]  # Assuming only one tool call per message",
          "                assistant_msg_content.append(",
          "                    ToolUseBlockParam(",
          "                        type=\"tool_use\",",
          "                        id=tool_call.id or \"\",",
          "                        name=tool_call.function,",
          "                        input=json.loads(tool_call.args),",
          "                    )",
          "                )",
          "            return MessageParam(",
          "                role=\"assistant\",",
          "                content=assistant_msg_content,",
          "            )",
          "        else:",
          "            other_content: list[ThinkingBlockParam | TextBlockParam] = []",
          "            if message.thinking_content and message.thinking_signature:",
          "                other_content.append(",
          "                    ThinkingBlockParam(",
          "                        type=\"thinking\",",
          "                        thinking=message.thinking_content,",
          "                        signature=message.thinking_signature,",
          "                    )",
          "                )",
          "            other_content.append(TextBlockParam(type=\"text\", text=message.content or \"\"))",
          "            return MessageParam(",
          "                role=message.role,  # type: ignore",
          "                content=other_content,",
          "            )",
          "",
          "    @staticmethod",
          "    def to_tool_dict(tool: FunctionTool | ClaudeTool) -> ToolParam:",
          "        if isinstance(tool, ClaudeTool):",
          "            return ToolParam(  # type: ignore",
          "                name=tool.name,",
          "                type=tool.type,",
          "            )",
          "",
          "        return ToolParam(",
          "            name=tool.name,",
          "            description=tool.description,",
          "            input_schema={",
          "                \"type\": \"object\",",
          "                \"properties\": {",
          "                    param[\"name\"]: {",
          "                        key: value",
          "                        for key, value in {",
          "                            \"type\": param[\"type\"],",
          "                            \"description\": param.get(\"description\", \"\"),",
          "                            \"items\": param.get(\"items\"),",
          "                        }.items()",
          "                        if value is not None",
          "                    }",
          "                    for param in tool.parameters",
          "                },",
          "                \"required\": tool.required,",
          "            },",
          "        )",
          "",
          "    @classmethod",
          "    def _prep_message_and_tools(",
          "        cls,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "    ) -> tuple[list[MessageParam], list[ToolParam] | None, list[TextBlockParam] | None]:",
          "        message_dicts = [cls.to_message_param(message) for message in messages] if messages else []",
          "        if prompt:",
          "            message_dicts.append(cls.to_message_param(Message(role=\"user\", content=prompt)))",
          "        if message_dicts:",
          "            message_dicts[-1][\"content\"][0][\"cache_control\"] = {\"type\": \"ephemeral\"}  # type: ignore[index]",
          "",
          "        tool_dicts = (",
          "            [cls.to_tool_dict(tool) for tool in tools] if tools and len(tools) > 0 else None",
          "        )",
          "",
          "        system_prompt_block = (",
          "            [TextBlockParam(type=\"text\", text=system_prompt, cache_control={\"type\": \"ephemeral\"})]",
          "            if system_prompt",
          "            else None",
          "        )",
          "",
          "        return message_dicts, tool_dicts, system_prompt_block",
          "",
          "    @observe(as_type=\"generation\", name=\"Anthropic Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> Iterator[Tuple[str, str] | ToolCall | Usage]:",
          "        message_dicts, tool_dicts, system_prompt_block = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        anthropic_client = self.get_client()",
          "",
          "        stream = anthropic_client.messages.create(",
          "            system=system_prompt_block or NOT_GIVEN,",
          "            model=self.model_name,",
          "            tools=cast(Iterable[ToolParam], tool_dicts) if tool_dicts else NOT_GIVEN,",
          "            messages=cast(Iterable[MessageParam], message_dicts),",
          "            max_tokens=max_tokens or 8192,",
          "            temperature=temperature or NOT_GIVEN,",
          "            timeout=timeout or NOT_GIVEN,",
          "            stream=True,",
          "            thinking=(",
          "                ThinkingConfigEnabledParam(",
          "                    type=\"enabled\",",
          "                    budget_tokens=(",
          "                        1024",
          "                        if reasoning_effort == \"low\"",
          "                        else 4092 if reasoning_effort == \"medium\" else 8192",
          "                    ),",
          "                )",
          "                if reasoning_effort",
          "                else NOT_GIVEN",
          "            ),",
          "        )",
          "",
          "        try:",
          "            current_tool_call: dict[str, Any] | None = None",
          "            current_input_json = []",
          "            total_input_write_tokens = 0",
          "            total_input_read_tokens = 0",
          "            total_input_tokens = 0",
          "            total_output_tokens = 0",
          "",
          "            for chunk in stream:",
          "                if chunk.type == \"message_start\" and chunk.message.usage:",
          "                    if chunk.message.usage.cache_creation_input_tokens:",
          "                        total_input_write_tokens += chunk.message.usage.cache_creation_input_tokens",
          "                    if chunk.message.usage.cache_read_input_tokens:",
          "                        total_input_read_tokens += chunk.message.usage.cache_read_input_tokens",
          "                    total_input_tokens += chunk.message.usage.input_tokens",
          "                    total_output_tokens += chunk.message.usage.output_tokens",
          "                elif chunk.type == \"message_delta\" and chunk.usage:",
          "                    total_output_tokens += chunk.usage.output_tokens",
          "",
          "                if chunk.type == \"message_stop\":",
          "                    break",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"text_delta\":",
          "                    yield \"content\", chunk.delta.text",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"thinking_delta\":",
          "                    yield \"thinking_content\", chunk.delta.thinking",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"signature_delta\":",
          "                    yield \"thinking_signature\", chunk.delta.signature",
          "                elif chunk.type == \"content_block_start\" and chunk.content_block.type == \"tool_use\":",
          "                    # Start accumulating a new tool call",
          "                    current_tool_call = {",
          "                        \"id\": chunk.content_block.id,",
          "                        \"function\": chunk.content_block.name,",
          "                        \"args\": \"\",",
          "                    }",
          "                elif chunk.type == \"content_block_delta\" and chunk.delta.type == \"input_json_delta\":",
          "                    # Accumulate the input JSON",
          "                    if current_tool_call:",
          "                        current_input_json.append(chunk.delta.partial_json)",
          "                elif chunk.type == \"content_block_stop\" and current_tool_call:",
          "                    # Tool call is complete, yield it",
          "                    current_tool_call[\"args\"] = \"\".join(current_input_json)",
          "                    yield ToolCall(**current_tool_call)",
          "                    current_tool_call = None",
          "                    current_input_json = []",
          "        finally:",
          "            usage = Usage(",
          "                completion_tokens=total_output_tokens,",
          "                prompt_tokens=total_input_tokens,",
          "                total_tokens=total_input_tokens + total_output_tokens,",
          "                prompt_cache_write_tokens=total_input_write_tokens,",
          "                prompt_cache_read_tokens=total_input_read_tokens,",
          "            )",
          "            yield usage",
          "            langfuse_context.update_current_observation(",
          "                model=self.model_name, usage=usage.to_langfuse_usage()",
          "            )",
          "            stream.response.close()",
          "",
          "    def construct_message_from_stream(",
          "        self,",
          "        content_chunks: list[str],",
          "        tool_calls: list[ToolCall],",
          "        thinking_content_chunks: list[str],",
          "        thinking_signature: str | None,",
          "    ) -> Message:",
          "        message = Message(",
          "            role=\"tool_use\" if tool_calls else \"assistant\",",
          "            content=\"\".join(content_chunks) if content_chunks else None,",
          "            thinking_content=\"\".join(thinking_content_chunks) if thinking_content_chunks else None,",
          "            thinking_signature=thinking_signature,",
          "        )",
          "",
          "        if tool_calls:",
          "            message.tool_calls = tool_calls",
          "            message.tool_call_id = tool_calls[0].id",
          "",
          "        return message",
          "",
          "",
          "@dataclass",
          "class GeminiProvider:",
          "    model_name: str",
          "    provider_name = LlmProviderType.GEMINI",
          "    defaults: LlmProviderDefaults | None = None",
          "",
          "    default_configs: ClassVar[list[LlmModelDefaultConfig]] = [",
          "        LlmModelDefaultConfig(",
          "            match=r\".*\",",
          "            defaults=LlmProviderDefaults(temperature=0.0),",
          "        ),",
          "    ]",
          "",
          "    @staticmethod",
          "    def get_client() -> genai.Client:",
          "        client = genai.Client(",
          "            vertexai=True,",
          "            location=\"us-central1\",",
          "        )",
          "        # The gemini client currently doesn't have a built-in retry mechanism.",
          "        retrier = backoff_on_exception(",
          "            GeminiProvider.is_completion_exception_retryable, max_tries=4",
          "        )",
          "        client.models.generate_content = retrier(client.models.generate_content)",
          "        return client",
          "",
          "    @classmethod",
          "    def model(cls, model_name: str) -> \"GeminiProvider\":",
          "        model_config = cls._get_config(model_name)",
          "        return cls(",
          "            model_name=model_name,",
          "            defaults=model_config.defaults if model_config else None,",
          "        )",
          "",
          "    @classmethod",
          "    def _get_config(cls, model_name: str):",
          "        for config in cls.default_configs:",
          "            if re.match(config.match, model_name):",
          "                return config",
          "        return None",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Generation with Grounding\")",
          "    def search_the_web(self, prompt: str, temperature: float | None = None) -> str:",
          "        client = self.get_client()",
          "        google_search_tool = GeminiTool(google_search=GoogleSearch())",
          "",
          "        response = client.models.generate_content(",
          "            model=self.model_name,",
          "            contents=prompt,",
          "            config=GenerateContentConfig(",
          "                tools=[google_search_tool],",
          "                response_modalities=[\"TEXT\"],",
          "                temperature=temperature or 0.0,",
          "            ),",
          "        )",
          "        answer = \"\"",
          "        if (",
          "            response.candidates",
          "            and response.candidates[0].content",
          "            and response.candidates[0].content.parts",
          "        ):",
          "            for each in response.candidates[0].content.parts:",
          "                if each.text:",
          "                    answer += each.text",
          "        return answer",
          "",
          "    @staticmethod",
          "    def is_completion_exception_retryable(exception: Exception) -> bool:",
          "        retryable_errors = (",
          "            \"Resource exhausted. Please try again later.\",",
          "            \"429 RESOURCE_EXHAUSTED\",",
          "            # https://sentry.sentry.io/issues/6301072208",
          "            \"TLS/SSL connection has been closed\",",
          "            \"Max retries exceeded with url\",",
          "            \"Internal error\",",
          "        )",
          "        return (",
          "            isinstance(exception, ServerError)",
          "            or (",
          "                isinstance(exception, ClientError)",
          "                and any(error in str(exception) for error in retryable_errors)",
          "            )",
          "            or isinstance(exception, LlmNoCompletionTokensError)",
          "        ) or isinstance(exception, LlmStreamTimeoutError)",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Generation\")",
          "    def generate_structured(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        response_format: Type[StructuredOutputType],",
          "        max_tokens: int | None = None,",
          "    ) -> LlmGenerateStructuredResponse[StructuredOutputType]:",
          "        message_dicts, tool_dicts, system_prompt = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        client = self.get_client()",
          "",
          "        max_retries = 2  # Gemini sometimes doesn't fill in response.parsed",
          "        for _ in range(max_retries + 1):",
          "            response = client.models.generate_content(",
          "                model=self.model_name,",
          "                contents=message_dicts,",
          "                config=GenerateContentConfig(",
          "                    tools=tool_dicts,",
          "                    response_modalities=[\"TEXT\"],",
          "                    temperature=temperature or 0.0,",
          "                    response_mime_type=\"application/json\",",
          "                    max_output_tokens=max_tokens or 8192,",
          "                    response_schema=response_format,",
          "                ),",
          "            )",
          "            if response.parsed is not None:",
          "                break",
          "",
          "        usage = Usage(",
          "            completion_tokens=response.usage_metadata.candidates_token_count or 0,",
          "            prompt_tokens=response.usage_metadata.prompt_token_count or 0,",
          "            total_tokens=response.usage_metadata.total_token_count or 0,",
          "        )",
          "        langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "        return LlmGenerateStructuredResponse(",
          "            parsed=response.parsed,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "    ) -> Iterator[str | ToolCall | Usage]:",
          "        message_dicts, tool_dicts, system_prompt = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        client = self.get_client()",
          "",
          "        total_prompt_tokens = 0",
          "        total_completion_tokens = 0",
          "",
          "        try:",
          "            stream = client.models.generate_content_stream(",
          "                model=self.model_name,",
          "                contents=message_dicts,",
          "                config=GenerateContentConfig(",
          "                    tools=tool_dicts,",
          "                    system_instruction=system_prompt,",
          "                    response_modalities=[\"TEXT\"],",
          "                    temperature=temperature or 0.0,",
          "                    max_output_tokens=max_tokens or 8192,",
          "                ),",
          "            )",
          "",
          "            current_tool_call: dict[str, Any] | None = None",
          "",
          "            for chunk in stream:",
          "                # Handle function calls",
          "                if (",
          "                    chunk.candidates[0].content",
          "                    and chunk.candidates[0].content.parts[0].function_call",
          "                ):",
          "                    function_call = chunk.candidates[0].content.parts[0].function_call",
          "                    if not current_tool_call:",
          "                        current_tool_call = {",
          "                            \"id\": str(hash(function_call.name + str(function_call.args))),",
          "                            \"function\": function_call.name,",
          "                            \"args\": json.dumps(function_call.args),",
          "                        }",
          "                        yield ToolCall(**current_tool_call)",
          "                        current_tool_call = None",
          "                # Handle text chunks",
          "                elif chunk.text is not None:",
          "                    yield \"content\", str(chunk.text)  # type: ignore[misc]",
          "",
          "                # Update token counts if available",
          "                if chunk.usage_metadata:",
          "                    if chunk.usage_metadata.prompt_token_count:",
          "                        total_prompt_tokens = chunk.usage_metadata.prompt_token_count",
          "                    if chunk.usage_metadata.candidates_token_count:",
          "                        total_completion_tokens = chunk.usage_metadata.candidates_token_count",
          "",
          "            if total_completion_tokens == 0:",
          "                raise LlmNoCompletionTokensError(\"No completion tokens returned from Gemini\")",
          "        finally:",
          "            # Yield final usage statistics",
          "            usage = Usage(",
          "                completion_tokens=total_completion_tokens,",
          "                prompt_tokens=total_prompt_tokens,",
          "                total_tokens=total_prompt_tokens + total_completion_tokens,",
          "            )",
          "            yield usage",
          "            langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "    @observe(as_type=\"generation\", name=\"Gemini Generation\")",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "    ):",
          "        message_dicts, tool_dicts, system_prompt = self._prep_message_and_tools(",
          "            messages=messages,",
          "            prompt=prompt,",
          "            system_prompt=system_prompt,",
          "            tools=tools,",
          "        )",
          "",
          "        client = self.get_client()",
          "        response = client.models.generate_content(",
          "            model=self.model_name,",
          "            contents=message_dicts,",
          "            config=GenerateContentConfig(",
          "                tools=tool_dicts,",
          "                system_instruction=system_prompt,",
          "                temperature=temperature or 0.0,",
          "                max_output_tokens=max_tokens or 8192,",
          "            ),",
          "        )",
          "",
          "        message = self._format_gemini_response_to_message(response)",
          "",
          "        usage = Usage(",
          "            completion_tokens=response.usage_metadata.candidates_token_count or 0,",
          "            prompt_tokens=response.usage_metadata.prompt_token_count or 0,",
          "            total_tokens=response.usage_metadata.total_token_count or 0,",
          "        )",
          "",
          "        langfuse_context.update_current_observation(model=self.model_name, usage=usage)",
          "",
          "        return LlmGenerateTextResponse(",
          "            message=message,",
          "            metadata=LlmResponseMetadata(",
          "                model=self.model_name,",
          "                provider_name=self.provider_name,",
          "                usage=usage,",
          "            ),",
          "        )",
          "",
          "    @classmethod",
          "    def _prep_message_and_tools(",
          "        cls,",
          "        *,",
          "        messages: list[Message] | None = None,",
          "        prompt: str | None = None,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool] | None = None,",
          "    ) -> tuple[list[Content], list[GeminiTool] | None, str | None]:",
          "        contents: list[Content] = []",
          "",
          "        if messages:",
          "            # Group consecutive tool messages together",
          "            grouped_messages: list[list[Message]] = []",
          "            current_group: list[Message] = []",
          "",
          "            for message in messages:",
          "                if message.role == \"tool\":",
          "                    current_group.append(message)",
          "                else:",
          "                    if current_group:",
          "                        grouped_messages.append(current_group)",
          "                        current_group = []",
          "                    grouped_messages.append([message])",
          "",
          "            if current_group:",
          "                grouped_messages.append(current_group)",
          "",
          "            # Convert each group into a Content object",
          "            for group in grouped_messages:",
          "                if len(group) == 1 and group[0].role != \"tool\":",
          "                    contents.append(cls.to_content(group[0]))",
          "                elif group[0].role == \"tool\":",
          "                    # Combine multiple tool messages into a single Content",
          "                    parts = [",
          "                        Part.from_function_response(",
          "                            name=msg.tool_call_function or \"\",",
          "                            response={\"response\": msg.content},",
          "                        )",
          "                        for msg in group",
          "                    ]",
          "                    contents.append(Content(role=\"user\", parts=parts))",
          "",
          "        if prompt:",
          "            contents.append(",
          "                Content(",
          "                    role=\"user\",",
          "                    parts=[Part(text=prompt)],",
          "                )",
          "            )",
          "",
          "        processed_tools = [cls.to_tool(tool) for tool in tools] if tools else []",
          "",
          "        return contents, processed_tools, system_prompt",
          "",
          "    @staticmethod",
          "    def to_content(message: Message) -> Content:",
          "        if message.role == \"tool_use\" or (message.role == \"assistant\" and message.tool_calls):",
          "            if not message.tool_calls:",
          "                return Content(",
          "                    role=\"model\",",
          "                    parts=[Part(text=message.content or \"\")],",
          "                )",
          "",
          "            parts = []",
          "            if message.content:",
          "                parts.append(Part(text=message.content))",
          "            for tool_call in message.tool_calls:",
          "                parts.append(",
          "                    Part.from_function_call(",
          "                        name=tool_call.function,",
          "                        args=json.loads(tool_call.args),",
          "                    )",
          "                )",
          "            return Content(role=\"model\", parts=parts)",
          "",
          "        elif message.role == \"assistant\":",
          "            return Content(",
          "                role=\"model\",",
          "                parts=[Part(text=message.content or \"\")],",
          "            )",
          "        else:",
          "            return Content(",
          "                role=\"user\",",
          "                parts=[Part(text=message.content or \"\")],",
          "            )",
          "",
          "    @staticmethod",
          "    def to_tool(tool: FunctionTool) -> GeminiTool:",
          "        return GeminiTool(",
          "            function_declarations=[",
          "                FunctionDeclaration(",
          "                    name=tool.name,",
          "                    description=tool.description,",
          "                    parameters={",
          "                        \"type\": \"OBJECT\",",
          "                        \"properties\": {",
          "                            param[\"name\"]: {",
          "                                key: value",
          "                                for key, value in {",
          "                                    \"type\": param[\"type\"].upper(),  # type: ignore",
          "                                    \"description\": param.get(\"description\", \"\"),",
          "                                    \"items\": (",
          "                                        {",
          "                                            **param.get(\"items\", {}),  # type: ignore",
          "                                            \"type\": param.get(\"items\", {}).get(\"type\", \"\").upper(),  # type: ignore",
          "                                        }",
          "                                        if param.get(\"items\") and \"type\" in param.get(\"items\", {})",
          "                                        else param.get(\"items\")",
          "                                    ),",
          "                                }.items()",
          "                                if value is not None",
          "                            }",
          "                            for param in tool.parameters",
          "                        },",
          "                        \"required\": tool.required,",
          "                    },",
          "                )",
          "            ],",
          "        )",
          "",
          "    def construct_message_from_stream(",
          "        self, content_chunks: list[str], tool_calls: list[ToolCall]",
          "    ) -> Message:",
          "        message = Message(",
          "            role=\"tool_use\" if tool_calls else \"assistant\",",
          "            content=\"\".join(content_chunks) if content_chunks else None,",
          "        )",
          "",
          "        if tool_calls:",
          "            message.tool_calls = tool_calls",
          "            message.tool_call_id = tool_calls[0].id",
          "",
          "        return message",
          "",
          "    def _format_gemini_response_to_message(self, response: GenerateContentResponse) -> Message:",
          "        parts = (",
          "            response.candidates[0].content.parts",
          "            if (",
          "                response.candidates",
          "                and len(response.candidates) > 0",
          "                and response.candidates[0].content",
          "                and response.candidates[0].content.parts",
          "            )",
          "            else []",
          "        )",
          "",
          "        message = Message(",
          "            role=\"assistant\",",
          "            content=(parts[0].text if parts and parts[0].text else None),",
          "        )",
          "",
          "        for part in parts:",
          "            if part.function_call:",
          "                if not message.tool_calls:",
          "                    message.tool_calls = []",
          "                message.tool_calls.append(",
          "                    ToolCall(",
          "                        id=part.function_call.id,",
          "                        function=part.function_call.name or \"\",",
          "                        args=json.dumps(part.function_call.args),",
          "                    )",
          "                )",
          "                message.role = \"tool_use\"",
          "                message.tool_call_id = part.function_call.id",
          "            if part.text:",
          "                message.content = part.text",
          "",
          "        return message",
          "",
          "",
          "LlmProvider = Union[OpenAiProvider, AnthropicProvider, GeminiProvider]",
          "",
          "",
          "class LlmClient:",
          "    @observe(name=\"Generate Text\")",
          "    def generate_text(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        model: LlmProvider,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        run_name: str | None = None,",
          "        timeout: float | None = None,",
          "        predicted_output: str | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> LlmGenerateTextResponse:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(name=run_name + \" - Generate Text\")",
          "",
          "            defaults = model.defaults",
          "            default_temperature = defaults.temperature if defaults else None",
          "",
          "            messages = LlmClient.clean_message_content(messages if messages else [])",
          "            if not tools:",
          "                messages = LlmClient.clean_tool_call_assistant_messages(messages)",
          "",
          "            if model.provider_name == LlmProviderType.OPENAI:",
          "                model = cast(OpenAiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for OpenAI\")",
          "",
          "                return model.generate_text(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                    timeout=timeout,",
          "                    predicted_output=predicted_output,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "                model = cast(AnthropicProvider, model)",
          "                return model.generate_text(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=tools,",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for Gemini\")",
          "",
          "                return model.generate_text(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                )",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "        except Exception as e:",
          "            logger.exception(f\"Text generation failed with provider {model.provider_name}: {e}\")",
          "            raise e",
          "",
          "    @observe(name=\"Generate Structured\")",
          "    def generate_structured(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        model: LlmProvider,",
          "        system_prompt: str | None = None,",
          "        response_format: Type[StructuredOutputType],",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        run_name: str | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "    ) -> LlmGenerateStructuredResponse[StructuredOutputType]:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(",
          "                    name=run_name + \" - Generate Structured\"",
          "                )",
          "",
          "            messages = LlmClient.clean_message_content(messages if messages else [])",
          "",
          "            if model.provider_name == LlmProviderType.OPENAI:",
          "                model = cast(OpenAiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for OpenAI\")",
          "",
          "                messages = LlmClient.clean_tool_call_assistant_messages(messages)",
          "                return model.generate_structured(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    response_format=response_format,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "                raise NotImplementedError(\"Anthropic structured outputs are not yet supported\")",
          "            elif model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for Gemini\")",
          "                return model.generate_structured(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    response_format=response_format,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                )",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "        except Exception as e:",
          "            logger.exception(f\"Text generation failed with provider {model.provider_name}: {e}\")",
          "            raise e",
          "",
          "    @observe(name=\"Generate Text Stream\")",
          "    def generate_text_stream(",
          "        self,",
          "        *,",
          "        prompt: str | None = None,",
          "        messages: list[Message] | None = None,",
          "        model: LlmProvider,",
          "        system_prompt: str | None = None,",
          "        tools: list[FunctionTool | ClaudeTool] | None = None,",
          "        temperature: float | None = None,",
          "        max_tokens: int | None = None,",
          "        run_name: str | None = None,",
          "        timeout: float | None = None,",
          "        reasoning_effort: str | None = None,",
          "        first_token_timeout: float = 40.0,  # Time to first token timeout",
          "        inactivity_timeout: float = 20.0,  # Timeout for inactivity after first token",
          "    ) -> Iterator[Tuple[str, str] | ToolCall | Usage]:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(",
          "                    name=run_name + \" - Generate Text Stream\"",
          "                )",
          "",
          "            defaults = model.defaults",
          "            default_temperature = defaults.temperature if defaults else None",
          "",
          "            messages = LlmClient.clean_message_content(messages if messages else [])",
          "            if not tools:",
          "                messages = LlmClient.clean_tool_call_assistant_messages(messages)",
          "",
          "            # Get the appropriate stream generator based on provider",
          "            if model.provider_name == LlmProviderType.OPENAI:",
          "                model = cast(OpenAiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for OpenAI\")",
          "",
          "                stream_generator = model.generate_text_stream(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "                model = cast(AnthropicProvider, model)",
          "                stream_generator = model.generate_text_stream(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=tools,",
          "                    timeout=timeout,",
          "                    reasoning_effort=reasoning_effort,",
          "                )",
          "            elif model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "",
          "                if tools and any(isinstance(tool, ClaudeTool) for tool in tools):",
          "                    raise ValueError(\"Claude tools are not supported for Gemini\")",
          "",
          "                stream_generator = model.generate_text_stream(",
          "                    max_tokens=max_tokens,",
          "                    messages=messages,",
          "                    prompt=prompt,",
          "                    system_prompt=system_prompt,",
          "                    temperature=temperature or default_temperature,",
          "                    tools=cast(list[FunctionTool], tools),",
          "                )",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "",
          "            # Add timeout check to the stream with differentiated timeouts",
          "            # for first token and subsequent tokens",
          "            last_yield_time = time.time()",
          "            first_token_received = False",
          "",
          "            for item in stream_generator:",
          "                current_time = time.time()",
          "                # Use first_token_timeout for the first token, inactivity_timeout for subsequent tokens",
          "                timeout_to_use = (",
          "                    first_token_timeout if not first_token_received else inactivity_timeout",
          "                )",
          "",
          "                if current_time - last_yield_time > timeout_to_use:",
          "                    if first_token_received:",
          "                        raise LlmStreamInactivityTimeoutError(",
          "                            f\"Stream inactivity timeout after {timeout_to_use} seconds\"",
          "                        )",
          "                    else:",
          "                        raise LlmStreamFirstTokenTimeoutError(",
          "                            f\"Stream time to first token timeout after {timeout_to_use} seconds\"",
          "                        )",
          "",
          "                # Mark that we've received at least one token",
          "                first_token_received = True",
          "                last_yield_time = current_time",
          "                yield item",
          "",
          "        except Exception as e:",
          "            logger.exception(",
          "                f\"Text stream generation failed with provider {model.provider_name}: {e}\"",
          "            )",
          "            raise e",
          "",
          "    @observe(name=\"Generate Text from Web Search\")",
          "    def generate_text_from_web_search(",
          "        self,",
          "        *,",
          "        prompt: str,",
          "        model: LlmProvider,",
          "        temperature: float | None = None,",
          "        run_name: str | None = None,",
          "    ) -> str:",
          "        try:",
          "            if run_name:",
          "                langfuse_context.update_current_observation(name=run_name + \" - Generate Text\")",
          "",
          "            defaults = model.defaults",
          "            default_temperature = defaults.temperature if defaults else None",
          "",
          "            if model.provider_name == LlmProviderType.GEMINI:",
          "                model = cast(GeminiProvider, model)",
          "                return model.search_the_web(prompt, temperature or default_temperature)",
          "            else:",
          "                raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "        except Exception as e:",
          "            logger.exception(",
          "                f\"Text generation from web failed with provider {model.provider_name}: {e}\"",
          "            )",
          "            raise e",
          "",
          "    @staticmethod",
          "    def clean_tool_call_assistant_messages(messages: list[Message]) -> list[Message]:",
          "        new_messages = []",
          "        for message in messages:",
          "            if message.role == \"assistant\" and message.tool_calls:",
          "                new_messages.append(",
          "                    Message(role=\"assistant\", content=message.content, tool_calls=[])",
          "                )",
          "            elif message.role == \"tool\":",
          "                new_messages.append(Message(role=\"user\", content=message.content, tool_calls=[]))",
          "            elif message.role == \"tool_use\":",
          "                new_messages.append(",
          "                    Message(role=\"assistant\", content=message.content, tool_calls=[])",
          "                )",
          "            else:",
          "                new_messages.append(message)",
          "        return new_messages",
          "",
          "    @staticmethod",
          "    def clean_assistant_messages(messages: list[Message]) -> list[Message]:",
          "        new_messages = []",
          "        for message in messages:",
          "            if message.role == \"assistant\" or message.role == \"tool_use\":",
          "                message.content = \".\"",
          "                new_messages.append(message)",
          "            else:",
          "                new_messages.append(message)",
          "        return new_messages",
          "",
          "    @staticmethod",
          "    def clean_message_content(messages: list[Message]) -> list[Message]:",
          "        new_messages = []",
          "        for message in messages:",
          "            if not message.content:",
          "                message.content = \".\"",
          "            new_messages.append(message)",
          "        return new_messages",
          "",
          "    def construct_message_from_stream(",
          "        self,",
          "        content_chunks: list[str],",
          "        tool_calls: list[ToolCall],",
          "        model: LlmProvider,",
          "        thinking_content_chunks: list[str] = [],",
          "        thinking_signature: str | None = None,",
          "    ) -> Message:",
          "        if model.provider_name == LlmProviderType.OPENAI:",
          "            model = cast(OpenAiProvider, model)",
          "            return model.construct_message_from_stream(content_chunks, tool_calls)",
          "        elif model.provider_name == LlmProviderType.ANTHROPIC:",
          "            model = cast(AnthropicProvider, model)",
          "            return model.construct_message_from_stream(",
          "                content_chunks, tool_calls, thinking_content_chunks, thinking_signature",
          "            )",
          "        elif model.provider_name == LlmProviderType.GEMINI:",
          "            model = cast(GeminiProvider, model)",
          "            return model.construct_message_from_stream(content_chunks, tool_calls)",
          "        else:",
          "            raise ValueError(f\"Invalid provider: {model.provider_name}\")",
          "",
          "",
          "@module.provider",
          "def provide_llm_client() -> LlmClient:",
          "    return LlmClient()",
          ""
        ]
      }
    },
    {
      "file": "seer/utils.py",
      "image": "seer.utils",
      "is_application": true,
      "line": 110,
      "name": "backoff_on_exception.<locals>.decorator.<locals>.wrapped_func",
      "path": "/app/src/seer/utils.py",
      "codeContext": {
        "file": "seer/utils.py",
        "line": 110,
        "name": "backoff_on_exception.<locals>.decorator.<locals>.wrapped_func",
        "code": "    if sleep_sec_scaler is None:\n        sleep_sec_scaler = lambda num_tries: min(2**num_tries, 10.0)\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapped_func(*args, **kwargs):\n            last_exception = None\n            for num_tries in range(1, max_tries + 1):\n                try:\n                    result = func(*args, **kwargs)\n                except Exception as exception:\n                    last_exception = exception\n                    if is_exception_retryable(exception):\n                        sleep_sec = sleep_sec_scaler(num_tries) + jitterer()\n                        logger.info(\n                            f\"Encountered {exception_formatter(exception)}. Sleeping for \"\n                            f\"{sleep_sec} seconds before try {num_tries + 1}/{max_tries}.\"\n                        )\n                        time.sleep(sleep_sec)\n                    else:",
        "lineRange": {
          "start": 101,
          "end": 120
        },
        "lines": [
          "import contextlib",
          "import functools",
          "import json",
          "import logging",
          "import random",
          "import time",
          "import weakref",
          "from enum import Enum",
          "from queue import Empty, Full, Queue",
          "from typing import Callable, Iterable, Sequence, TypeVar",
          "",
          "from sqlalchemy.orm import DeclarativeBase, Session",
          "from tqdm.auto import tqdm",
          "",
          "logger = logging.getLogger(__name__)",
          "",
          "",
          "def class_method_lru_cache(*lru_args, **lru_kwargs):",
          "    def decorator(func):",
          "        @functools.wraps(func)",
          "        def wrapped_func(self, *args, **kwargs):",
          "            # We're storing the wrapped method inside the instance. If we had",
          "            # a strong reference to self the instance would never die.",
          "            self_weak = weakref.ref(self)",
          "",
          "            @functools.wraps(func)",
          "            @functools.lru_cache(*lru_args, **lru_kwargs)",
          "            def cached_method(*args, **kwargs):",
          "                return func(self_weak(), *args, **kwargs)",
          "",
          "            setattr(self, func.__name__, cached_method)",
          "            return cached_method(*args, **kwargs)",
          "",
          "        return wrapped_func",
          "",
          "    return decorator",
          "",
          "",
          "class SeerJSONEncoder(json.JSONEncoder):",
          "    def default(self, obj):",
          "        if isinstance(obj, Enum):",
          "            return obj.value",
          "        return super().default(obj)",
          "",
          "",
          "def json_dumps(data, **kwargs) -> str:",
          "    return json.dumps(data, cls=SeerJSONEncoder, **kwargs)",
          "",
          "",
          "def batch_save_to_db(session: Session, data: Sequence[DeclarativeBase], batch_size: int = 512):",
          "    \"\"\"",
          "    Save a list of data to the database in batches. Flushes the session after each batch.",
          "    NOTE: Needs to be called inside a session/transaction.",
          "    \"\"\"",
          "    for i in range(0, len(data), batch_size):",
          "        session.bulk_save_objects(data[i : i + batch_size])",
          "",
          "        # Flush to move the data to the db transaction buffer",
          "        session.flush()",
          "",
          "",
          "@contextlib.contextmanager",
          "def closing_queue(*queues: Queue):",
          "    try:",
          "        yield",
          "    finally:",
          "        for queue in queues:",
          "            try:",
          "                queue.put_nowait(None)",
          "            except Full:",
          "                pass",
          "",
          "            try:",
          "                queue.get_nowait()",
          "            except Empty:",
          "                pass",
          "",
          "",
          "def exception_formatter(exception: Exception) -> str:",
          "    return f\"{type(exception).__module__}.{type(exception).__qualname__}: {exception}\"",
          "",
          "",
          "class MaxTriesExceeded(Exception):",
          "    pass",
          "",
          "",
          "def backoff_on_exception(",
          "    is_exception_retryable: Callable[[Exception], bool],",
          "    max_tries: int = 2,",
          "    sleep_sec_scaler: Callable[[int], float] | None = None,",
          "    jitterer: Callable[[], float] = lambda: random.uniform(0, 0.5),",
          "):",
          "    \"\"\"",
          "    Returns a decorator which retries a function on exception iff `is_exception_retryable(exception)`.",
          "    Defaults to exponential backoff with random jitter and one retry.",
          "    \"\"\"",
          "",
          "    if max_tries < 1:",
          "        raise ValueError(\"max_tries must be at least 1\")  # pragma: no cover",
          "",
          "    if sleep_sec_scaler is None:",
          "        sleep_sec_scaler = lambda num_tries: min(2**num_tries, 10.0)",
          "",
          "    def decorator(func):",
          "        @functools.wraps(func)",
          "        def wrapped_func(*args, **kwargs):",
          "            last_exception = None",
          "            for num_tries in range(1, max_tries + 1):",
          "                try:",
          "                    result = func(*args, **kwargs)",
          "                except Exception as exception:",
          "                    last_exception = exception",
          "                    if is_exception_retryable(exception):",
          "                        sleep_sec = sleep_sec_scaler(num_tries) + jitterer()",
          "                        logger.info(",
          "                            f\"Encountered {exception_formatter(exception)}. Sleeping for \"",
          "                            f\"{sleep_sec} seconds before try {num_tries + 1}/{max_tries}.\"",
          "                        )",
          "                        time.sleep(sleep_sec)",
          "                    else:",
          "                        raise exception",
          "                else:",
          "                    if num_tries > 1:",
          "                        logger.info(f\"Retried call successful after {num_tries} tries.\")",
          "                    return result",
          "",
          "            raise MaxTriesExceeded(",
          "                f\"Max tries ({max_tries}) exceeded. \"",
          "                f\"Last exception: {exception_formatter(last_exception)}\"",
          "            ) from last_exception",
          "",
          "        return wrapped_func",
          "",
          "    return decorator",
          "",
          "",
          "def prefix_logger(prefix: str, logger: logging.Logger) -> logging.LoggerAdapter:",
          "    \"\"\"",
          "    Returns a logger that prefixes all messages with `prefix`.",
          "    \"\"\"",
          "",
          "    class PrefixedLoggingAdapter(logging.LoggerAdapter):",
          "        def process(self, msg, kwargs):",
          "            return f\"{prefix}{msg}\", kwargs",
          "",
          "    return PrefixedLoggingAdapter(logger)",
          "",
          "",
          "_Batch = TypeVar(\"_Batch\")",
          "",
          "",
          "def tqdm_sized(",
          "    iterable: Iterable[_Batch],",
          "    length: Callable[[_Batch], int] = len,  # type: ignore[assignment]",
          "    **kwargs,",
          "):",
          "    with tqdm(**kwargs) as pbar:",
          "        for batch in iterable:",
          "            pbar.update(length(batch))",
          "            yield batch",
          ""
        ]
      }
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/genai/models.py",
      "image": "google.genai.models",
      "is_application": false,
      "line": 4672,
      "name": "Models.generate_content",
      "path": "/usr/local/lib/python3.11/dist-packages/google/genai/models.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/genai/models.py",
      "image": "google.genai.models",
      "is_application": false,
      "line": 3831,
      "name": "Models._generate_content",
      "path": "/usr/local/lib/python3.11/dist-packages/google/genai/models.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py",
      "image": "google.genai._api_client",
      "is_application": false,
      "line": 455,
      "name": "ApiClient.request",
      "path": "/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py",
      "image": "google.genai._api_client",
      "is_application": false,
      "line": 373,
      "name": "ApiClient._request",
      "path": "/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py",
      "image": "google.auth._default",
      "is_application": false,
      "line": 685,
      "name": "default",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py",
      "image": "google.auth._default",
      "is_application": false,
      "line": 681,
      "name": "default.<locals>.<lambda>",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py",
      "image": "google.auth._default",
      "is_application": false,
      "line": 350,
      "name": "_get_gce_credentials",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py",
      "image": "google.auth.compute_engine._metadata",
      "is_application": false,
      "line": 78,
      "name": "is_on_gce",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py",
      "image": "google.auth.compute_engine._metadata",
      "is_application": false,
      "line": 131,
      "name": "ping",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/transport/_http_client.py",
      "image": "google.auth.transport._http_client",
      "is_application": false,
      "line": 104,
      "name": "Request.__call__",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/transport/_http_client.py"
    },
    {
      "file": "http/client.py",
      "image": "http.client",
      "is_application": false,
      "line": 941,
      "name": "HTTPConnection.connect",
      "path": "/usr/lib/python3.11/http/client.py"
    },
    {
      "file": "socket.py",
      "image": "socket",
      "is_application": false,
      "line": 830,
      "name": "create_connection",
      "path": "/usr/lib/python3.11/socket.py"
    },
    {
      "file": "socket.py",
      "image": "socket",
      "is_application": false,
      "line": 232,
      "name": "socket.__init__",
      "path": "/usr/lib/python3.11/socket.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py",
      "image": "google.auth.compute_engine._metadata",
      "is_application": false,
      "line": 286,
      "name": "get_project_id",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py",
      "image": "google.auth.compute_engine._metadata",
      "is_application": false,
      "line": 207,
      "name": "get",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py",
      "image": "google.auth.transport.requests",
      "is_application": false,
      "line": 537,
      "name": "AuthorizedSession.request",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py",
      "image": "google.auth.credentials",
      "is_application": false,
      "line": 239,
      "name": "Credentials.before_request",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py",
      "image": "google.auth.credentials",
      "is_application": false,
      "line": 202,
      "name": "Credentials._blocking_refresh",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/credentials.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/credentials.py",
      "image": "google.auth.compute_engine.credentials",
      "is_application": false,
      "line": 127,
      "name": "Credentials.refresh",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/credentials.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py",
      "image": "google.auth.compute_engine._metadata",
      "is_application": false,
      "line": 371,
      "name": "get_service_account_token",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py",
      "image": "google.auth.transport.requests",
      "is_application": false,
      "line": 186,
      "name": "Request.__call__",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/credentials.py",
      "image": "google.auth.compute_engine.credentials",
      "is_application": false,
      "line": 99,
      "name": "Credentials._retrieve_info",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/credentials.py"
    },
    {
      "file": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py",
      "image": "google.auth.compute_engine._metadata",
      "is_application": false,
      "line": 338,
      "name": "get_service_account_info",
      "path": "/usr/local/lib/python3.11/dist-packages/google/auth/compute_engine/_metadata.py"
    },
    {
      "file": "__init__.py",
      "image": "json",
      "is_application": false,
      "line": 346,
      "name": "loads",
      "path": "/usr/lib/python3.11/json/__init__.py"
    },
    {
      "file": "json/decoder.py",
      "image": "json.decoder",
      "is_application": false,
      "line": 337,
      "name": "JSONDecoder.decode",
      "path": "/usr/lib/python3.11/json/decoder.py"
    },
    {
      "file": "json/decoder.py",
      "image": "json.decoder",
      "is_application": false,
      "line": 353,
      "name": "JSONDecoder.raw_decode",
      "path": "/usr/lib/python3.11/json/decoder.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1849,
      "name": "LoggerAdapter.info",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1887,
      "name": "LoggerAdapter.log",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1559,
      "name": "Logger.log",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1634,
      "name": "Logger._log",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1644,
      "name": "Logger.handle",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "sentry_sdk/integrations/logging.py",
      "image": "sentry_sdk.integrations.logging",
      "is_application": false,
      "line": 98,
      "name": "LoggingIntegration.setup_once.<locals>.sentry_patched_callhandlers",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/integrations/logging.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1706,
      "name": "Logger.callHandlers",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 978,
      "name": "Handler.handle",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "seer/logging.py",
      "image": "seer.logging",
      "is_application": true,
      "line": 75,
      "name": "StructLogHandler.emit",
      "path": "/app/src/seer/logging.py",
      "codeContext": {
        "file": "seer/logging.py",
        "line": 75,
        "name": "StructLogHandler.emit",
        "code": "    def emit(self, record, logger=None):\n        # If anyone wants to use the 'extra' kwarg to provide context within\n        # structlog, we have to strip all of the default attributes from\n        # a record because the RootLogger will take the 'extra' dictionary\n        # and just turn them into attributes.\n        try:\n            if logger is None:\n                logger = get_logger()\n\n            logger.log(**self.get_log_kwargs(record=record, logger=logger))\n        except Exception:\n            if logging.raiseExceptions:\n                raise\n\n\nclass GunicornHealthCheckFilterLogger(GunicornBaseLogger):\n    def access(self, resp, req, environ, request_time):\n        if \"/health\" not in req.path:\n            super().access(resp, req, environ, request_time)\n",
        "lineRange": {
          "start": 66,
          "end": 85
        },
        "lines": [
          "import logging",
          "import sys",
          "from typing import Annotated",
          "",
          "import structlog",
          "from gunicorn.glogging import Logger as GunicornBaseLogger  # type: ignore[import-untyped]",
          "from structlog import get_logger",
          "",
          "from seer.dependency_injection import Labeled, Module, inject, injected",
          "",
          "DefaultLoggingHandlers = Annotated[list[logging.Handler], Labeled(\"default\")]",
          "LogLevel = Annotated[int, Labeled(\"log_level\")]",
          "LoggingPrefixes = Annotated[list[str], Labeled(\"logging_prefixes\")]",
          "",
          "logging_module = Module()",
          "",
          "logging_module.constant(LogLevel, logging.INFO)",
          "logging_module.constant(LoggingPrefixes, [\"seer.\", \"celery_app.\"])",
          "",
          "",
          "@logging_module.provider",
          "def default_handlers() -> DefaultLoggingHandlers:",
          "    return [StructLogHandler(sys.stdout)]",
          "",
          "",
          "@inject",
          "def initialize_logs(",
          "    prefixes: list[str],",
          "    handlers: DefaultLoggingHandlers = injected,",
          "    log_level: LogLevel = injected,",
          "):",
          "    for log_name in logging.root.manager.loggerDict:",
          "        if any(log_name.startswith(prefix) for prefix in prefixes):",
          "            logger = logging.getLogger(log_name)",
          "            logger.setLevel(log_level)",
          "            for handler in handlers:",
          "                logger.addHandler(handler)",
          "",
          "    structlog.configure(",
          "        processors=[",
          "            structlog.stdlib.add_log_level,",
          "            structlog.processors.format_exc_info,",
          "            structlog.processors.JSONRenderer(),",
          "        ]",
          "    )",
          "",
          "",
          "class StructLogHandler(logging.StreamHandler):",
          "    def get_log_kwargs(self, record, logger):",
          "        kwargs = {k: v for k, v in vars(record).items() if k not in throwaways and v is not None}",
          "        kwargs.update({\"level\": record.levelno, \"event\": record.msg})",
          "",
          "        if record.args:",
          "            # record.args inside of LogRecord.__init__ gets unrolled",
          "            # if it's the shape `({},)`, a single item dictionary.",
          "            # so we need to check for this, and re-wrap it because",
          "            # down the line of structlog, it's expected to be this",
          "            # original shape.",
          "            if isinstance(record.args, (tuple, list)):",
          "                kwargs[\"positional_args\"] = record.args",
          "            else:",
          "                kwargs[\"positional_args\"] = (record.args,)",
          "",
          "        return kwargs",
          "",
          "    def emit(self, record, logger=None):",
          "        # If anyone wants to use the 'extra' kwarg to provide context within",
          "        # structlog, we have to strip all of the default attributes from",
          "        # a record because the RootLogger will take the 'extra' dictionary",
          "        # and just turn them into attributes.",
          "        try:",
          "            if logger is None:",
          "                logger = get_logger()",
          "",
          "            logger.log(**self.get_log_kwargs(record=record, logger=logger))",
          "        except Exception:",
          "            if logging.raiseExceptions:",
          "                raise",
          "",
          "",
          "class GunicornHealthCheckFilterLogger(GunicornBaseLogger):",
          "    def access(self, resp, req, environ, request_time):",
          "        if \"/health\" not in req.path:",
          "            super().access(resp, req, environ, request_time)",
          "",
          "",
          "throwaways = frozenset(",
          "    (",
          "        \"threadName\",",
          "        \"thread\",",
          "        \"created\",",
          "        \"process\",",
          "        \"processName\",",
          "        \"args\",",
          "        \"module\",",
          "        \"filename\",",
          "        \"levelno\",",
          "        \"exc_text\",",
          "        \"msg\",",
          "        \"pathname\",",
          "        \"lineno\",",
          "        \"funcName\",",
          "        \"relativeCreated\",",
          "        \"levelname\",",
          "        \"msecs\",",
          "    )",
          ")",
          "",
          "logging_module.enable()",
          ""
        ]
      }
    },
    {
      "file": "structlog/_native.py",
      "image": "structlog._native",
      "is_application": false,
      "line": 179,
      "name": "_make_filtering_bound_logger.<locals>.log",
      "path": "/usr/local/lib/python3.11/dist-packages/structlog/_native.py"
    },
    {
      "file": "structlog/_base.py",
      "image": "structlog._base",
      "is_application": false,
      "line": 223,
      "name": "BoundLoggerBase._proxy_to_logger",
      "path": "/usr/local/lib/python3.11/dist-packages/structlog/_base.py"
    },
    {
      "file": "structlog/_output.py",
      "image": "structlog._output",
      "is_application": false,
      "line": 110,
      "name": "PrintLogger.msg",
      "path": "/usr/local/lib/python3.11/dist-packages/structlog/_output.py"
    },
    {
      "file": "celery/utils/log.py",
      "image": "celery.utils.log",
      "is_application": false,
      "line": 232,
      "name": "LoggingProxy.write",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/utils/log.py"
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1113,
      "name": "StreamHandler.emit",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "langfuse/decorators/langfuse_decorator.py",
      "image": "langfuse.decorators.langfuse_decorator",
      "is_application": false,
      "line": 1106,
      "name": "LangfuseDecorator.client_instance",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/decorators/langfuse_decorator.py"
    },
    {
      "file": "langfuse/utils/langfuse_singleton.py",
      "image": "langfuse.utils.langfuse_singleton",
      "is_application": false,
      "line": 66,
      "name": "LangfuseSingleton.get",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/utils/langfuse_singleton.py"
    },
    {
      "file": "langfuse/client.py",
      "image": "langfuse.client",
      "is_application": false,
      "line": 290,
      "name": "Langfuse.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/client.py"
    },
    {
      "file": "httpx/_client.py",
      "image": "httpx._client",
      "is_application": false,
      "line": 695,
      "name": "Client.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_client.py"
    },
    {
      "file": "httpx/_client.py",
      "image": "httpx._client",
      "is_application": false,
      "line": 743,
      "name": "Client._init_transport",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_client.py"
    },
    {
      "file": "httpx/_transports/default.py",
      "image": "httpx._transports.default",
      "is_application": false,
      "line": 139,
      "name": "HTTPTransport.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py"
    },
    {
      "file": "httpx/_config.py",
      "image": "httpx._config",
      "is_application": false,
      "line": 55,
      "name": "create_ssl_context",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_config.py"
    },
    {
      "file": "httpx/_config.py",
      "image": "httpx._config",
      "is_application": false,
      "line": 79,
      "name": "SSLConfig.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_config.py"
    },
    {
      "file": "httpx/_config.py",
      "image": "httpx._config",
      "is_application": false,
      "line": 91,
      "name": "SSLConfig.load_ssl_context",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_config.py"
    },
    {
      "file": "httpx/_config.py",
      "image": "httpx._config",
      "is_application": false,
      "line": 149,
      "name": "SSLConfig.load_ssl_context_verify",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_config.py"
    },
    {
      "file": "langfuse/api/client.py",
      "image": "langfuse.api.client",
      "is_application": false,
      "line": 185,
      "name": "AsyncFernLangfuse.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/api/client.py"
    },
    {
      "file": "httpx/_client.py",
      "image": "httpx._client",
      "is_application": false,
      "line": 1442,
      "name": "AsyncClient.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_client.py"
    },
    {
      "file": "httpx/_client.py",
      "image": "httpx._client",
      "is_application": false,
      "line": 1490,
      "name": "AsyncClient._init_transport",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_client.py"
    },
    {
      "file": "httpx/_transports/default.py",
      "image": "httpx._transports.default",
      "is_application": false,
      "line": 280,
      "name": "AsyncHTTPTransport.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py"
    },
    {
      "file": "httpx/_config.py",
      "image": "httpx._config",
      "is_application": false,
      "line": 164,
      "name": "SSLConfig._create_default_ssl_context",
      "path": "/usr/local/lib/python3.11/dist-packages/httpx/_config.py"
    },
    {
      "file": "ssl.py",
      "image": "ssl",
      "is_application": false,
      "line": 500,
      "name": "SSLContext.__new__",
      "path": "/usr/lib/python3.11/ssl.py"
    },
    {
      "file": "langfuse/_task_manager/task_manager.py",
      "image": "langfuse._task_manager.task_manager",
      "is_application": false,
      "line": 81,
      "name": "TaskManager.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/_task_manager/task_manager.py"
    },
    {
      "file": "langfuse/_task_manager/task_manager.py",
      "image": "langfuse._task_manager.task_manager",
      "is_application": false,
      "line": 111,
      "name": "TaskManager.init_resources",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/_task_manager/task_manager.py"
    },
    {
      "file": "sentry_sdk/integrations/threading.py",
      "image": "sentry_sdk.integrations.threading",
      "is_application": false,
      "line": 80,
      "name": "ThreadingIntegration.setup_once.<locals>.sentry_start",
      "path": "/usr/local/lib/python3.11/dist-packages/sentry_sdk/integrations/threading.py"
    },
    {
      "file": "threading.py",
      "image": "threading",
      "is_application": false,
      "line": 962,
      "name": "Thread.start",
      "path": "/usr/lib/python3.11/threading.py"
    },
    {
      "file": "threading.py",
      "image": "threading",
      "is_application": false,
      "line": 622,
      "name": "Event.wait",
      "path": "/usr/lib/python3.11/threading.py"
    },
    {
      "file": "threading.py",
      "image": "threading",
      "is_application": false,
      "line": 320,
      "name": "Condition.wait",
      "path": "/usr/lib/python3.11/threading.py"
    },
    {
      "file": "langfuse/prompt_cache.py",
      "image": "langfuse.prompt_cache",
      "is_application": false,
      "line": 143,
      "name": "PromptCache.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/prompt_cache.py"
    },
    {
      "file": "langfuse/prompt_cache.py",
      "image": "langfuse.prompt_cache",
      "is_application": false,
      "line": 81,
      "name": "PromptCacheTaskManager.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/langfuse/prompt_cache.py"
    },
    {
      "file": "seer/automation/codegen/step.py",
      "image": "seer.automation.codegen.step",
      "is_application": true,
      "line": 56,
      "name": "CodegenStep._post_invoke",
      "path": "/app/src/seer/automation/codegen/step.py",
      "codeContext": {
        "file": "seer/automation/codegen/step.py",
        "line": 56,
        "name": "CodegenStep._post_invoke",
        "code": "                \"langfuse_metadata\": metadata,\n                \"langfuse_session_id\": str(current_state.run_id),\n                \"sentry_tags\": tags,\n                \"sentry_data\": metadata,\n            }\n        except Exception:\n            return {}\n\n    def _post_invoke(self, result: Any):\n        with self.context.state.update() as current_state:\n            signal = make_done_signal(self.request.step_id)\n            current_state.signals.append(signal)\n\n    def _handle_exception(self, exception: Exception):\n        self.logger.error(f\"Failed to run {self.request.step_id}. Error: {str(exception)}\")\n\n        with self.context.state.update() as current_state:\n            current_state.status = CodegenStatus.ERRORED\n            sentry_sdk.set_context(\"codegen_state\", current_state.dict())\n            sentry_sdk.capture_exception(exception)",
        "lineRange": {
          "start": 47,
          "end": 66
        },
        "lines": [
          "from typing import Any",
          "",
          "import sentry_sdk",
          "",
          "from seer.automation.codegen.codegen_context import CodegenContext",
          "from seer.automation.codegen.models import CodegenStatus",
          "from seer.automation.pipeline import PipelineContext, PipelineStep, PipelineStepTaskRequest",
          "from seer.automation.state import DbStateRunTypes",
          "from seer.automation.utils import make_done_signal",
          "",
          "",
          "class CodegenStep(PipelineStep):",
          "    context: CodegenContext",
          "",
          "    @staticmethod",
          "    def _instantiate_context(",
          "        request: PipelineStepTaskRequest, type: DbStateRunTypes | None = None",
          "    ) -> PipelineContext:",
          "        if type is None:",
          "            type = DbStateRunTypes.UNIT_TEST",
          "        return CodegenContext.from_run_id(request.run_id, type=type)",
          "",
          "    def _invoke(self, **kwargs: Any) -> Any:",
          "        sentry_sdk.set_tag(\"run_id\", self.context.run_id)",
          "        super()._invoke(**kwargs)",
          "",
          "    def _pre_invoke(self) -> bool:",
          "        done_signal = make_done_signal(self.request.step_id)",
          "        return done_signal not in self.context.signals",
          "",
          "    def _get_extra_invoke_kwargs(self) -> dict[str, Any]:",
          "        try:",
          "            current_state = self.context.state.get()",
          "            repo = self.context.repo",
          "",
          "            tags = {",
          "                \"run_id\": current_state.run_id,",
          "                \"repo\": repo.full_name,",
          "                \"repo_id\": repo.external_id,",
          "            }",
          "",
          "            metadata = {\"run_id\": current_state.run_id, \"repo\": repo}",
          "            langfuse_tags = [f\"{key}:{value}\" for key, value in tags.items() if value is not None]",
          "",
          "            return {",
          "                \"langfuse_tags\": langfuse_tags,",
          "                \"langfuse_metadata\": metadata,",
          "                \"langfuse_session_id\": str(current_state.run_id),",
          "                \"sentry_tags\": tags,",
          "                \"sentry_data\": metadata,",
          "            }",
          "        except Exception:",
          "            return {}",
          "",
          "    def _post_invoke(self, result: Any):",
          "        with self.context.state.update() as current_state:",
          "            signal = make_done_signal(self.request.step_id)",
          "            current_state.signals.append(signal)",
          "",
          "    def _handle_exception(self, exception: Exception):",
          "        self.logger.error(f\"Failed to run {self.request.step_id}. Error: {str(exception)}\")",
          "",
          "        with self.context.state.update() as current_state:",
          "            current_state.status = CodegenStatus.ERRORED",
          "            sentry_sdk.set_context(\"codegen_state\", current_state.dict())",
          "            sentry_sdk.capture_exception(exception)",
          ""
        ]
      }
    },
    {
      "file": "celery/app/base.py",
      "image": "celery.app.base",
      "is_application": false,
      "line": 862,
      "name": "Celery.connection",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/app/base.py"
    },
    {
      "file": "celery/app/base.py",
      "image": "celery.app.base",
      "is_application": false,
      "line": 827,
      "name": "Celery.connection_for_write",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/app/base.py"
    },
    {
      "file": "celery/app/base.py",
      "image": "celery.app.base",
      "is_application": false,
      "line": 878,
      "name": "Celery._connection",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/app/base.py"
    },
    {
      "file": "kombu/connection.py",
      "image": "kombu.connection",
      "is_application": false,
      "line": 211,
      "name": "Connection.__init__",
      "path": "/usr/local/lib/python3.11/dist-packages/kombu/connection.py"
    },
    {
      "file": "kombu/connection.py",
      "image": "kombu.connection",
      "is_application": false,
      "line": 265,
      "name": "Connection._init_params",
      "path": "/usr/local/lib/python3.11/dist-packages/kombu/connection.py"
    },
    {
      "file": "kombu/transport/__init__.py",
      "image": "kombu.transport",
      "is_application": false,
      "line": 13,
      "name": "supports_librabbitmq",
      "path": "/usr/local/lib/python3.11/dist-packages/kombu/transport/__init__.py"
    },
    {
      "file": "<frozen importlib._bootstrap>",
      "image": "importlib._bootstrap",
      "is_application": true,
      "line": 1178,
      "name": "_find_and_load",
      "path": "/app/<frozen importlib._bootstrap>"
    },
    {
      "file": "<frozen importlib._bootstrap>",
      "image": "importlib._bootstrap",
      "is_application": true,
      "line": 1140,
      "name": "_find_and_load_unlocked",
      "path": "/app/<frozen importlib._bootstrap>"
    },
    {
      "file": "<frozen importlib._bootstrap>",
      "image": "importlib._bootstrap",
      "is_application": true,
      "line": 1080,
      "name": "_find_spec",
      "path": "/app/<frozen importlib._bootstrap>"
    },
    {
      "file": "<frozen importlib._bootstrap_external>",
      "image": "importlib._bootstrap_external",
      "is_application": true,
      "line": 1504,
      "name": "PathFinder.find_spec",
      "path": "/app/<frozen importlib._bootstrap_external>"
    },
    {
      "file": "<frozen importlib._bootstrap_external>",
      "image": "importlib._bootstrap_external",
      "is_application": true,
      "line": 1476,
      "name": "PathFinder._get_spec",
      "path": "/app/<frozen importlib._bootstrap_external>"
    },
    {
      "file": "<frozen importlib._bootstrap_external>",
      "image": "importlib._bootstrap_external",
      "is_application": true,
      "line": 1612,
      "name": "FileFinder.find_spec",
      "path": "/app/<frozen importlib._bootstrap_external>"
    },
    {
      "file": "<frozen importlib._bootstrap_external>",
      "image": "importlib._bootstrap_external",
      "is_application": true,
      "line": 147,
      "name": "_path_stat",
      "path": "/app/<frozen importlib._bootstrap_external>"
    },
    {
      "file": "celery/utils/dispatch/signal.py",
      "image": "celery.utils.dispatch.signal",
      "is_application": false,
      "line": 276,
      "name": "Signal.send",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/utils/dispatch/signal.py"
    },
    {
      "file": "celery_app/app.py",
      "image": "celery_app.app",
      "is_application": true,
      "line": 73,
      "name": "handle_task_prerun",
      "path": "/app/src/celery_app/app.py",
      "codeContext": {
        "file": "celery_app/app.py",
        "line": 73,
        "name": "handle_task_prerun",
        "code": "def handle_task_publish(sender, **kwargs):\n    routing_key = kwargs.get(\"routing_key\")\n    exchange = kwargs.get(\"exchange\")\n\n    logger.info(f\"Task published, task: {sender}, routing_key: {routing_key}, exchange: {exchange}\")\n\n\n@signals.task_prerun.connect\ndef handle_task_prerun(**kwargs):\n    logger.info(\n        f\"Task started, worker: {os.environ.get('WORKER_NAME')}, process: {billiard.process.current_process().index}\"\n    )\n\n\n# For some reason, any celery stubs library does not have this signal when it's actually present...\n@signals.task_internal_error.connect  # type: ignore\ndef handle_task_internal_error(**kwargs):\n    logger.error(\"Task internal error\", exc_info=kwargs[\"exception\"])\n\n",
        "lineRange": {
          "start": 64,
          "end": 83
        },
        "lines": [
          "import logging",
          "import os",
          "from typing import Any",
          "",
          "import billiard  # type: ignore[import-untyped]",
          "from celery import Celery, signals",
          "from sentry_sdk.integrations.celery import CeleryIntegration",
          "",
          "from celery_app.config import CeleryConfig",
          "from seer.bootup import bootup",
          "from seer.dependency_injection import inject, injected",
          "",
          "logger = logging.getLogger(__name__)",
          "celery_app = Celery(\"seer\")",
          "",
          "",
          "# This abstract helps tests that want to validate the entry point process.",
          "def setup_celery_entrypoint(app: Celery):",
          "    app.on_configure.connect(on_configure)",
          "    app.on_after_finalize.connect(on_after_finalize)",
          "",
          "",
          "# on_configure signal sent when celery app is being configured. This is *also* called when the celery app is imported and a task is scheduled from outside celery as well.",
          "@inject",
          "def on_configure(*args: Any, sender: Celery, config: CeleryConfig = injected, **kwargs: Any):",
          "    for k, v in config.items():",
          "        setattr(sender.conf, k, v)",
          "",
          "",
          "# on_after_finalize signal sent after celery app has been finalized. This should only be called from the celery worker and celery beat itself and not from flask or other external sources.",
          "def on_after_finalize(sender, **kwargs):",
          "    from celery_app.tasks import setup_periodic_tasks",
          "",
          "    setup_periodic_tasks(sender)",
          "",
          "",
          "setup_celery_entrypoint(celery_app)",
          "",
          "",
          "@signals.celeryd_after_setup.connect",
          "def capture_worker_name(sender, instance, **kwargs):",
          "    os.environ[\"WORKER_NAME\"] = \"{0}\".format(sender)",
          "",
          "",
          "# celeryd_init signal sent after celery app has been initialized.",
          "@signals.celeryd_init.connect",
          "def bootup_celery_worker(sender, **kwargs):",
          "    bootup(",
          "        start_model_loading=False,",
          "        integrations=[CeleryIntegration(propagate_traces=True)],",
          "    )",
          "",
          "",
          "# celerybeat_init signal sent after celerybeat app has been initialized.",
          "@signals.beat_init.connect",
          "def bootup_celery_beat(sender, **kwargs):",
          "    bootup(",
          "        start_model_loading=False,",
          "        integrations=[CeleryIntegration(propagate_traces=True)],",
          "    )",
          "",
          "",
          "@signals.after_task_publish.connect",
          "def handle_task_publish(sender, **kwargs):",
          "    routing_key = kwargs.get(\"routing_key\")",
          "    exchange = kwargs.get(\"exchange\")",
          "",
          "    logger.info(f\"Task published, task: {sender}, routing_key: {routing_key}, exchange: {exchange}\")",
          "",
          "",
          "@signals.task_prerun.connect",
          "def handle_task_prerun(**kwargs):",
          "    logger.info(",
          "        f\"Task started, worker: {os.environ.get('WORKER_NAME')}, process: {billiard.process.current_process().index}\"",
          "    )",
          "",
          "",
          "# For some reason, any celery stubs library does not have this signal when it's actually present...",
          "@signals.task_internal_error.connect  # type: ignore",
          "def handle_task_internal_error(**kwargs):",
          "    logger.error(\"Task internal error\", exc_info=kwargs[\"exception\"])",
          "",
          "",
          "@signals.task_failure.connect",
          "def handle_task_failure(**kwargs):",
          "    logger.error(\"Task failed\", exc_info=kwargs[\"exception\"])",
          ""
        ]
      }
    },
    {
      "file": "__init__.py",
      "image": "logging",
      "is_application": false,
      "line": 1489,
      "name": "Logger.info",
      "path": "/usr/lib/python3.11/logging/__init__.py"
    },
    {
      "file": "celery/worker/consumer/consumer.py",
      "image": "celery.worker.consumer.consumer",
      "is_application": false,
      "line": 340,
      "name": "Consumer.start",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/worker/consumer/consumer.py"
    },
    {
      "file": "celery/worker/consumer/consumer.py",
      "image": "celery.worker.consumer.consumer",
      "is_application": false,
      "line": 742,
      "name": "Evloop.start",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/worker/consumer/consumer.py"
    },
    {
      "file": "celery/worker/loops.py",
      "image": "celery.worker.loops",
      "is_application": false,
      "line": 97,
      "name": "asynloop",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/worker/loops.py"
    },
    {
      "file": "kombu/asynchronous/hub.py",
      "image": "kombu.asynchronous.hub",
      "is_application": false,
      "line": 373,
      "name": "Hub.create_loop",
      "path": "/usr/local/lib/python3.11/dist-packages/kombu/asynchronous/hub.py"
    },
    {
      "file": "celery/concurrency/asynpool.py",
      "image": "celery.concurrency.asynpool",
      "is_application": false,
      "line": 487,
      "name": "AsynPool._event_process_exit",
      "path": "/usr/local/lib/python3.11/dist-packages/celery/concurrency/asynpool.py"
    },
    {
      "file": "billiard/pool.py",
      "image": "billiard.pool",
      "is_application": false,
      "line": 1351,
      "name": "Pool.maintain_pool",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/pool.py"
    },
    {
      "file": "billiard/pool.py",
      "image": "billiard.pool",
      "is_application": false,
      "line": 1343,
      "name": "Pool._maintain_pool",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/pool.py"
    },
    {
      "file": "billiard/pool.py",
      "image": "billiard.pool",
      "is_application": false,
      "line": 1328,
      "name": "Pool._repopulate_pool",
      "path": "/usr/local/lib/python3.11/dist-packages/billiard/pool.py"
    },
    {
      "file": "psycopg/pq/pq_ctypes.py",
      "image": "psycopg.pq.pq_ctypes",
      "is_application": false,
      "line": 806,
      "name": "PGresult.fformat",
      "path": "/usr/local/lib/python3.11/dist-packages/psycopg/pq/pq_ctypes.py"
    }
  ],
  "profiles": [
    {
      "project_id": 6178942,
      "profile_id": "8e6055496385453693e4f260a3199b2e",
      "start": 1744817861.611776,
      "end": 1744817862.729392
    },
    {
      "project_id": 6178942,
      "profile_id": "d05b6c065122401e990e0d80f8993950",
      "start": 1744817861.6826189,
      "end": 1744817862.7722955
    },
    {
      "project_id": 6178942,
      "profile_id": "833c7a078d174ef5a72492a49f7cb520",
      "start": 1744817806.843145,
      "end": 1744817807.8807259
    },
    {
      "project_id": 6178942,
      "profile_id": "c8d28451d6134432858fd7ae07ecf914",
      "start": 1744815534.654134,
      "end": 1744815535.5745006
    },
    {
      "project_id": 6178942,
      "profile_id": "a3f67463ffe54ccb9bddbb29feea3b9e",
      "start": 1744815521.392995,
      "end": 1744815524.160979
    },
    {
      "project_id": 6178942,
      "profile_id": "574945a19e504e67b0e0a4d6e9b3e7d6",
      "start": 1744813123.457477,
      "end": 1744813126.2516515
    },
    {
      "project_id": 6178942,
      "profile_id": "6080a78c24404d92a4769009c39924de",
      "start": 1744817330.9857678,
      "end": 1744817331.9053118
    },
    {
      "project_id": 6178942,
      "profile_id": "f1e54350db194a5d8e4d330299be7a87",
      "start": 1744813188.698378,
      "end": 1744813189.6972435
    },
    {
      "project_id": 6178942,
      "profile_id": "47d824d32a2d4b36a60e1a74b10ee421",
      "start": 1744813186.993288,
      "end": 1744813187.8627067
    },
    {
      "project_id": 6178942,
      "profile_id": "a00e463ed57745de940e4d9151776d1d",
      "start": 1744812653.24529,
      "end": 1744812654.1134372
    },
    {
      "project_id": 6178942,
      "profile_id": "0ad9ae48b94542d1bdedbc3ad21129bd",
      "start": 1744813121.928992,
      "end": 1744813124.5872564
    },
    {
      "project_id": 6178942,
      "profile_id": "de3c549bf4c645e7bd2143dd299801f4",
      "start": 1744817322.330178,
      "end": 1744817324.8458226
    },
    {
      "project_id": 6178942,
      "profile_id": "2b77266011b74440911c7c53c2cd72aa",
      "start": 1744813182.378477,
      "end": 1744813183.377695
    },
    {
      "project_id": 6178942,
      "profile_id": "84fe4daf4fd64d3da0afd04bbf754821",
      "start": 1744815582.194579,
      "end": 1744815583.2068515
    },
    {
      "project_id": 6178942,
      "profile_id": "93b79d9c9e584ac8a263ec39f9fcc249",
      "start": 1744813180.914464,
      "end": 1744813182.1016407
    },
    {
      "project_id": 6178942,
      "profile_id": "776652b64615456394ec0e91b3162cad",
      "start": 1744813184.653294,
      "end": 1744813192.2015724
    },
    {
      "project_id": 6178942,
      "profile_id": "cb85567f06454966ba002f2742d643ea",
      "start": 1744817742.817276,
      "end": 1744817743.8567853
    },
    {
      "project_id": 6178942,
      "profile_id": "afae74a9a20947b6818a42922dc73b16",
      "start": 1744812650.705558,
      "end": 1744812651.5569382
    },
    {
      "project_id": 6178942,
      "profile_id": "1f6734cfae644c9c82df9397fe185c9f",
      "start": 1744818401.401464,
      "end": 1744818402.3800151
    },
    {
      "project_id": 6178942,
      "profile_id": "ce9d751d0ba04f829fc655c4a517e238",
      "start": 1744817382.412564,
      "end": 1744817383.5119455
    },
    {
      "project_id": 6178942,
      "profile_id": "b5c09c6e13ad4ac8b3e25af77fb08d62",
      "start": 1744813186.205805,
      "end": 1744813187.165215
    },
    {
      "project_id": 6178942,
      "profile_id": "511315b6f29143f09ff33bbbbc6f28c3",
      "start": 1744812890.657307,
      "end": 1744812891.5288458
    },
    {
      "project_id": 6178942,
      "profile_id": "3cfad466b9134d80a4de44f7ca62dfae",
      "start": 1744815534.1235158,
      "end": 1744815535.084887
    },
    {
      "project_id": 6178942,
      "profile_id": "863618f7dcf541ae81701a9df2f2c753",
      "start": 1744815526.270978,
      "end": 1744815527.2003105
    },
    {
      "project_id": 6178942,
      "profile_id": "109470f5df1040b69bd2d34ba8813911",
      "start": 1744812823.117269,
      "end": 1744812824.3682706
    },
    {
      "project_id": 6178942,
      "profile_id": "ba8ee2f873e54bd3851d42c326a5fec3",
      "start": 1744817814.250164,
      "end": 1744817815.3761637
    },
    {
      "project_id": 6178942,
      "profile_id": "4eacb2fd1172401dbda75eddb8adbaa5",
      "start": 1744817810.989763,
      "end": 1744817811.9972332
    },
    {
      "project_id": 6178942,
      "profile_id": "b4e67ba52d5d4f8db66e478844c28dad",
      "start": 1744817807.689877,
      "end": 1744817808.668009
    },
    {
      "project_id": 6178942,
      "profile_id": "d410013c149a448f936f1860ffefb46a",
      "start": 1744813190.337966,
      "end": 1744813191.1972384
    },
    {
      "project_id": 6178942,
      "profile_id": "523bdcdd68fa4f93b4cf63d1c4ec2d9f",
      "start": 1744813184.564435,
      "end": 1744813192.277043
    },
    {
      "project_id": 6178942,
      "profile_id": "8b11bb42b223485b9cca80930e865c36",
      "start": 1744812644.137488,
      "end": 1744812645.1161125
    },
    {
      "project_id": 6178942,
      "profile_id": "842f7dfa3c6e4e31aacab2c37c222487",
      "start": 1744817816.371907,
      "end": 1744817817.4696174
    },
    {
      "project_id": 6178942,
      "profile_id": "887026a9431a4417bd066837486852c8",
      "start": 1744817326.0736058,
      "end": 1744817326.9732974
    },
    {
      "project_id": 6178942,
      "profile_id": "7fa544c243bf4b14b35d8baa8b9657f9",
      "start": 1744816903.0005069,
      "end": 1744816904.0310726
    },
    {
      "project_id": 6178942,
      "profile_id": "a6dfcca6aadf46edaaad1c63c76f3661",
      "start": 1744815523.77474,
      "end": 1744815524.6842864
    },
    {
      "project_id": 6178942,
      "profile_id": "cd40f670efa94287898e478d94177ab4",
      "start": 1744813185.5185978,
      "end": 1744813186.706198
    },
    {
      "project_id": 6178942,
      "profile_id": "7911a9e153b74517ac1850b5b3f68c09",
      "start": 1744813123.6485069,
      "end": 1744813124.8668213
    },
    {
      "project_id": 6178942,
      "profile_id": "614e24518943485389f875abdce71224",
      "start": 1744817805.312555,
      "end": 1744817806.460309
    },
    {
      "project_id": 6178942,
      "profile_id": "d3276b6f29454d6eb5b5fa75484922cb",
      "start": 1744815581.2653708,
      "end": 1744815582.2136927
    },
    {
      "project_id": 6178942,
      "profile_id": "3d779565a4c54103a07ce5e3e48c2511",
      "start": 1744815532.439924,
      "end": 1744815533.279088
    },
    {
      "project_id": 6178942,
      "profile_id": "9147052b173b46568687141f9b7f0a0b",
      "start": 1744817863.39773,
      "end": 1744817864.311106
    },
    {
      "project_id": 6178942,
      "profile_id": "a1fe2fd618874a20a21f66faad7b15fe",
      "start": 1744817324.729048,
      "end": 1744817325.6096349
    },
    {
      "project_id": 6178942,
      "profile_id": "825fbf007f534b94b0390763f4f353e8",
      "start": 1744816906.384146,
      "end": 1744816908.9380612
    },
    {
      "project_id": 6178942,
      "profile_id": "3757593f0cc04366a7c6ff9f232ee010",
      "start": 1744815529.7558641,
      "end": 1744815530.6791916
    },
    {
      "project_id": 6178942,
      "profile_id": "7062fa4114e84b8f9c556cca81d27913",
      "start": 1744813960.8438342,
      "end": 1744813961.864979
    },
    {
      "project_id": 6178942,
      "profile_id": "db6a176ce5364e4eab6abc2ff1f373d0",
      "start": 1744812653.302026,
      "end": 1744812654.1934075
    },
    {
      "project_id": 6178942,
      "profile_id": "63a0170a8e274db5af2f08b681ce3a13",
      "start": 1744812644.846997,
      "end": 1744812646.0474284
    },
    {
      "project_id": 6178942,
      "profile_id": "f7be605b445e48b08f1261210ef953bc",
      "start": 1744817739.632096,
      "end": 1744817740.8637345
    },
    {
      "project_id": 6178942,
      "profile_id": "0286a9fdc53146bda12929f4591c266a",
      "start": 1744815528.916213,
      "end": 1744815529.8152013
    },
    {
      "project_id": 6178942,
      "profile_id": "6e2e2f1a4de84b23a1cea40459541ec3",
      "start": 1744815528.0077178,
      "end": 1744815528.9581013
    },
    {
      "project_id": 6178942,
      "profile_id": "ebc2b39854284259b9e7bf79ffd9d272",
      "start": 1744815522.205125,
      "end": 1744815524.739952
    },
    {
      "project_id": 6178942,
      "profile_id": "3442796e228e4919b09606d7ff2cf77b",
      "start": 1744813187.7971148,
      "end": 1744813188.845853
    },
    {
      "project_id": 6178942,
      "profile_id": "2f2abaa8a1fe4cd1ad74e92edd7e91ed",
      "start": 1744817325.284506,
      "end": 1744817326.1822145
    },
    {
      "project_id": 6178942,
      "profile_id": "c885dddb0acc413594fb2c46a9269a36",
      "start": 1744815530.7204878,
      "end": 1744815531.6694324
    },
    {
      "project_id": 6178942,
      "profile_id": "a2d9fa04e29e4504b032820da41a78fe",
      "start": 1744812642.0828428,
      "end": 1744812643.1046114
    },
    {
      "project_id": 6178942,
      "profile_id": "7cd0ce4b98874043b134a289b48b25fa",
      "start": 1744815534.182764,
      "end": 1744815535.0720227
    },
    {
      "project_id": 6178942,
      "profile_id": "0232ca328b4f407c9386f209876fc8cc",
      "start": 1744812642.232244,
      "end": 1744812643.3094995
    },
    {
      "project_id": 6178942,
      "profile_id": "cea87d7c964948069da9033bae653c06",
      "start": 1744812640.765941,
      "end": 1744812641.8730237
    },
    {
      "project_id": 6178942,
      "profile_id": "a9279dea4b6741fc965ab08e33f01635",
      "start": 1744817864.7991822,
      "end": 1744817865.6995454
    },
    {
      "project_id": 6178942,
      "profile_id": "db8a3601563546a98838d07ccafb9897",
      "start": 1744817802.875316,
      "end": 1744817804.172392
    },
    {
      "project_id": 6178942,
      "profile_id": "0ade8bc27f9742b8965ed99b154b8f7f",
      "start": 1744817515.900463,
      "end": 1744817528.0202181
    },
    {
      "project_id": 6178942,
      "profile_id": "a6d1312116d4490dbd1d849c24fdcd68",
      "start": 1744812890.068267,
      "end": 1744812890.9786563
    },
    {
      "project_id": 6178942,
      "profile_id": "c2ae36b041cd40d7a2f53a45098cad47",
      "start": 1744812882.135373,
      "end": 1744812883.1939373
    },
    {
      "project_id": 6178942,
      "profile_id": "2188f9d622ff40a8897b5abd5fa4f980",
      "start": 1744817802.126673,
      "end": 1744817803.3534908
    },
    {
      "project_id": 6178942,
      "profile_id": "ebe83cc1ff1942c680cebc4a731cd933",
      "start": 1744813182.979346,
      "end": 1744813183.876855
    },
    {
      "project_id": 6178942,
      "profile_id": "726f5cc8b8b340138b592eac0ded9d11",
      "start": 1744815581.479816,
      "end": 1744815582.4792223
    },
    {
      "project_id": 6178942,
      "profile_id": "a9a512f8ccc041e2a1e715dde451333b",
      "start": 1744813122.099512,
      "end": 1744813123.1589646
    },
    {
      "project_id": 6178942,
      "profile_id": "b592ec2a0e784bc6b373bcc403379236",
      "start": 1744812281.602464,
      "end": 1744812282.8699696
    },
    {
      "project_id": 6178942,
      "profile_id": "1f7a577bd1cc4ff6afa18efbbcab8589",
      "start": 1744812212.956213,
      "end": 1744812214.4016962
    },
    {
      "project_id": 6178942,
      "profile_id": "1b051cc3517d4b86af751af5e2778dd0",
      "start": 1744817811.928863,
      "end": 1744817813.2667131
    },
    {
      "project_id": 6178942,
      "profile_id": "4b3655b8b7554fdaaafea6a1f8120f86",
      "start": 1744812884.263802,
      "end": 1744812885.3621912
    },
    {
      "project_id": 6178942,
      "profile_id": "45a529c70f4344eea94ac79e466cf6b8",
      "start": 1744812649.8757088,
      "end": 1744812650.8230379
    },
    {
      "project_id": 6178942,
      "profile_id": "ba6b64ae8f254c78b3bd2f55438d3f66",
      "start": 1744813183.1311638,
      "end": 1744813184.1382759
    },
    {
      "project_id": 6178942,
      "profile_id": "b14018920df94554b58de4fb06903eed",
      "start": 1744813120.5853262,
      "end": 1744813121.7782536
    },
    {
      "project_id": 6178942,
      "profile_id": "eb46a83e7fb34541abf5e932cb160c28",
      "start": 1744813187.909962,
      "end": 1744813188.8682866
    },
    {
      "project_id": 6178942,
      "profile_id": "dcf95d33b532467992e13bda8f97a973",
      "start": 1744812652.4078772,
      "end": 1744812653.357025
    },
    {
      "project_id": 6178942,
      "profile_id": "a66f2f6532344d5ea05cc9094434d546",
      "start": 1744812588.046867,
      "end": 1744812590.2502995
    },
    {
      "project_id": 6178942,
      "profile_id": "0b637e0141074ff585c1dae2fcdc5e7b",
      "start": 1744818401.975012,
      "end": 1744818404.592863
    },
    {
      "project_id": 6178942,
      "profile_id": "fc783c038d924066a5c17569f81df421",
      "start": 1744813186.382763,
      "end": 1744813189.060799
    },
    {
      "project_id": 6178942,
      "profile_id": "0723440520bb40dba12cc9c385733b8f",
      "start": 1744817326.246436,
      "end": 1744817327.1969137
    },
    {
      "project_id": 6178942,
      "profile_id": "3ec1564885ef45d88bd45ef368814307",
      "start": 1744815522.119158,
      "end": 1744815523.216509
    },
    {
      "project_id": 6178942,
      "profile_id": "4be8cf574a5749b7bbe7eba312a6565b",
      "start": 1744816540.364373,
      "end": 1744816552.3203533
    },
    {
      "project_id": 6178942,
      "profile_id": "da2aa67bf7e1482aafa3e9095ce44540",
      "start": 1744813125.9266071,
      "end": 1744813127.1144135
    },
    {
      "project_id": 6178942,
      "profile_id": "91b3b0628a3a4ae8a72b293712c19ce6",
      "start": 1744813125.738601,
      "end": 1744813126.8295271
    },
    {
      "project_id": 6178942,
      "profile_id": "392204bb8124434e8debcd64faddb585",
      "start": 1744813120.405924,
      "end": 1744813123.0245035
    },
    {
      "project_id": 6178942,
      "profile_id": "6750bedb44564836bcd3521efc74a1f1",
      "start": 1744817802.2564778,
      "end": 1744817803.2765584
    },
    {
      "project_id": 6178942,
      "profile_id": "b9ab84e563a748fb94baa33da72d1f6b",
      "start": 1744817326.8184268,
      "end": 1744817327.7457488
    },
    {
      "project_id": 6178942,
      "profile_id": "e7d54155a7a2420e8ebb2eed1c050f48",
      "start": 1744816842.612278,
      "end": 1744816843.6409445
    },
    {
      "project_id": 6178942,
      "profile_id": "bc169710139d4600b4a30ea555c3b9e4",
      "start": 1744813190.26635,
      "end": 1744813191.157664
    },
    {
      "project_id": 6178942,
      "profile_id": "735afa00f27e47338e54a5a491b734b5",
      "start": 1744813185.4485052,
      "end": 1744813186.4266477
    },
    {
      "project_id": 6178942,
      "profile_id": "2942c3ff63154339a32c83646c619878",
      "start": 1744812644.970025,
      "end": 1744812645.9212995
    },
    {
      "project_id": 6178942,
      "profile_id": "9cc28f39be6b47898f4e6c64fa42493d",
      "start": 1744815523.67156,
      "end": 1744815524.740117
    },
    {
      "project_id": 6178942,
      "profile_id": "b9a8c92bc9d24c7095d963560f515b1d",
      "start": 1744812648.233245,
      "end": 1744812649.3140066
    },
    {
      "project_id": 6178942,
      "profile_id": "a5211dd668084eb28b9966fca9fcc226",
      "start": 1744812652.4616058,
      "end": 1744812653.460169
    },
    {
      "project_id": 6178942,
      "profile_id": "0c0440a77a0c499089315bac4c68a858",
      "start": 1744812886.434324,
      "end": 1744812887.3050048
    },
    {
      "project_id": 6178942,
      "profile_id": "a8a8c32eafd74030b22a156045244fb3",
      "start": 1744813182.2182028,
      "end": 1744813183.2880776
    },
    {
      "project_id": 6178942,
      "profile_id": "b8c1c15bacb34033be36026ff19b5848",
      "start": 1744813122.895691,
      "end": 1744813123.994837
    },
    {
      "project_id": 6178942,
      "profile_id": "d61af13f46a54a3a865716ee9c00d2b9",
      "start": 1744817384.540139,
      "end": 1744817385.642799
    },
    {
      "project_id": 6178942,
      "profile_id": "add92a6306e848dc97d605edcda128af",
      "start": 1744812884.99179,
      "end": 1744812885.9935086
    },
    {
      "project_id": 6178942,
      "profile_id": "8ae91b5b817741f18d59a281ab7abbf0",
      "start": 1744812651.599465,
      "end": 1744812652.4992259
    },
    {
      "project_id": 6178942,
      "profile_id": "db656419d4c7477caee92f58a0f3a6f5",
      "start": 1744812284.93047,
      "end": 1744812285.9189568
    },
    {
      "project_id": 6178942,
      "profile_id": "9c13538d3f564623b573c1fd3b600664",
      "start": 1744812221.65815,
      "end": 1744812222.6567273
    },
    {
      "project_id": 6178942,
      "profile_id": "626b96307ed1420590fc52c0d4d2c2ad",
      "start": 1744817809.399415,
      "end": 1744817810.2799385
    },
    {
      "project_id": 6178942,
      "profile_id": "2e2822639fd84f88b4c63cab1dc518d2",
      "start": 1744818400.874084,
      "end": 1744818401.9521163
    },
    {
      "project_id": 6178942,
      "profile_id": "a33f22938d8f403d836eb630b0d70331",
      "start": 1744817621.5872772,
      "end": 1744817622.7278624
    },
    {
      "project_id": 6178942,
      "profile_id": "a5a8409e796049a2a33e01d3ad51f275",
      "start": 1744817382.945833,
      "end": 1744817384.1426508
    },
    {
      "project_id": 6178942,
      "profile_id": "2177f081501547618996f521ed4f6e86",
      "start": 1744815531.587245,
      "end": 1744815532.6042888
    },
    {
      "project_id": 6178942,
      "profile_id": "3cbf481837244155ac70c83f7a37ddc5",
      "start": 1744815471.441466,
      "end": 1744815474.1677837
    },
    {
      "project_id": 6178942,
      "profile_id": "87c932f22cef4304be8c1e9d5377c77f",
      "start": 1744812886.175072,
      "end": 1744812887.4735596
    },
    {
      "project_id": 6178942,
      "profile_id": "a16d52d1e8e64edf872780512719df92",
      "start": 1744812649.054333,
      "end": 1744812650.1134377
    },
    {
      "project_id": 6178942,
      "profile_id": "cc1749382e0b45e5869dd24b5a3ffeb8",
      "start": 1744815531.5708082,
      "end": 1744815532.4711835
    },
    {
      "project_id": 6178942,
      "profile_id": "7c1e5aca33164fe18b880201bd31de39",
      "start": 1744813124.3742661,
      "end": 1744813125.324862
    },
    {
      "project_id": 6178942,
      "profile_id": "ecd2c9da7f804f30a58f6c95cc82ae62",
      "start": 1744812886.9819992,
      "end": 1744812887.9810288
    },
    {
      "project_id": 6178942,
      "profile_id": "e05cd3c4ce59491b954888b6cf48a84e",
      "start": 1744812646.605088,
      "end": 1744812648.0303314
    },
    {
      "project_id": 6178942,
      "profile_id": "7d72d6271b1c43189591697c427792a6",
      "start": 1744812219.778435,
      "end": 1744812220.8890889
    },
    {
      "project_id": 6178942,
      "profile_id": "868945c5aeca476884ea274503df8a11",
      "start": 1744818400.347649,
      "end": 1744818401.4054065
    },
    {
      "project_id": 6178942,
      "profile_id": "a76e8ea870924342b1a967a9a7e27776",
      "start": 1744817386.139872,
      "end": 1744817387.3300645
    },
    {
      "project_id": 6178942,
      "profile_id": "c2947506b8e74052bdf4c428f1893902",
      "start": 1744817327.807062,
      "end": 1744817328.7659779
    },
    {
      "project_id": 6178942,
      "profile_id": "046a5b03179e4e59b55ac8ce4f044e3d",
      "start": 1744817327.532526,
      "end": 1744817328.551043
    },
    {
      "project_id": 6178942,
      "profile_id": "cbf498a9dc1442b4b414755655729f6e",
      "start": 1744815533.2552,
      "end": 1744815534.1160572
    },
    {
      "project_id": 6178942,
      "profile_id": "2d99c5e43dab43a183e9732f6b4402b4",
      "start": 1744815528.887711,
      "end": 1744815529.7555223
    },
    {
      "project_id": 6178942,
      "profile_id": "4b5cddef4c504467ad29036b1215bc68",
      "start": 1744813962.502858,
      "end": 1744813963.3919222
    },
    {
      "project_id": 6178942,
      "profile_id": "a0d140eac44c463398a1f4c0896e381e",
      "start": 1744816840.874389,
      "end": 1744816841.9928217
    },
    {
      "project_id": 6178942,
      "profile_id": "3277d25327124b17aab31276eaf3abcd",
      "start": 1744817814.927526,
      "end": 1744817815.9682572
    },
    {
      "project_id": 6178942,
      "profile_id": "c58edd8456a6496ca9bf4711172fe045",
      "start": 1744812654.174109,
      "end": 1744812655.0322025
    },
    {
      "project_id": 6178942,
      "profile_id": "42346dd596494d469332c6ef42191882",
      "start": 1744816907.4904819,
      "end": 1744816908.4912844
    },
    {
      "project_id": 6178942,
      "profile_id": "949fd7e253c74baca077a095e37fec1b",
      "start": 1744815523.010626,
      "end": 1744815524.0406327
    },
    {
      "project_id": 6178942,
      "profile_id": "93cef92a98a2460897cbde3f9b1742e8",
      "start": 1744817860.800428,
      "end": 1744817861.908209
    },
    {
      "project_id": 6178942,
      "profile_id": "a2cecf3709e442cf961d1fffa0126ec2",
      "start": 1744817804.350678,
      "end": 1744817805.3046985
    },
    {
      "project_id": 6178942,
      "profile_id": "273e91b89a7e498aaff5036427aeea14",
      "start": 1744817621.595459,
      "end": 1744817629.778692
    },
    {
      "project_id": 6178942,
      "profile_id": "bcfd5c152a5e41ca9e163be0280362c0",
      "start": 1744815525.42754,
      "end": 1744815527.011388
    },
    {
      "project_id": 6178942,
      "profile_id": "4b8f407a77e5437285ae7ef0050e9f6d",
      "start": 1744817322.56034,
      "end": 1744817323.7024033
    },
    {
      "project_id": 6178942,
      "profile_id": "47b963351ecc4cf4a33f0c883ffacd3a",
      "start": 1744812885.456589,
      "end": 1744812886.4348779
    },
    {
      "project_id": 6178942,
      "profile_id": "eec2e22b023c48aa828a66850ae7d833",
      "start": 1744815524.594819,
      "end": 1744815525.6022232
    },
    {
      "project_id": 6178942,
      "profile_id": "079974f5ca434317b8a73d68f6a95e37",
      "start": 1744812889.273221,
      "end": 1744812890.093089
    },
    {
      "project_id": 6178942,
      "profile_id": "9fddd1a9805c47fa91c31be543e2a444",
      "start": 1744813189.411856,
      "end": 1744813190.2917392
    },
    {
      "project_id": 6178942,
      "profile_id": "b9b7947b79af46c581bebbdcc071518a",
      "start": 1744817816.044734,
      "end": 1744817817.076982
    },
    {
      "project_id": 6178942,
      "profile_id": "174502f61393483a8b4c492704d9adb4",
      "start": 1744816908.042719,
      "end": 1744816908.9575353
    },
    {
      "project_id": 6178942,
      "profile_id": "628fdbf3d810473cb6d0c78b204e686c",
      "start": 1744812647.381173,
      "end": 1744812648.2023504
    },
    {
      "project_id": 6178942,
      "profile_id": "5d56082b929a493e911c7842ebf9c38a",
      "start": 1744812643.3687198,
      "end": 1744812644.250579
    },
    {
      "project_id": 6178942,
      "profile_id": "fc5f7d55622142839d2039a7140d2d06",
      "start": 1744812284.0272372,
      "end": 1744812284.9557083
    },
    {
      "project_id": 6178942,
      "profile_id": "c2fe716a2ca847d99fbd95bb2d32ed1d",
      "start": 1744817817.8085701,
      "end": 1744817818.677462
    },
    {
      "project_id": 6178942,
      "profile_id": "d1cbd5be920f4b399740cd77c403bb68",
      "start": 1744815532.409539,
      "end": 1744815533.2492793
    },
    {
      "project_id": 6178942,
      "profile_id": "3105454a7f16434f92fd0385a0e2b02d",
      "start": 1744812883.1385589,
      "end": 1744812884.3100421
    },
    {
      "project_id": 6178942,
      "profile_id": "a37bb90b8b2d4efab073dff69889083e",
      "start": 1744812648.133595,
      "end": 1744812649.1232696
    },
    {
      "project_id": 6178942,
      "profile_id": "c393350d18f24c4ab46b3c282c8a1fc6",
      "start": 1744817864.156551,
      "end": 1744817865.109643
    },
    {
      "project_id": 6178942,
      "profile_id": "fae4b24d0d47415e85e8727c3a34db8b",
      "start": 1744817331.542079,
      "end": 1744817332.4890687
    },
    {
      "project_id": 6178942,
      "profile_id": "3315291751cc4d11be27f1b5ae1e2d5c",
      "start": 1744817813.429987,
      "end": 1744817814.3587313
    },
    {
      "project_id": 6178942,
      "profile_id": "d1e114f6b1af4f4c971b81c70b2d1312",
      "start": 1744817803.612906,
      "end": 1744817804.5413086
    },
    {
      "project_id": 6178942,
      "profile_id": "33cd37b219d04e7cbbbd4fb6845681ca",
      "start": 1744817323.0457149,
      "end": 1744817324.2964733
    },
    {
      "project_id": 6178942,
      "profile_id": "6cd57be38e43458596dfbcbc805f6ed4",
      "start": 1744812823.9156039,
      "end": 1744812824.8860233
    },
    {
      "project_id": 6178942,
      "profile_id": "0dfa412cefbc4c8ca15e3f3d679552bd",
      "start": 1744817321.045759,
      "end": 1744817322.2436178
    },
    {
      "project_id": 6178942,
      "profile_id": "23a2a6184a734cb79c8078307bbfc7f6",
      "start": 1744813187.1212711,
      "end": 1744813188.0249827
    },
    {
      "project_id": 6178942,
      "profile_id": "7fa2fbd849d24d27821925a53675e492",
      "start": 1744812888.673544,
      "end": 1744812889.6328502
    },
    {
      "project_id": 6178942,
      "profile_id": "b574a8b3e2b143bea63b61b41e153580",
      "start": 1744812885.731633,
      "end": 1744812886.8785884
    },
    {
      "project_id": 6178942,
      "profile_id": "8e1afded72544c9e920a3656a8206dbc",
      "start": 1744812823.9782798,
      "end": 1744812825.0293882
    },
    {
      "project_id": 6178942,
      "profile_id": "7f140ee461d24a22a2e8f9c1891c0e87",
      "start": 1744816842.3106828,
      "end": 1744816843.613143
    },
    {
      "project_id": 6178942,
      "profile_id": "c6d10be3b54d4a77895eb7290259ba1d",
      "start": 1744817808.555718,
      "end": 1744817809.4796576
    },
    {
      "project_id": 6178942,
      "profile_id": "b18b2ff761504de891d38df8badcb2ab",
      "start": 1744813961.381802,
      "end": 1744813962.4420316
    },
    {
      "project_id": 6178942,
      "profile_id": "4d54181c4c364504a0e9c08ccb2e2f1f",
      "start": 1744813183.894997,
      "end": 1744813184.9741187
    },
    {
      "project_id": 6178942,
      "profile_id": "dbf902cd7e314dbfa9736f14c982191b",
      "start": 1744812821.979119,
      "end": 1744812823.0378356
    },
    {
      "project_id": 6178942,
      "profile_id": "39065dff4dd54fc0a41e312436b8bf0d",
      "start": 1744817862.4492002,
      "end": 1744817863.448932
    },
    {
      "project_id": 6178942,
      "profile_id": "cb40d6b97cd141caa19c1772fe224271",
      "start": 1744812887.905559,
      "end": 1744812888.916252
    },
    {
      "project_id": 6178942,
      "profile_id": "c300d4845c2f48f699c207a9385d2950",
      "start": 1744812820.8871412,
      "end": 1744812822.0858848
    },
    {
      "project_id": 6178942,
      "profile_id": "cccc89fc05c64de18207ff109f120e0b",
      "start": 1744812221.1151948,
      "end": 1744812222.1225119
    },
    {
      "project_id": 6178942,
      "profile_id": "67ca64eeb8424f5ab868da2d15b4ac05",
      "start": 1744817620.7392771,
      "end": 1744817622.0231538
    },
    {
      "project_id": 6178942,
      "profile_id": "0b81cb54d48648dda8064fecbc3d5b1e",
      "start": 1744817817.5142121,
      "end": 1744817818.4718668
    },
    {
      "project_id": 6178942,
      "profile_id": "eedda2eb1b80494d84244d5e0928d66d",
      "start": 1744817863.253581,
      "end": 1744817864.1330206
    },
    {
      "project_id": 6178942,
      "profile_id": "f13f9a1797a9488c98c81098a8621bca",
      "start": 1744817811.121505,
      "end": 1744817812.2794342
    },
    {
      "project_id": 6178942,
      "profile_id": "cfd0081b5069450b9307f8ca74059038",
      "start": 1744817682.091437,
      "end": 1744817683.2116985
    },
    {
      "project_id": 6178942,
      "profile_id": "00ccf46c683a4bda8e8129f8df6298f2",
      "start": 1744816841.559933,
      "end": 1744816844.0456955
    },
    {
      "project_id": 6178942,
      "profile_id": "83501f7f340a4379b27ef3979ff6e76c",
      "start": 1744813124.200453,
      "end": 1744813125.4591527
    },
    {
      "project_id": 6178942,
      "profile_id": "ecbccb46284c42ebb93c199e7e8a4b87",
      "start": 1744817323.2742012,
      "end": 1744817324.2733855
    },
    {
      "project_id": 6178942,
      "profile_id": "75a229bbe484430495a94e51e74a4e58",
      "start": 1744813961.967819,
      "end": 1744813962.985404
    },
    {
      "project_id": 6178942,
      "profile_id": "305f9f5a89cf4e0aaa4e4ed7f6c94235",
      "start": 1744813121.326035,
      "end": 1744813122.6363049
    },
    {
      "project_id": 6178942,
      "profile_id": "384c1c5fa0db4809be7f17b95ffb13b9",
      "start": 1744812588.919996,
      "end": 1744812590.00926
    },
    {
      "project_id": 6178942,
      "profile_id": "100485b2f8624f9a823cb2da898c3857",
      "start": 1744813122.7100532,
      "end": 1744813123.6503747
    },
    {
      "project_id": 6178942,
      "profile_id": "4ef668a902624fc09556d2cf52a2a0f1",
      "start": 1744812282.3935392,
      "end": 1744812283.9533796
    },
    {
      "project_id": 6178942,
      "profile_id": "02f4576404a843bf940634762aacf766",
      "start": 1744817814.5316591,
      "end": 1744817817.3227265
    },
    {
      "project_id": 6178942,
      "profile_id": "dad80d0fd4aa430c81c970b7544887b8",
      "start": 1744815529.778631,
      "end": 1744815530.6872168
    },
    {
      "project_id": 6178942,
      "profile_id": "551ef76bfc764d758d6c876612782c9b",
      "start": 1744812647.3042922,
      "end": 1744812648.3916225
    },
    {
      "project_id": 6178942,
      "profile_id": "3f656d5841bd4d92857942bcc34dc119",
      "start": 1744813963.5970628,
      "end": 1744813964.4158611
    },
    {
      "project_id": 6178942,
      "profile_id": "c1c19eb2682c43c0a3cb921535e6ba0d",
      "start": 1744812648.9431732,
      "end": 1744812649.832443
    },
    {
      "project_id": 6178942,
      "profile_id": "df3dfd3ae3f640f5b8d11f7595471c4c",
      "start": 1744817806.1208022,
      "end": 1744817807.2813404
    },
    {
      "project_id": 6178942,
      "profile_id": "b8d8f9d0129045478e2f047b062bf2c3",
      "start": 1744815582.003602,
      "end": 1744815583.021732
    },
    {
      "project_id": 6178942,
      "profile_id": "e518e817ac914c5e86f7ff5abb414889",
      "start": 1744817385.0936651,
      "end": 1744817386.1467314
    },
    {
      "project_id": 6178942,
      "profile_id": "bf57947c19c444a18e29a4ff56b3b2f7",
      "start": 1744817816.767083,
      "end": 1744817817.6301303
    },
    {
      "project_id": 6178942,
      "profile_id": "1a02e7c0f0c74576834d83a2a5466702",
      "start": 1744813190.87012,
      "end": 1744813191.7078393
    },
    {
      "project_id": 6178942,
      "profile_id": "60b592f52222489393d42d5e735199a5",
      "start": 1744813902.124962,
      "end": 1744813903.1830962
    },
    {
      "project_id": 6178942,
      "profile_id": "dde4cf3d86204ffea6465de1fea93d34",
      "start": 1744813188.604983,
      "end": 1744813189.565282
    },
    {
      "project_id": 6178942,
      "profile_id": "87e1c8fab411405a9092c613899e7487",
      "start": 1744812643.253008,
      "end": 1744812646.1105247
    },
    {
      "project_id": 6178942,
      "profile_id": "ac2d0503eb654fae8d4782a75acda888",
      "start": 1744817808.609515,
      "end": 1744817809.4695263
    },
    {
      "project_id": 6178942,
      "profile_id": "efc27959b6454375abc08b59f57b60ca",
      "start": 1744816904.625937,
      "end": 1744816905.5038335
    },
    {
      "project_id": 6178942,
      "profile_id": "0d6da836546844c3997ef1ec9fe82216",
      "start": 1744812284.8535302,
      "end": 1744812285.8520164
    },
    {
      "project_id": 6178942,
      "profile_id": "7d3b6574027d469db1aaa651199f5f64",
      "start": 1744818347.109548,
      "end": 1744818348.4170449
    },
    {
      "project_id": 6178942,
      "profile_id": "9b1e02f447174050bb79c218ab248c21",
      "start": 1744817803.7831979,
      "end": 1744817804.7518327
    },
    {
      "project_id": 6178942,
      "profile_id": "5720c56740a4425f9867c274bc5ae3b8",
      "start": 1744817326.988543,
      "end": 1744817327.8971484
    },
    {
      "project_id": 6178942,
      "profile_id": "2fb2f490bac24e2080fbe08e95447643",
      "start": 1744812282.488262,
      "end": 1744812283.5263007
    },
    {
      "project_id": 6178942,
      "profile_id": "eee2c049de754759af3158608dd863d3",
      "start": 1744816908.6122072,
      "end": 1744816909.6560135
    },
    {
      "project_id": 6178942,
      "profile_id": "5335fb09bb894f4289ad94ebbb006294",
      "start": 1744815527.124201,
      "end": 1744815528.0229375
    },
    {
      "project_id": 6178942,
      "profile_id": "30f5a74943204d6cb7be73ba1bb20805",
      "start": 1744813901.5538602,
      "end": 1744813902.5017245
    },
    {
      "project_id": 6178942,
      "profile_id": "af8c6f12370d40f685a4ad8bfb522b67",
      "start": 1744815526.348317,
      "end": 1744815527.2674396
    },
    {
      "project_id": 6178942,
      "profile_id": "c4fb661507314c9fb4afefc3612a6e86",
      "start": 1744816905.211618,
      "end": 1744816906.621139
    },
    {
      "project_id": 6178942,
      "profile_id": "fa6c431450af48f4a21f8a2be94d5354",
      "start": 1744812646.4850729,
      "end": 1744812647.7268467
    },
    {
      "project_id": 6178942,
      "profile_id": "892c09a6699445568955bcfb5df7e27f",
      "start": 1744816660.748286,
      "end": 1744816663.50627
    },
    {
      "project_id": 6178942,
      "profile_id": "03e32b11c93c485e97c210951abe1a07",
      "start": 1744818402.50955,
      "end": 1744818403.3690305
    },
    {
      "project_id": 6178942,
      "profile_id": "9931add89ffc4a7b88021183a5ef8816",
      "start": 1744813125.170021,
      "end": 1744813126.191835
    },
    {
      "project_id": 6178942,
      "profile_id": "af46c71e91d44dd09969e8082c3cc5a4",
      "start": 1744813124.969837,
      "end": 1744813125.8707337
    },
    {
      "project_id": 6178942,
      "profile_id": "48dec15de712430fb305c45c7871174a",
      "start": 1744816910.2758222,
      "end": 1744816911.2356331
    },
    {
      "project_id": 6178942,
      "profile_id": "6ed7682a340f4f59b8371f585994b64e",
      "start": 1744817864.019268,
      "end": 1744817864.9271488
    },
    {
      "project_id": 6178942,
      "profile_id": "20a59888a85c410ba6d1ce55d8a4f64b",
      "start": 1744812284.094239,
      "end": 1744812285.2374744
    },
    {
      "project_id": 6178942,
      "profile_id": "3f93bf6cf0dc491fa7bb4ff328fbf866",
      "start": 1744815527.170447,
      "end": 1744815528.0200827
    },
    {
      "project_id": 6178942,
      "profile_id": "3fc66495d7cc492e92d583c75828e4a2",
      "start": 1744817803.021101,
      "end": 1744817804.0386865
    },
    {
      "project_id": 6178942,
      "profile_id": "b1c04bf0b20749f7b8aa07586eed6145",
      "start": 1744817324.012395,
      "end": 1744817325.0321982
    },
    {
      "project_id": 6178942,
      "profile_id": "3afad6addb4d479d89efcb69f6d05646",
      "start": 1744816902.4670231,
      "end": 1744816903.7054012
    },
    {
      "project_id": 6178942,
      "profile_id": "2b476115190b4a658682f9324c93a7a1",
      "start": 1744815580.590605,
      "end": 1744815581.6197457
    },
    {
      "project_id": 6178942,
      "profile_id": "8f5b6147377f40baaffb23512625ded2",
      "start": 1744812882.413219,
      "end": 1744812883.55122
    },
    {
      "project_id": 6178942,
      "profile_id": "f9608978c4c24546b953eb83685c428b",
      "start": 1744812888.492193,
      "end": 1744812889.5935712
    },
    {
      "project_id": 6178942,
      "profile_id": "0d305d4e9b214c369fbbe8e1125d10d9",
      "start": 1744817811.836544,
      "end": 1744817812.6778595
    },
    {
      "project_id": 6178942,
      "profile_id": "62b6759fb9ec4c4f9754e2c19b706a8b",
      "start": 1744817812.748849,
      "end": 1744817813.717187
    },
    {
      "project_id": 6178942,
      "profile_id": "2b2d6faaa38148d681a9939abd8f7343",
      "start": 1744817809.530339,
      "end": 1744817810.687362
    },
    {
      "project_id": 6178942,
      "profile_id": "3854fc7d52c9415fa070150262be79eb",
      "start": 1744816909.7188401,
      "end": 1744816910.5713031
    },
    {
      "project_id": 6178942,
      "profile_id": "43c257452a414331aae3cf9e8998e471",
      "start": 1744812700.654043,
      "end": 1744812701.7214987
    },
    {
      "project_id": 6178942,
      "profile_id": "5561bb9284fd40fe8194a2409e116f91",
      "start": 1744812650.558935,
      "end": 1744812651.519773
    },
    {
      "project_id": 6178942,
      "profile_id": "f1029e15ff1b4786bf436f9da647ec11",
      "start": 1744816904.090165,
      "end": 1744816905.099404
    },
    {
      "project_id": 6178942,
      "profile_id": "8d7bdd69dba04954a157f5b1c276102b",
      "start": 1744817815.650383,
      "end": 1744817816.5987022
    },
    {
      "project_id": 6178942,
      "profile_id": "e1ab85a3ed2248f08adbc496f0c61e5e",
      "start": 1744817800.953513,
      "end": 1744817802.153213
    },
    {
      "project_id": 6178942,
      "profile_id": "87282dfb15c249c28d42165e799ba1df",
      "start": 1744813181.478703,
      "end": 1744813182.60758
    },
    {
      "project_id": 6178942,
      "profile_id": "ee04aa3b649d4c5f915021baa4b6264f",
      "start": 1744812824.4694662,
      "end": 1744812825.3796308
    },
    {
      "project_id": 6178942,
      "profile_id": "4675cb387f9c4575b09dc51b298a79df",
      "start": 1744812821.431735,
      "end": 1744812822.4791963
    },
    {
      "project_id": 6178942,
      "profile_id": "92a3ddb897cb407c90db3fffaaab2702",
      "start": 1744817810.3154628,
      "end": 1744817811.3323991
    },
    {
      "project_id": 6178942,
      "profile_id": "d8c871d882064d5f8d228672b27b9deb",
      "start": 1744817386.6663198,
      "end": 1744817387.5242288
    },
    {
      "project_id": 6178942,
      "profile_id": "a9aff365549a45a59cd6b7656d54bdcf",
      "start": 1744812882.800158,
      "end": 1744812883.7386563
    },
    {
      "project_id": 6178942,
      "profile_id": "d76757fcf0224d1a8195ab558ff38908",
      "start": 1744812651.567133,
      "end": 1744812652.5007477
    },
    {
      "project_id": 6178942,
      "profile_id": "5b660830352444d181bbb0eedbde2d52",
      "start": 1744817810.1866,
      "end": 1744817811.2150917
    },
    {
      "project_id": 6178942,
      "profile_id": "311308b6910c4357921fb145decf4a58",
      "start": 1744816841.933217,
      "end": 1744816844.5936766
    },
    {
      "project_id": 6178942,
      "profile_id": "f05e641f70804fddb89b943d49e70236",
      "start": 1744812641.309885,
      "end": 1744812642.680363
    },
    {
      "project_id": 6178942,
      "profile_id": "830974daa4d441bf90a205fdfab1d7dc",
      "start": 1744817813.570292,
      "end": 1744817814.5615764
    },
    {
      "project_id": 6178942,
      "profile_id": "0e5f4d8900e3401b9be1113b1f07884b",
      "start": 1744812889.433053,
      "end": 1744812890.431816
    },
    {
      "project_id": 6178942,
      "profile_id": "3da1967741824989b888e47cad394156",
      "start": 1744812887.154359,
      "end": 1744812888.3524103
    },
    {
      "project_id": 6178942,
      "profile_id": "8139954bb1694c1ba2555440ee65533c",
      "start": 1744812281.680606,
      "end": 1744812282.8319821
    },
    {
      "project_id": 6178942,
      "profile_id": "04544dacba234f13ba493cf7badd5892",
      "start": 1744817860.873039,
      "end": 1744817861.9062939
    },
    {
      "project_id": 6178942,
      "profile_id": "37dd35a60219402a97f9c3d42506e973",
      "start": 1744817812.641197,
      "end": 1744817813.6756997
    },
    {
      "project_id": 6178942,
      "profile_id": "01d8181f0e184f318facfcb85dbcd7e2",
      "start": 1744812654.1338692,
      "end": 1744812655.0827866
    },
    {
      "project_id": 6178942,
      "profile_id": "8c6eaf043f73418eabac1bf804119ba8",
      "start": 1744812645.6880481,
      "end": 1744812646.7775667
    },
    {
      "project_id": 6178942,
      "profile_id": "061e9d62419c4222ab25297b90bef21f",
      "start": 1744812644.03953,
      "end": 1744812644.9084659
    },
    {
      "project_id": 6178942,
      "profile_id": "c7559dd03e51405b9db74e99bdfa0c2e",
      "start": 1744817325.4849331,
      "end": 1744817326.383095
    },
    {
      "project_id": 6178942,
      "profile_id": "a731d80eb8924e88954e6b4b62bf0198",
      "start": 1744816905.86182,
      "end": 1744816907.0188718
    },
    {
      "project_id": 6178942,
      "profile_id": "46abbed74f4c41e19756f16a8a5c3d9e",
      "start": 1744812881.4701178,
      "end": 1744812882.661016
    },
    {
      "project_id": 6178942,
      "profile_id": "e502ed1f319c41d4a0b24020cea62761",
      "start": 1744812283.274286,
      "end": 1744812284.1625154
    },
    {
      "project_id": 6178942,
      "profile_id": "e493620c079142f5899e31914dd7c2f9",
      "start": 1744812285.446634,
      "end": 1744812286.4563773
    },
    {
      "project_id": 6178942,
      "profile_id": "7ef3a9c5c392499fbab44180bacd9601",
      "start": 1744812210.601717,
      "end": 1744812213.6407933
    },
    {
      "project_id": 6178942,
      "profile_id": "ef3ed20f63844039a97878a54849e169",
      "start": 1744817806.015135,
      "end": 1744817807.0268047
    },
    {
      "project_id": 6178942,
      "profile_id": "c23df774a6944b2e8581755b329a28ba",
      "start": 1744817807.776396,
      "end": 1744817808.8757205
    },
    {
      "project_id": 6178942,
      "profile_id": "77fb90f09e3345a1b7ac75d47f54fbb2",
      "start": 1744817622.158704,
      "end": 1744817623.402058
    },
    {
      "project_id": 6178942,
      "profile_id": "6b76b17bc9ea4c8098d11d5ad2ca3ddb",
      "start": 1744817815.2832,
      "end": 1744817816.2177935
    },
    {
      "project_id": 6178942,
      "profile_id": "63bf68cc4da346ccb1f2cfbe96e3132d",
      "start": 1744812883.587432,
      "end": 1744812884.5748556
    },
    {
      "project_id": 6178942,
      "profile_id": "6d34fc8ab21048c2883baa4d9e37105a",
      "start": 1744812883.885109,
      "end": 1744812884.9725213
    },
    {
      "project_id": 6178942,
      "profile_id": "bdd6628513794a4abe0935b2d95f8d05",
      "start": 1744812881.7065349,
      "end": 1744812883.03383
    },
    {
      "project_id": 6178942,
      "profile_id": "d9e1f6a66b6d4382b499db13c1e9dceb",
      "start": 1744817805.1966982,
      "end": 1744817806.315488
    },
    {
      "project_id": 6178942,
      "profile_id": "ceb1d41c9297480e86418e3ef69490d8",
      "start": 1744817804.518076,
      "end": 1744817805.4383323
    },
    {
      "project_id": 6178942,
      "profile_id": "dd8905d65fcd4430b137c419d63d64bb",
      "start": 1744817328.245756,
      "end": 1744817329.1461926
    },
    {
      "project_id": 6178942,
      "profile_id": "918d7e3522714bdd98a342a099bb2a1b",
      "start": 1744815530.693672,
      "end": 1744815531.5015032
    },
    {
      "project_id": 6178942,
      "profile_id": "7d2c1d83c72b4397a9c78bae18431710",
      "start": 1744813189.520154,
      "end": 1744813190.5078402
    },
    {
      "project_id": 6178942,
      "profile_id": "832cc4b115f2475ea7d40aa805886e85",
      "start": 1744817864.9351628,
      "end": 1744817865.7874002
    },
    {
      "project_id": 6178942,
      "profile_id": "41c7da32533840bfad59994a1162dcc7",
      "start": 1744816909.166733,
      "end": 1744816910.109033
    },
    {
      "project_id": 6178942,
      "profile_id": "615cebe11fa742818bae2f04681fabfe",
      "start": 1744813901.023217,
      "end": 1744813902.1502976
    },
    {
      "project_id": 6178942,
      "profile_id": "dec52cdf9435411a9569484fd2ed7372",
      "start": 1744815470.397844,
      "end": 1744815472.9457996
    },
    {
      "project_id": 6178942,
      "profile_id": "ca18e6bccd134a2692c3fb51cab24c94",
      "start": 1744812649.7732549,
      "end": 1744812650.6939692
    },
    {
      "project_id": 6178942,
      "profile_id": "65601b507e5e48e4bf3ccdbcdec9da9d",
      "start": 1744813963.048853,
      "end": 1744813964.1301408
    },
    {
      "project_id": 6178942,
      "profile_id": "3e592a5e72134330bf24292c3dee91d1",
      "start": 1744817383.498601,
      "end": 1744817384.5863492
    },
    {
      "project_id": 6178942,
      "profile_id": "c0f3e83fa74f48378a33a527f71b5287",
      "start": 1744817681.5706801,
      "end": 1744817682.6885576
    },
    {
      "project_id": 6178942,
      "profile_id": "35b88473b8014af2825eb0d7404a95a0",
      "start": 1744812283.177639,
      "end": 1744812284.3084722
    },
    {
      "project_id": 6178942,
      "profile_id": "8c0bcf2b2aa142d6b3ae0f4e5299b111",
      "start": 1744812280.8317802,
      "end": 1744812281.9104319
    },
    {
      "project_id": 6178942,
      "profile_id": "c37b61e9a7e14bd68024b5cdac40bc07",
      "start": 1744816903.537046,
      "end": 1744816904.530997
    },
    {
      "project_id": 6178942,
      "profile_id": "ad4ff710d2034aecb1f2a963c7f0601c",
      "start": 1744813903.3637109,
      "end": 1744813904.2521954
    },
    {
      "project_id": 6178942,
      "profile_id": "a631a965f6014d59af7e898eef72e61a",
      "start": 1744815533.291071,
      "end": 1744815534.1509175
    },
    {
      "project_id": 6178942,
      "profile_id": "baf1f492e5b740dda008ec4040b3d523",
      "start": 1744813900.465864,
      "end": 1744813901.6360579
    },
    {
      "project_id": 6178942,
      "profile_id": "feec42aa67a3415ba22e54bfc8f08d15",
      "start": 1744817862.51731,
      "end": 1744817863.3558345
    },
    {
      "project_id": 6178942,
      "profile_id": "6a6b44dfcdd34deeb8afab4e3f53ecc0",
      "start": 1744817331.039513,
      "end": 1744817332.078953
    },
    {
      "project_id": 6178942,
      "profile_id": "f36fee95d62d445ba571dc01a723762e",
      "start": 1744817682.6058822,
      "end": 1744817683.6506844
    },
    {
      "project_id": 6178942,
      "profile_id": "0c9d5bceeeda42018054d064ef91f5d4",
      "start": 1744815528.0291798,
      "end": 1744815528.9304729
    },
    {
      "project_id": 6178942,
      "profile_id": "dcbd7f1971d3438a8c8d8b4ce4cfee53",
      "start": 1744813183.7481952,
      "end": 1744813188.7322814
    },
    {
      "project_id": 6178942,
      "profile_id": "264128acf4da4bd3b170e25d2765ad04",
      "start": 1744815580.7768161,
      "end": 1744815581.798687
    },
    {
      "project_id": 6178942,
      "profile_id": "c710081be6fc460a9a9b46a430c19a3e",
      "start": 1744817801.4977221,
      "end": 1744817802.7854106
    },
    {
      "project_id": 6178942,
      "profile_id": "f58c0bc9b8f64953a3f411d20301fd5d",
      "start": 1744817385.609287,
      "end": 1744817386.518211
    },
    {
      "project_id": 6178942,
      "profile_id": "63ab2a799e96400fbddff8f034c0f81f",
      "start": 1744817321.573602,
      "end": 1744817322.7712016
    },
    {
      "project_id": 6178942,
      "profile_id": "0c2207082faa4ee2b0c0f4543adb53bc",
      "start": 1744816841.215019,
      "end": 1744816842.3266687
    },
    {
      "project_id": 6178942,
      "profile_id": "32ca9bbf17df40d384954fa1b313a4a4",
      "start": 1744817324.4622562,
      "end": 1744817325.4015853
    },
    {
      "project_id": 6178942,
      "profile_id": "e9cf15c97e1c41d781ed85578acc7d5b",
      "start": 1744816906.932909,
      "end": 1744816907.881479
    },
    {
      "project_id": 6178942,
      "profile_id": "2f53fad243b74915b1685ef69f74025e",
      "start": 1744815524.487238,
      "end": 1744815525.404522
    },
    {
      "project_id": 6178942,
      "profile_id": "656edf899128454bb4086d17839d0781",
      "start": 1744812890.230038,
      "end": 1744812891.1110082
    },
    {
      "project_id": 6178942,
      "profile_id": "5ee52f87f0134cb083d4fa2298d062c7",
      "start": 1744812884.626178,
      "end": 1744812885.615949
    },
    {
      "project_id": 6178942,
      "profile_id": "e7a53cf57ada4868b922e8e652c46207",
      "start": 1744815525.492467,
      "end": 1744815526.3617148
    },
    {
      "project_id": 6178942,
      "profile_id": "ad022d1645ab45fea79a071afa8f202b",
      "start": 1744815520.854229,
      "end": 1744815523.5807502
    },
    {
      "project_id": 6178942,
      "profile_id": "86e82443e6be48128f031a352f66f5c1",
      "start": 1744812825.184267,
      "end": 1744812826.0829473
    },
    {
      "project_id": 6178942,
      "profile_id": "55496cdab5e54b58a98d7232e16efbf6",
      "start": 1744812887.727875,
      "end": 1744812888.6073303
    },
    {
      "project_id": 6178942,
      "profile_id": "5a20f6c12185417b924f2467561c04c6",
      "start": 1744817384.012383,
      "end": 1744817385.0452082
    },
    {
      "project_id": 6178942,
      "profile_id": "6b4c462f216a4c449ef384aa814fb86a",
      "start": 1744812212.2522829,
      "end": 1744812213.6970887
    },
    {
      "project_id": 6178942,
      "profile_id": "59d3b2a475304bb489d049e41137c493",
      "start": 1744812822.55735,
      "end": 1744812823.664674
    }
  ]
}